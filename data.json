[
    {
        "title": "Information retrieval",
        "url": "https://en.wikipedia.org/wiki/Information_retrieval",
        "content": "Information retrieval (IR) in computing and information science is the task of identifying and retrieving information system resources that are relevant to an information need.  The information need can be specified in the form of a search query. In the case of document retrieval, queries can be based on full-text or other content-based indexing. Information retrieval is the science[1] of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\n Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; it also stores and manages those documents. Web search engines are the most visible IR applications.\n An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval, a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevance.\n An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.[2]\n Depending on the application the data objects may be, for example, text documents, images,[3] audio,[4] mind maps[5] or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.\n Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.[6]\n there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945.[7] It would appear that Bush was inspired by patents for a 'statistical machine' \u2013 filed by Emanuel Goldberg in the 1920s and 1930s \u2013 that searched for documents stored on film.[8] The first description of a computer searching for information was described by Holmstrom in 1948,[9] detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents).[7] Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.\n In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.\n Areas where information retrieval techniques are employed include (the entries are in alphabetical order within each category):\n Methods/Techniques in which information retrieval techniques are employed include:\n In order to effectively retrieve relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.\n The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval[clarification needed] or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevance: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevance.\n"
    },
    {
        "title": "Nearest centroid classifier",
        "url": "https://en.wikipedia.org/wiki/Nearest_centroid_classifier",
        "content": "In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation. When applied to text classification using word vectors containing tf*idf weights to represent documents, the nearest centroid classifier is known as the Rocchio classifier because of its similarity to the Rocchio algorithm for relevance feedback.[1]\n An extended version of the nearest centroid classifier has found applications in the medical domain, specifically classification of tumors.[2]\n Given labeled training samples \n\n\n\n\n{\n(\n\n\n\n\nx\n\u2192\n\n\n\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n\u2026\n,\n(\n\n\n\n\nx\n\u2192\n\n\n\n\nn\n\n\n,\n\ny\n\nn\n\n\n)\n}\n\n\n\n{\\displaystyle \\textstyle \\{({\\vec {x}}_{1},y_{1}),\\dots ,({\\vec {x}}_{n},y_{n})\\}}\n\n with class labels \n\n\n\n\ny\n\ni\n\n\n\u2208\n\nY\n\n\n\n{\\displaystyle y_{i}\\in \\mathbf {Y} }\n\n, compute the per-class centroids \n\n\n\n\n\n\n\n\n\u03bc\n\u2192\n\n\n\n\n\u2113\n\n\n=\n\n\n1\n\n\n|\n\n\nC\n\n\u2113\n\n\n\n|\n\n\n\n\n\n\n\u2211\n\ni\n\u2208\n\nC\n\n\u2113\n\n\n\n\n\n\n\n\n\nx\n\u2192\n\n\n\n\ni\n\n\n\n\n\n{\\displaystyle \\textstyle {\\vec {\\mu }}_{\\ell }={\\frac {1}{|C_{\\ell }|}}{\\underset {i\\in C_{\\ell }}{\\sum }}{\\vec {x}}_{i}}\n\n where \n\n\n\n\nC\n\n\u2113\n\n\n\n\n{\\displaystyle C_{\\ell }}\n\n is the set of indices of samples belonging to class \n\n\n\n\u2113\n\u2208\n\nY\n\n\n\n{\\displaystyle \\ell \\in \\mathbf {Y} }\n\n.\n The class assigned to an observation \n\n\n\n\n\n\nx\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {x}}}\n\n is \n\n\n\n\n\n\ny\n^\n\n\n\n=\n\n\narg\n\u2061\nmin\n\n\n\u2113\n\u2208\n\nY\n\n\n\n\u2016\n\n\n\n\n\u03bc\n\u2192\n\n\n\n\n\u2113\n\n\n\u2212\n\n\n\nx\n\u2192\n\n\n\n\u2016\n\n\n{\\displaystyle {\\hat {y}}={\\arg \\min }_{\\ell \\in \\mathbf {Y} }\\|{\\vec {\\mu }}_{\\ell }-{\\vec {x}}\\|}\n\n.\n"
    },
    {
        "title": "Fuzzy retrieval",
        "url": "https://en.wikipedia.org/wiki/Fuzzy_retrieval",
        "content": "Fuzzy retrieval techniques are based on the Extended Boolean model and the Fuzzy set theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the P-norms algorithm.\n In fuzzy-set theory, an element has a varying degree of membership, say dA, to a given set A instead of the traditional membership choice (is an element/is not an element).\nIn MMM[1] each index term has a fuzzy set associated with it. A document's weight with respect to an index term A is considered to be the degree of membership of the document in the fuzzy set associated with A. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:\n According to this, documents that should be retrieved for a query of the form A or B, should be in the fuzzy set associated with the union of the two sets A and B. Similarly, the documents that should be retrieved for a query of the form A and B, should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the or query to be max(dA, dB) and the similarity of the document to the and query to be min(dA, dB). The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the min and max document weights.\n Given a document D with index-term weights dA1, dA2, ..., dAn for terms A1, A2, ..., An, and the queries:\n Qor = (A1 or A2 or ... or An)\nQand = (A1 and A2 and ... and An)\n the query-document similarity in the MMM model is computed as follows:\n SlM(Qor, D) = Cor1 * max(dA1, dA2, ..., dAn) + Cor2 * min(dA1, dA2, ..., dAn)\nSlM(Qand, D) = Cand1 * min(dA1, dA2, ..., dAn) + Cand2 * max(dA1, dA2 ..., dAn)\n where Cor1, Cor2 are \"softness\" coefficients for the or operator, and Cand1, Cand2 are softness coefficients for the and operator. Since we would like to give the maximum of the document weights more importance while considering an or query and the minimum more importance while considering an and query, generally we have Cor1 > Cor2 and Cand1 > Cand2. For simplicity it is generally assumed that Cor1 = 1 - Cor2 and Cand1 = 1 - Cand2.\n Lee and Fox[2] experiments indicate that the best performance usually occurs with Cand1 in the range [0.5, 0.8] and with Cor1 > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the Standard Boolean model.\n The Paice model[3] is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:\n where r is a constant coefficient and wdi is arranged in ascending order for and queries and descending order for or queries. When n = 2 the Paice model shows the same behavior as the MMM model.\n The experiments of Lee and Fox[2] have shown that setting the r to 1.0 for and queries and 0.7 for or queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of min or max of a set of term weights each time an and or or clause is considered, which can be done in O(n). The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an and clause or an or clause is being considered. This requires at least an 0(n log n) sorting algorithm. A good deal of floating point calculation is needed too.\n Lee and Fox[2] compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:\n These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.\n In 2005, Kang et al.[4] have devised a fuzzy retrieval system indexed by concept identification.\n If we look at documents on a pure Tf-idf approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.\nThey report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.\n Zadrozny[5] revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:\n The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.\n"
    },
    {
        "title": "Science and technology studies",
        "url": "https://en.wikipedia.org/wiki/Science_and_technology_studies",
        "content": "Science and technology studies (STS) or science, technology, and society is an interdisciplinary field that examines the creation, development, and consequences of science and technology in their historical, cultural, and social contexts.[1]\n Like most interdisciplinary fields of study, STS emerged from the confluence of a variety of disciplines and disciplinary subfields, all of which had developed an interest\u2014typically, during the 1960s or 1970s\u2014in viewing science and technology as socially embedded enterprises.[2]  The key disciplinary components of STS took shape independently, beginning in the 1960s, and developed in isolation from each other well into the 1980s, although Ludwik Fleck's (1935) monograph Genesis and Development of a Scientific Fact anticipated many of STS's key themes. In the 1970s Elting E. Morison founded the STS program at the Massachusetts Institute of Technology (MIT), which served as a model. By 2011, 111 STS research centers and academic programs were counted worldwide.[3]\n During the 1970s and 1980s, universities in the US, UK, and Europe began drawing these various components together in new, interdisciplinary programs. For example, in the 1970s, Cornell University developed a new program that united science studies and policy-oriented scholars with historians and philosophers of science and technology. Each of these programs developed unique identities due to variations in the components that were drawn together, as well as their location within the various universities. For example, the University of Virginia's STS program united scholars drawn from a variety of fields (with particular strength in the history of technology); however, the program's teaching responsibilities\u2014it is located within an engineering school and teaches ethics to undergraduate engineering students\u2014means that all of its faculty share a strong interest in engineering ethics.[5]\n A decisive moment in the development of STS was the mid-1980s addition of technology studies to the range of interests reflected in science. During that decade, two works appeared en seriatim that signaled what Steve Woolgar was to call the \"turn to technology\".[6] In a seminal 1984 article, Trevor Pinch and Wiebe Bijker showed how the sociology of technology could proceed along the theoretical and methodological lines established by the sociology of scientific knowledge. [7] This was the intellectual foundation of the field they called the social construction of technology. Donald MacKenzie and Judy Wajcman primed the pump by publishing a collection of articles attesting to the influence of society on technological design (Social Shaping of Technology, 1985).[8] Social science research continued to interrogate STS research from this point onward as researchers moved from post-modern to post-structural frameworks of thought, Bijker and Pinch contributing to SCOT knowledge and Wajcman providing boundary work through a feminist lens.[9]\n The \"turn to technology\" helped to cement an already growing awareness of underlying unity among the various emerging STS programs. More recently, there has been an associated turn to ecology, nature, and materiality in general, whereby the socio-technical and natural/material co-produce each other. This is especially evident in work in STS analyses of biomedicine (such as Carl May and Annemarie Mol) and ecological interventions (such as Bruno Latour, Sheila Jasanoff, Matthias Gross, Sara B. Pritchard, and S. Lochlann Jain).\n Social constructions are human-created ideas, objects, or events created by a series of choices and interactions.[10] These interactions have consequences that change the perception that different groups of people have on these constructs. Some examples of social construction include class, race, money, and citizenship.\n The following also alludes to the notion that not everything is set, a circumstance or result could potentially be one way or the other. According to the article \"What is Social Construction?\" by Ian Hacking, \"Social construction work is critical of the status quo. Social constructionists about X tend to hold that:\n Very often they go further, and urge that:\n In the past, there have been viewpoints that were widely regarded as fact until being called to question due to the introduction of new knowledge. Such viewpoints include the past concept of a correlation between intelligence and the nature of a human's ethnicity or race (X may not be at all as it is).[11]\n An example of the evolution and interaction of various social constructions within science and technology can be found in the development of both the high-wheel bicycle, or velocipede, and then of the bicycle.  The velocipede was widely used in the latter half of the 19th century. In the latter half of the 19th century, a social need was first recognized for a more efficient and rapid means of transportation. Consequently, the velocipede was first developed, which was able to reach higher translational velocities than the smaller non-geared bicycles of the day, by replacing the front wheel with a larger radius wheel. One notable trade-off was a certain decreased stability leading to a greater risk of falling. This trade-off resulted in many riders getting into accidents by losing balance while riding the bicycle or being thrown over the handlebars.\n The first \"social construction\" or progress of the velocipede caused the need for a newer \"social construction\" to be recognized and developed into a safer bicycle design.  Consequently, the velocipede was then developed into what is now commonly known as the \"bicycle\" to fit within society's newer \"social construction,\" the newer standards of higher vehicle safety. Thus the popularity of the modern geared bicycle design came as a response to the first social construction, the original need for greater speed, which had caused the high-wheel bicycle to be designed in the first place.  The popularity of the modern geared bicycle design ultimately ended the widespread use of the velocipede itself, as eventually it was found to best accomplish the social needs/social constructions of both greater speed and of greater safety.[12]\n With methodology from ANT, feminist STS theorists built upon SCOT's theory of co-construction to explore the relationship between gender and technology, proposing one cannot exist separately from the other.[13] This approach suggests the material and social are not separate, reality being produced through interactions and studied through representations of those realities.[14] Building on Steve Woolgar's boundary work on user configuration,[15] feminist critiques shifted the focus away from users of technology and science towards whether technology and science represent a fixed, unified reality.[16] According to this approach, identity could no longer be treated as causal in human interactions with technology as it cannot exist prior to that interaction, feminist STS researchers proposing a \"double-constructivist\" approach to account for this contradiction.[17] John Law credits feminist STS scholars for contributing material-semiotic approaches to the broader discipline of STS, stating that research not only attempts to describe reality, but enacts it through the research process.[18]\n Sociotechnical imaginaries are what certain communities, societies, and nations envision as achievable through the combination of scientific innovation and social changes. These visions can be based on what is possible to achieve for a certain society, and can also show what a certain state or nation desires.[19] STIs are often bound with ideologies and ambitions of those who create and circulate them. Sociotechnical imaginaries can be created by states and policymakers, smaller groups within society, or can be a result of the interaction of both.[19]\n The term was coined in 2009 by Sheila Jasanoff and Sang-Hyun Kim who compared and contrasted sociotechnical imaginaries of nuclear energy in the USA with those of South Korea over the second half of the 20th century.[19] Jasanoff and Kim analyzed the discourse of government representatives, national policies, and civil society organizations, looked at the technological and infrastructural developments, and social protests, and conducted interviews with experts. They concluded that in South Korea nuclear energy was imagined mostly as the means of national development, while in the US the dominant sociotechnical imaginary framed nuclear energy as risky and in need of containment.[19]\n The concept has been applied to several objects of study including biomedical research,[20][21] nanotechnology development[22] and energy systems and climate change.[23][24][25][26][27][19] Within energy systems, research has focused on nuclear energy,[19] fossil fuels,[24][27] renewables[23] as well as broader topics of energy transitions,[25] and the development of new technologies to address climate change.[26]\n Social technical systems are an interplay between technologies and humans, this is clearly expressed in the sociotechnical systems theory. To expound on this interplay, humans fulfill and define tasks, then humans in companies use IT and IT supports people, and finally, IT processes tasks and new IT generates new tasks. This IT redefines work practices. This is what we call the sociotechnical systems.[28] In socio-technical systems, there are two principles to internalize, that is joint optimization and complementarity. Joint optimization puts an emphasis on developing both systems in parallel and it is only in the interaction of both systems that the success of an organization arises.[28] The principle of complementarity means that both systems have to be optimized.[28] If you focus on one system and have bias over the other it will likely lead to the failure of the organization or jeopardize the success of a system. Although the above socio-technical system theory is focused on an organization, it is undoubtedly imperative to correlate this theory and its principles to society today and in science and technology studies.\n According to Barley and Bailey, there is a\u00a0 tendency for AI designers and scholars of design studies to privilege the technical over the social, focusing more on taking \"humans out of the loop\" paradigm than the \"augmented intelligence\" paradigm.[29]\n Recent work on artificial intelligence considers large sociotechnical systems, such as social networks and online marketplaces, as agents whose behavior can be purposeful and adaptive. The behavior of recommender systems can therefore be analyzed in the language and framework of sociotechnical systems, leading also to a new perspective for their legal regulation.[30][31]\n Technoscience is a subset of Science, Technology, and Society studies that focuses on the inseparable connection between science and technology. It states that fields are linked and grow together, and scientific knowledge requires an infrastructure of technology in order to remain stationary or move forward. Both technological development and scientific discovery drive one another towards more advancement. Technoscience excels at shaping human thoughts and behavior by opening up new possibilities that gradually or quickly come to be perceived as necessities.[32]\n \"Technological action is a social process.\"[33] Social factors and technology are intertwined so that they are dependent upon each other. This includes the aspect that social, political, and economic factors are inherent in technology and that social structure influences what technologies are pursued. In other words, \"technoscientific phenomena combined inextricably with social/political/economic/psychological phenomena, so 'technology' includes a spectrum of artifacts, techniques, organizations, and systems.\"[34] Winner expands on this idea by saying \"in the late twentieth-century technology and society, technology and culture, technology and politics are by no means separate.\"[35]\n Deliberative democracy is a reform of representative or direct democracies which mandates discussion and debate of popular topics which affect society.  Deliberative democracy is a tool for making decisions.  Deliberative democracy can be traced back all the way to Aristotle's writings. More recently, the term was coined by Joseph Bessette in his 1980 work Deliberative Democracy: The Majority Principle in Republican Government, where he uses the idea in opposition to the elitist interpretations of the United States Constitution with emphasis on public discussion.[37]\n Deliberative democracy can lead to more legitimate, credible, and trustworthy outcomes. Deliberative democracy allows for \"a wider range of public knowledge\", and it has been argued that this can lead to \"more socially intelligent and robust\" science. One major shortcoming of deliberative democracy is that many models insufficiently ensure critical interaction.[38]\n According to Ryfe, there are five mechanisms that stand out as critical to the successful design of deliberative democracy:\n Recently,[when?] there has been a movement towards greater transparency in the fields of policy and technology. Jasanoff comes to the conclusion that there is no longer a question of if there needs to be increased public participation in making decisions about science and technology, but now there need to be ways to make a more meaningful conversation between the public and those developing the technology.[40]\n Bruce Ackerman and James S. Fishkin offered an example of a reform in their paper \"Deliberation Day.\" The deliberation is to enhance public understanding of popular, complex and controversial issues through devices such as Fishkin's deliberative polling,[41] though implementation of these reforms is unlikely in a large government such as that of the United States. However, things similar to this have been implemented in small, local governments like New England towns and villages. New England town hall meetings are a good example of deliberative democracy in a realistic setting.[37]\n An ideal deliberative democracy balances the voice and influence of all participants. While the main aim is to reach consensus, deliberative democracy should encourage the voices of those with opposing viewpoints, concerns due to uncertainties, and questions about assumptions made by other participants. It should take its time and ensure that those participating understand the topics on which they debate. Independent managers of debates should also have a substantial grasp of the concepts discussed, but must \"[remain] independent and impartial as to the outcomes of the process.\"[38]\n In 1968, Garrett Hardin popularised the phrase \"tragedy of the commons.\" It is an economic theory where rational people act against the best interest of the group by consuming a common resource. Since then, the tragedy of the commons has been used to symbolize the degradation of the environment whenever many individuals use a common resource. Although Garrett Hardin was not an STS scholar, the concept of the tragedy of the commons still applies to science, technology, and society.[42]\n In a contemporary setting, the Internet acts as an example of the tragedy of the commons through the exploitation of digital resources and private information. Data and internet passwords can be stolen much more easily than physical documents. Virtual spying is almost free compared to the costs of physical spying.[43] Additionally, net neutrality can be seen as an example of tragedy of the commons in an STS context. The movement for net neutrality argues that the Internet should not be a resource that is dominated by one particular group, specifically those with more money to spend on Internet access.\n A counterexample to the tragedy of the commons is offered by Andrew Kahrl. Privatization can be a way to deal with the tragedy of the commons. However, Kahrl suggests that the privatization of beaches on Long Island, in an attempt to combat the overuse of Long Island beaches, made the residents of Long Island more susceptible to flood damage from Hurricane Sandy. The privatization of these beaches took away from the protection offered by the natural landscape. Tidal lands that offer natural protection were drained and developed. This attempt to combat the tragedy of the commons by privatization was counter-productive. Privatization actually destroyed the public good of natural protection from the landscape.[44]\n Alternative modernity[45][46] is a conceptual tool conventionally used to represent the state of present western society. Modernity represents the political and social structures of society, the sum of interpersonal discourse, and ultimately a snapshot of society's direction at a point in time. Unfortunately, conventional modernity is incapable of modeling alternative directions for further growth within our society. Also, this concept is ineffective at analyzing similar but unique modern societies such as those found in the diverse cultures of the developing world. Problems can be summarized into two elements: inward failure to analyze the growth potentials of a given society, and outward failure to model different cultures and social structures and predict their growth potentials.\n Previously, modernity carried a connotation of the current state of being modern, and its evolution through European colonialism. The process of becoming \"modern\" is believed to occur in a linear, pre-determined way, and is seen by Philip Brey as a way to interpret and evaluate social and cultural formations. This thought ties in with modernization theory, the thought that societies progress from \"pre-modern\" to \"modern\" societies.\n Within the field of science and technology, there are two main lenses with which to view modernity. The first is as a way for society to quantify what it wants to move towards. In effect, we can discuss the notion of \"alternative modernity\" (as described by Andrew Feenberg) and which of these we would like to move towards. Alternatively, modernity can be used to analyze the differences in interactions between cultures and individuals. From this perspective, alternative modernities exist simultaneously, based on differing cultural and societal expectations of how a society (or an individual within society) should function. Because of different types of interactions across different cultures, each culture will have a different modernity.\n The pace of innovation is the speed at which technological innovation or advancement is occurring, with the most apparent instances being too slow or too rapid. Both these rates of innovation are extreme and therefore have effects on the people that get to use this technology.\n \"No innovation without representation\" is a democratic ideal of ensuring that everyone involved gets a chance to be represented fairly in technological developments.\n Legacy thinking is defined as an inherited method of thinking imposed from an external source without objection by the individual because it is already widely accepted by society.\n Legacy thinking can impair the ability to drive technology for the betterment of society by blinding people to innovations that do not fit into their accepted model of how society works.  By accepting ideas without questioning them, people often see all solutions that contradict these accepted ideas as impossible or impractical.  Legacy thinking tends to advantage the wealthy, who have the means to project their ideas on the public.  It may be used by the wealthy as a vehicle to drive technology in their favor rather than for the greater good.\nExamining the role of citizen participation and representation in politics provides an excellent example of legacy thinking in society. The belief that one can spend money freely to gain influence has been popularized, leading to public acceptance of corporate lobbying. As a result, a self-established role in politics has been cemented where the public does not exercise the power ensured to them by the Constitution to the fullest extent. This can become a barrier to political progress as corporations who have the capital to spend have the potential to wield great influence over policy.[50] Legacy thinking, however, keeps the population from acting to change this, despite polls from Harris Interactive that report over 80% of Americans to feel that big business holds too much power in government.[51] Therefore, Americans are beginning to try to steer away from this line of thought, rejecting legacy thinking, and demanding less corporate, and more public, participation in political decision-making.\n Additionally, an examination of net neutrality functions as a separate example of legacy thinking. Starting with dial-up, the internet has always been viewed as a private luxury good.[52][53] Internet today is a vital part of modern-day society members. They use it in and out of life every day.[54] Corporations are able to mislabel and greatly overcharge for their internet resources. Since the American public is so dependent upon the internet there is little for them to do. Legacy thinking has kept this pattern on track despite growing movements arguing that the internet should be considered a utility. Legacy thinking prevents progress because it was widely accepted by others before us through advertising that the internet is a luxury and not a  utility. Due to pressure from grassroots movements the Federal Communications Commission (FCC) has redefined the requirements for broadband and internet in general as a utility.[54] Now AT&T and other major internet providers are lobbying against this action and are in large able to delay the onset of this movement due to legacy thinking's grip on American[specify] culture and politics.\n For example, those who cannot overcome the barrier of legacy thinking may not consider the privatization of clean drinking water as an issue.[55] This is partial because access to water has become such a given fact of the matter to them. For a person living in such circumstances, it may be widely accepted to not concern themselves with drinking water because they have not needed to be concerned with it in the past. Additionally, a person living within an area that does not need to worry about their water supply or the sanitation of their water supply is less likely to be concerned with the privatization of water.\n This notion can be examined through the thought experiment of \"veil of ignorance\".[56] Legacy thinking causes people to be particularly ignorant about the implications behind the \"you get what you pay for\" mentality applied to a life necessity. By utilizing the \"veil of ignorance\", one can overcome the barrier of legacy thinking as it requires a person to imagine that they are unaware of their own circumstances, allowing them to free themselves from externally imposed thoughts or widely accepted ideas.\n STS is taught in several countries. According to the STS wiki, STS programs can be found in twenty countries, including 45 programs in the United States, three programs in India, and eleven programs in the UK.[62] STS programs can be found in Canada,[63] Germany,[64][65] Israel,[66] Malaysia,[67] and Taiwan.[68] Some examples of institutions offering STS programs are Stanford University,[69] University College London,[70] Harvard University,[71] the University of Oxford,[72] Mines ParisTech,[73] Bar-Ilan University,[74] and York University.[63] In Europe the European Inter-University Association on Society, Science and Technology (ESST) offers an MA degree in STS through study programs and student exchanges with over a dozen specializations.\n The field has professional associations in regions and countries around the world.\n Notable peer-reviewed journals in STS include: \n Student journals in STS include: \n"
    },
    {
        "title": "Thomas P. Hughes",
        "url": "https://en.wikipedia.org/wiki/Thomas_P._Hughes",
        "content": "Thomas Parke Hughes (September 13, 1923[1] \u2013 February 3, 2014[2]) was an American historian of technology. He was an emeritus professor of history at the University of Pennsylvania[3] and a visiting professor at MIT and Stanford.[4]\n He received his Ph.D. from the University of Virginia in 1953.\n Hughes, along with John B. Rae, Carl W. Condit, and Melvin Kranzberg, were responsible for the establishment of the Society for the History of Technology and he was a recipient of its highest honor, the Leonardo da Vinci Medal in 1985.[5]\n He contributed to the concepts of technological momentum, technological determinism, large technical systems, social construction of technology, and introduced systems theory into the history of technology.\n His book American Genesis was shortlisted for the Pulitzer Prize. He was elected to the American Philosophical Society in 2003.[6]\n"
    },
    {
        "title": "Association for Computing Machinery",
        "url": "https://en.wikipedia.org/wiki/Association_for_Computing_Machinery",
        "content": "\n The Association for Computing Machinery (ACM) is a US-based international learned society for computing. It was founded in 1947 and is the world's largest scientific and educational computing society.[1] The ACM is a non-profit professional membership group,[2] reporting nearly 110,000 student and professional members as of 2022[update]. Its headquarters are in New York City.\n The ACM is an umbrella organization for academic and scholarly interests in computer science (informatics). Its motto is \"Advancing Computing as a Science & Profession\".\n In 1947, a notice was sent to various people:[3][4]\n On January 10, 1947, at the Symposium on Large-Scale Digital Calculating Machinery at the Harvard computation Laboratory, Professor Samuel H. Caldwell of Massachusetts Institute of Technology spoke of the need for an association of those interested in computing machinery, and of the need for communication between them.\n[...]\nAfter making some inquiries during May and June, we believe there is ample interest to start an informal association of many of those interested in the new machinery for computing and reasoning. Since there has to be a beginning, we are acting as a temporary committee to start such an association:\n\n The committee (except for Curtiss) had gained experience with computers during World War II: Berkeley, Campbell, and Goheen helped build Harvard Mark I under Howard H. Aiken, Mauchly and Sharpless were involved in building ENIAC, Tompkins had used \"the secret Navy code-breaking machines\", and Taylor had worked on Bush's Differential analyzers.[4]\n The ACM was then founded in 1947 under the name Eastern Association for Computing Machinery, which was changed the following year to the Association for Computing Machinery.[5][6][7] The ACM History Committee since 2016 has published the A.M.Turing Oral History project, the ACM Key Award Winners Video Series, and the India Industry Leaders Video project.[8]\n ACM is organized into over 180 local professional chapters[9] and 38 Special Interest Groups (SIGs),[10] through which it conducts most of its activities. Additionally, there are over 680 student chapters.[9] The first student chapter was founded in 1961 at the University of Louisiana at Lafayette.[11][12]\n Many of the SIGs, such as SIGGRAPH, SIGDA, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.[13]\n ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.[14]\n ACM publishes over 50 journals[15] including the prestigious[16] Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:\n Although Communications no longer publishes primary research and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.\n ACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. ACM also offers insurance, online courses, and other services to its members.\n In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN\u00a00897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.\n The ACM Portal is an online service of the ACM.[19] Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.[20]\n The ACM Digital Library was launched in October 1997.[21] It is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries.[19] The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.\n ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.[22]\n ACM was a \"green\" publisher before the term was invented.[23] Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record.\n All metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription. In addition, starting on April 7, 2022, ACM made its publications from 1951 to 2000 open access through the Digital Library in celebration of the 75th anniversary of the organization's founding.[24]\n In 2020, ACM launched a major push to become a fully open access publisher by 2026. ACM restructured its pricing for the ACM Digital Library on the basis of publishing activity by affiliated lead authors in ACM's journals, magazines, and conference proceedings. Under this model, termed \"ACM Open,\" institutions pay set fees for full access to ACM Digital Library contents as well as unlimited open access publishing by their affiliated authors. Authors not affiliated with a participating institution will be expected to pay an article processing charge.[25][26] As of May 2024, ACM reported that more than 1,340 institutions worldwide had signed on for ACM Open, putting ACM at just over halfway to meeting its target of 2,500 participating institutions by 2026.[27]\n In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and \"demonstrated performance that sets them apart from their peers\".[28]\n The number of Fellows, Distinguished Members, and Senior Members cannot exceed 1%, 10%, and 25% of the total number of professional members, respectively.[29]\n The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 \"to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM.\" There are 1,310 Fellows as of 2020[update][30] out of about 100,000 members.\n In 2006, ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and \"have made a significant impact on the computing field\". In 2006 when the Distinguished Members first came out, one of the three levels was called \"Distinguished Member\" and was changed about two years later to \"Distinguished Educator\". Those who already had the Distinguished Member title had their titles changed to one of the other three titles.\n List of Distinguished Members of the Association for Computing Machinery [31]\n Also in 2006, ACM began recognizing Senior Members. According to the ACM, \"The Senior Members Grade recognizes those ACM members with at least 10 years of professional experience and 5 years of continuous Professional Membership who have demonstrated performance through technical leadership, and technical or professional contributions\".[32] Senior membership also requires 3 letters of reference\n While not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'.[33] The distinguished speakers program (DSP) has been in existence for over 20 years and serves as an outreach program that brings renowned experts from Academia, Industry and Government to present on the topic of their expertise.[34]  The DSP is overseen by a committee [35]\n ACM has three kinds of chapters: Special Interest Groups,[36] Professional Chapters, and Student Chapters.[37]\n As of 2022[update], ACM has professional & SIG Chapters in 56 countries.[38]\n As of 2022[update], there exist ACM student chapters in 41 countries.[39]\n ACM and its Special Interest Groups (SIGs) sponsors numerous conferences worldwide. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, SIGGRAPH 2007 attracted about 30000 attendees, while CIKM 2005 and RecSys 2022 had paper acceptance rates of only accepted 15% and 17% respectively.[41]\n The ACM is a co\u2013presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.[47]\n Some conferences are hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM.[48] In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.\n For additional non-ACM conferences, see this list of computer science conferences.\n The ACM presents or co\u2013presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.[49][50][51]\n Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.[55]\n The President of ACM for 2022\u20132024 is Yannis Ioannidis, Professor at the National and Kapodistrian University of Athens.[56] He is successor of Gabriele Kotsis (2020\u20132022), Professor at the Johannes Kepler University Linz; Cherri M. Pancake (2018\u20132020), professor emeritus at Oregon State University and Director of the Northwest Alliance for Computational Science and Engineering (NACSE); Vicki L. Hanson (2016\u20132018), Distinguished Professor at the Rochester Institute of Technology and visiting professor at the University of Dundee; Alexander L. Wolf (2014\u20132016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012\u20132014), American computer scientist and Internet pioneer; Alain Chesnais (2010\u20132012); and Dame Wendy Hall of the University of Southampton, UK (2008\u20132010).[57]\n ACM is led by a council consisting of the president, vice-president, treasurer, past president, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members-At-Large. This institution is often referred to simply as \"Council\" in Communications of the ACM.\n ACM has numerous boards, committees, and task forces which run the organization:[58]\n ACM-W,[59] the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM\u2013W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively.  ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and Committee on the Status of Women in Computing Research (CRA-W).\nThe ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science.[60] This program began in 2006. Speakers are nominated by SIG officers.[61]\n ACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards.[62] ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE.[63] Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS.[64] They occasionally cooperate on projects like developing computing curricula.[65]\n ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).[66]\n In December 2019, the ACM co-signed a letter with over one hundred other publishers to President Donald Trump saying that an open access mandate would increase costs to taxpayers or researchers and hurt intellectual property. This was in response to rumors that he was considering issuing an executive order that would require federally funded research be made freely available online immediately after being published. It is unclear how these rumors started.[67] Many ACM members opposed the letter, leading ACM to issue a statement clarifying that they remained committed to open access,[68] and they wanted to see communication with stakeholders about the potential mandate. The statement did not significantly assuage criticism from ACM members.[69]\n The SoCG conference, while originally an ACM conference, parted ways with ACM in 2014[70] because of problems when organizing conferences abroad.[71]\n"
    },
    {
        "title": "RTX Corporation",
        "url": "https://en.wikipedia.org/wiki/Raytheon_Technologies",
        "content": "\nRTX Corporation, formerly Raytheon Technologies Corporation,[3][4] is an American multinational aerospace and defense conglomerate headquartered in Arlington, Virginia. It is one of the largest aerospace and defense manufacturers in the world by revenue and market capitalization, as well as one of the largest providers of intelligence services.[note 1][7] In 2023, the company's seat in Forbes Global 2000 was 79.[8] RTX manufactures aircraft engines, avionics, aerostructures, cybersecurity solutions, guided missiles, air defense systems, satellites, and drones. The company is a large military contractor, getting much of its revenue from the U.S. government.[9][10]\n The company was formed in 2020 by a merger of equals between the aerospace subsidiaries of United Technologies Corporation (UTC) and the Raytheon Company. Before the merger, UTC spun off its non-aerospace subsidiaries Otis Elevator Company and Carrier Corporation. UTC is the nominal survivor of the merger but it changed its name to Raytheon Technologies and moved its headquarters to Waltham, Massachusetts.[2][11] Former UTC CEO and chairman Gregory J. Hayes is chairman and CEO of the combined company,[12] which changed its name to RTX in July 2023.\n The company has three subsidiaries: Collins Aerospace, Pratt & Whitney, and Raytheon (formerly Raytheon Intelligence & Space and Raytheon Missiles & Defense).[3]\n The Raytheon Company was founded in 1922 in Cambridge, Massachusetts, by Laurence K. Marshall, Vannevar Bush, and Charles G. Smith as the American Appliance Company.[13] Its focus, which was originally on new refrigeration technology, soon shifted to electronics. The company's first product was a gaseous (helium) rectifier that was based on Charles Smith's earlier astronomical research of the star Zeta Puppis.[14] The electron tube was christened with the name Raytheon (\"light of/from the gods\"[15]) and was used in a battery eliminator, a type of radio-receiver power supply that plugged into the power grid in place of large batteries. This made it possible to convert household alternating current to direct current for radios and thus eliminate the need for expensive, short-lived batteries.\n In 1925, the company changed its name to Raytheon Manufacturing Company and began marketing its rectifier under the Raytheon brand name, with commercial success. In 1928, Raytheon merged with Q.R.S. Company, an American manufacturer of electron tubes and switches, to form the successor of the same name, Raytheon Manufacturing Company.[citation needed] By the 1930s, it had already grown to become one of the world's largest vacuum tube manufacturing companies.[citation needed] In 1933 it diversified by acquiring Acme-Delta Company, a producer of transformers, power equipment, and electronic auto parts.\n During World War II, Raytheon mass-manufactured magnetron tubes for use in microwave radar sets and then complete radar systems. At war's end in 1945, the company was responsible for about 80 percent of all magnetrons manufactured. During the war, Raytheon also pioneered the production of shipboard radar systems, particularly for submarine detection. Raytheon ranked 71st among United States corporations in the value of World War II military production contracts.[16]  In 1945, Raytheon's Percy Spencer invented the microwave oven by discovering that the magnetron could rapidly heat food. In 1947, the company demonstrated the Radarange microwave oven for commercial use.\n After the war, Raytheon developed the first guidance system for a missile that could intercept a flying target. In 1948, Raytheon began to manufacture guided missiles, including the SAM-N-2 Lark, and eventually the air-to-air AIM-7 Sparrow, and the ground-to-air MIM-23 Hawk missiles. In 1959, Raytheon acquired the marine electronics company Apelco Applied Electronics, which significantly increased its strength in commercial marine navigation and radio gear, and changed its name to Raytheon Company.\n During the post-war years, Raytheon also made generally low- to medium-powered radio and television transmitters and related equipment for the commercial market. In the 1950s, Raytheon began manufacturing transistors, including the CK722, priced for and marketed to hobbyists. Under the direction of Thomas L. Phillips in 1965, it acquired Amana Refrigeration, Inc., a manufacturer of refrigerators and air conditioners. Using the Amana brand name and its distribution channels, Raytheon began selling the first countertop household microwave oven in 1967 and became a dominant manufacturer in the microwave oven business.\n In 1991, during the Persian Gulf War, Raytheon's Patriot missile received great international exposure. It was credited for downing Iraqi Scud missiles.[17] The exposure resulted in a substantial increase in sales for the company outside the United States. By 2006, Raytheon reported $283.9 million in global revenues for its Patriot missile system.[17]\n In an effort to establish leadership in the defense electronics business, Raytheon purchased in quick succession Dallas-based E-Systems (1995); Chrysler Corporation's defense electronics and aircraft-modification businesses, and the defense unit of Texas Instruments, Defense Systems & Electronics Group (1997).[18] The businesses were purchased for $2.3 billion and $2.95 billion, respectively.[18] Also in 1997, Raytheon acquired the aerospace and defense business of Hughes Aircraft Company from Hughes Electronics Corporation, a subsidiary of General Motors, which included a number of product lines previously purchased by Hughes Electronics, including the former General Dynamics missile business (Pomona facility), the defense portion of Delco Electronics (Delco Systems Operations), and Magnavox Electronic Systems.[19] Raytheon also divested itself of several nondefense businesses in the 1990s, including Amana Refrigeration and Seismograph Service Ltd (sold to Schlumberger-Geco-Prakla).\n In November 2007, Raytheon purchased robotics company Sarcos,[20] and in October 2009, Raytheon acquired BBN Technologies.[21][22] In December 2010, Applied Signal Technology agreed to be acquired by Raytheon for $490 million.[23]\n In October 2014, Raytheon beat rivals Lockheed Martin and Northrop Grumman for a contract to build 3DELRR, a next-generation long-range radar system, for the US Air Force worth an estimated $1 billion.[24] The contract award involved the construction of next-generation radar that can track aircraft, missiles, and remotely piloted aircraft.[25] It was immediately protested by Raytheon's competitors. After re-evaluating the bids following the protests,[26] the US Air Force decided to delay awarding the 3DELRR EMD contract until 2017 and was to issue an amended solicitation at the end of July 2016.[27] In 2017 the Air Force again awarded the contract to Raytheon.[28]\n In May 2015, Raytheon acquired cybersecurity firm Websense, Inc. from Vista Equity Partners for $1.9 billion[29] and combined it with RCP, formerly part of its IIS segment to form Raytheon|Websense.[30] In October 2015, Raytheon|Websense acquired Foreground Security, a provider of security operations centers, managed security service solutions[buzzword] and cybersecurity professional services,[31] for $62 million.[32] In January 2016, Raytheon|Websense acquired the firewall provider Stonesoft from Intel Security for an undisclosed amount and renamed itself to Forcepoint.[33]\n In July 2016, Poland's Defence Minister Antoni Macierewicz planned to sign a letter of intent with Raytheon for a $5.6 billion deal to upgrade its Patriot missile-defence shield,[34][35] and in 2017, Saudi Arabia signed business deals worth billions of dollars with multiple American companies, including Raytheon.[36][37]\n In February 2020, Raytheon completed the first radar antenna array for the US Army's new missile defense radar, known as the Lower Tier Air and Missile Defense Sensor (LTAMDS), to replace the service's Patriot air and missile defense system sensor.[38]\n In 1929, William Boeing's Boeing Airplane & Transport Corporation teamed up with Frederick Rentschler's Pratt & Whitney to form the United Aircraft and Transport Corporation, a large, vertically integrated, amalgamated firm, uniting business interests in all aspects of aviation\u2014a combination of aircraft engine and airframe manufacturing and airline business, to serve all aviation markets, both civil aviation (cargo, passenger, private, air mail) and military aviation.[39] After the Air Mail scandal of 1934, the U.S. government concluded that such large holding companies as United Aircraft and Transport were anti-competitive, and new antitrust laws were passed forbidding airframe or engine manufacturers from having interests in airlines.[40]\n United Aircraft Corporation was formed in 1934 from United Aircraft and Transport's manufacturing interests east of the Mississippi River (Pratt & Whitney, Sikorsky, Vought, and Hamilton Standard Propeller Company), headquartered in Hartford with Frederick Rentschler, founder of Pratt & Whitney, as president.[40][41]\n United Aircraft became a component of the Dow Jones Industrial Average on March 4, 1939, when United Aircraft and AT&T were added to replace Nash Motors and International Business Machines. The company and its successors remained a component of the Dow Jones Industrial Average through August 2020. It was announced that starting August 31, 2020, Raytheon Technologies would be substituted in the Dow Jones Industrial Average by Honeywell International.[42][43]\n During World War II, United Aircraft ranked sixth among United States corporations in the value of wartime production contracts.[16] At the close of the war, United Aircraft entered the emerging markets for jet engines and helicopters, via Pratt & Whitney and Sikorsky, respectively.[40]\n In the 1950s, United Aircraft began developing jet engines, including the Pratt & Whitney J57, the most powerful jet engine on the market for some years.[40] In the 1960s, Pratt & Whitney produced the Pratt & Whitney JT9D for the Boeing 747.[40]\n In 1974, Harry Jack Gray left Litton Industries to become the CEO of United Aircraft.[40] He pursued a strategy of growth and diversification, changing the parent corporation's name to United Technologies Corporation (UTC) in 1975 to reflect the intent to diversify into numerous high tech fields beyond aerospace.[44] (The change became official on May 1, 1975.) The diversification was partially to balance civilian business against any overreliance on military business.[40] UTC became a mergers and acquisitions (M&A)\u2013focused organization, with various forced takeovers of unwilling smaller corporations.[40] The next year (1976), UTC forcibly acquired Otis Elevator.[45] In 1979, Carrier Refrigeration was acquired;[46]\n At one point the military portion of UTC's business, whose sensitivity to \"excess profits\" and boom/bust demand drove UTC to diversify away from it, actually carried the weight of losses incurred by the commercial M&A side of the business.[40] Although M&A activity was not new to United Aircraft, the M&A activity of the 1970s and 1980s was higher-stakes and arguably unfocused. Rather than aviation being the central theme of UTC businesses, high tech (of any type) was the new theme. Some Wall Street watchers questioned the true value of M&A at almost any price, seemingly for its own sake.[40]\n In 1999, UTC acquired Sundstrand Corporation and merged it into UTC's Hamilton Standard unit to form Hamilton Sundstrand. In 2003, UTC entered the fire and security business by purchasing Chubb Security. In 2004, UTC acquired the Schweizer Aircraft Corporation which planned to operate as a wholly owned subsidiary under their Sikorsky Aircraft division.[47] In 2005, UTC further pursued its stake in the fire and security business by purchasing Kidde. Also in 2005, UTC acquired Boeing's Rocketdyne division, which was merged into the Pratt & Whitney business unit and renamed Pratt & Whitney Rocketdyne (later sold to Aerojet and merged into Aerojet Rocketdyne). In November 2008, UTC's Carrier Corporation acquired NORESCO, an energy service company.[48]\n In 2010, UTC conducted its largest acquisition to date, General Electric's security equipment business for US$1.8 billion, a move to support UTC's Fire & Security unit.[49]\n In September 2011, UTC acquired an $18.4 billion deal (including $1.9 billion in net debt assumed) for aircraft components maker Goodrich Corporation.[50] In July 2012, United Technologies acquired Goodrich and merged it with Hamilton Sundstrand, forming UTC Aerospace Systems.\n In November 2018, UTC acquired Rockwell Collins for $23 billion ($30 billion including Rockwell Collins' net debt).[51][52] As part of the deal, Pratt and Whitney and the newly-formed Collins Aerospace remained under United Technologies, while Otis Elevator and UTC Climate, Controls & Security (doing business as Carrier) were spun off as two independent companies.[53] The spin off was completed in March 2020.[54]\n In June 2019, United Technologies announced the intention to merge with the Raytheon Company. The combined company, valued at more than $100 billion after planned spinoffs, would be the world's second-largest aerospace-and-defense company by sales behind Boeing.[55] Although UTC was the legal survivor, the merged company took the name Raytheon Technologies and based its headquarters at Raytheon's former campus in Waltham, Massachusetts, rather than UTC's former base in Farmington, Connecticut.[56] The merger was completed in April 2020.[11] Raytheon Technologies began trading at $51 per share, on the New York Stock Exchange under the ticker RTX.[57][58]\n On July 28, 2020, the company announced cutting of over 8,000 jobs in its commercial aviation division due to travel slowdown induced by the global COVID-19 pandemic.[59]\n In December 2020, the Board of Directors authorized a $5 billion repurchase of common stock.[60]\n In 2022, during the Russian invasion of Ukraine, major arms manufacturers, including Raytheon Technologies,[61] reported a sharp increase in interim sales and profits.[62][63][64]\n On June 7, 2022, the company announced plans to move its global headquarters to Arlington, Virginia.[5] The move was completed in July.[6]\n In January 2023, Raytheon Technologies announced it would combine its missiles and defense division and intelligence and space division into a single business unit, effective July 1. The reorganization created three divisions at Raytheon Technologies: Collins Aerospace, Pratt & Whitney, and Raytheon.[65] The reorganization was preceded by the rebranding to RTX in June 2023.\n In July 2023, Raytheon Technologies Corporation changed its name to RTX Corporation.[4]\n RTX's supply of weapons to Israel led to protests against the company during the 2023 Israel\u2013Hamas war.[66] On December 14, 2023, for example, protestors blocked the entrance to an RTX facility in Arizona.[67] In early 2024, 15 people were arrested after blocking access to RTX and BAE Systems facilities in Louisville, Kentucky in protest against supplying weapons to Israel.[68]\n In December 2023, RTX announced that CEO Greg Hayes would step down the following May and be replaced by company president Christopher Calio.[69]\n In August 2024, RTX was fined US$200 million for International Traffic in Arms Regulations violations, including exchanging data and products with prohibited countries such as China.[70]\n After the 2020 merger, Raytheon Technologies Corporation consisted of four business units:\n In 2023, the company changed its name to RTX Corporation and re-organized into three business units:[71][72]\n RTX Corporation has agreed to pay over $950 million to resolve multiple federal investigations involving bribery, government contracting violations, and export control breaches.[73] [74] The settlement, announced on October 16, 2024, includes penalties for bribing a Qatari official with ties to the country's royal family and defrauding the U.S. Defense Department in procurement contracts. According to the SEC's order, Raytheon used sham subcontracts with a supplier to pay bribes of nearly $2 million to Qatari military and other officials to obtain defense contracts. Additionally, for almost two decades until 2020, Raytheon paid more than $30 million to a Qatari agent related to the Qatari Emir, despite the agent lacking a background in defense contracting. The second agreement, made with DOJ officials in Boston, involves RTX paying $574 million to settle allegations of overcharging in federal contracts. This includes schemes to defraud the U.S. Department of Defense in connection with the provision of defense products and services, such as PATRIOT missile systems and radar systems intended for an undisclosed foreign customer. [75] As part of the settlement, RTX will also pay a $124 million penalty to the Securities and Exchange Commission. The company has acknowledged responsibility for the misconduct, which largely occurred prior to 2020, and has stated that it is working on remediation efforts. [76]\n \n"
    },
    {
        "title": "Women in science",
        "url": "https://en.wikipedia.org/wiki/Women_in_science",
        "content": "\n The presence of women in science spans the earliest times of the history of science wherein they have made significant contributions. Historians with an interest in gender and science have researched the scientific endeavors and accomplishments of women, the barriers they have faced, and the strategies implemented to have their work peer-reviewed and accepted in major scientific journals and other publications. The historical, critical, and sociological study of these issues has become an academic discipline in its own right.\n The involvement of women in medicine occurred in several early Western civilizations, and the study of natural philosophy in ancient Greece was open to women. Women contributed to the proto-science of alchemy in the first or second centuries CE During the Middle Ages, religious convents were an important place of education for women, and some of these communities provided opportunities for women to contribute to scholarly research. The 11th century saw the emergence of the first universities; women were, for the most part, excluded from university education.[1] Outside academia, botany was the science that benefitted most from the contributions of women in early modern times.[2] The attitude toward educating women in medical fields appears to have been more liberal in Italy than elsewhere. The first known woman to earn a university chair in a scientific field of studies was eighteenth-century Italian scientist Laura Bassi.\n Gender roles were largely deterministic in the eighteenth century and women made substantial advances in science.  During the nineteenth century, women were excluded from most formal scientific education, but they began to be admitted into learned societies during this period. In the later nineteenth century, the rise of the women's college provided jobs for women scientists and opportunities for education. Marie Curie paved the way for scientists to study radioactive decay and discovered the elements radium and polonium.[3] Working as a physicist and chemist, she conducted pioneering research on radioactive decay and was the first woman to receive a Nobel Prize in Physics and became the first person to receive a second Nobel Prize in Chemistry. Sixty women have been awarded the Nobel Prize between 1901 and 2022. Twenty-four women have been awarded the Nobel Prize in physics, chemistry, physiology or medicine.[4]\n In the 1970s and 1980s, many books and articles about women scientists were appearing; virtually all of the published sources ignored women of color and women outside of Europe and North America.[5] The formation of the Kovalevskaia Fund in 1985 and the Organization for Women in Science for the Developing World in 1993 gave more visibility to previously marginalized women scientists, but even today there is a dearth of information about current and historical women in science in developing countries. According to academic Ann Hibner Koblitz:[6]\n Most work on women scientists has focused on the personalities and scientific subcultures of Western Europe and North America, and historians of women in science have implicitly or explicitly assumed that the observations made for those\nregions will hold true for the rest of the world. Koblitz has said that these generalizations about women in science often do not hold up cross-culturally:[7]\n A scientific or technical field that might be considered 'unwomanly' in one country in a given period may enjoy the participation of many women in a different historical period or in another country.  An example is engineering, which in many countries is considered the exclusive domain of men, especially in usually prestigious subfields such as electrical or mechanical engineering.  There are exceptions to this, however.  In the former Soviet Union all subspecialties of engineering had high percentages of women, and at the Universidad Nacional de Ingenier\u00eda of Nicaragua, women made up 70% of engineering students in 1990. The involvement of women in the field of medicine has been recorded in several early civilizations. An ancient Egyptian physician, Peseshet (c.\u20092600\u20132500 B.C.E.), described in an inscription as \"lady overseer of the female physicians\",[8][9] is the earliest known female physician named in the history of science.[10] Agamede was cited by Homer as a healer in ancient Greece before the Trojan War (c. 1194\u20131184 BCE).[11][12][13] According to one late antique legend, Agnodice was the first female physician to practice legally in fourth century BCE Athens.[14]\n The study of natural philosophy in ancient Greece was open to women. Recorded examples include Aglaonike, who predicted eclipses; and Theano, mathematician and physician, who was a pupil (possibly also wife) of Pythagoras, and one of a school in Crotone founded by Pythagoras, which included many other women.[15] A passage in Pollux speaks about those who invented the process of coining money mentioning Pheidon and Demodike from Cyme, wife of the Phrygian king, Midas, and daughter of King Agamemnon of Cyme.[16] A daughter of a certain Agamemnon, king of Aeolian Cyme, married a Phrygian king called Midas.[17] This link may have facilitated the Greeks \"borrowing\" their alphabet from the Phrygians because the Phrygian letter shapes are closest to the inscriptions from Aeolis.[17]\n During the period of the Babylonian civilization, around 1200 BCE, two perfumeresses named Tapputi-Belatekallim and -ninu (first half of her name unknown) were able to obtain the essences from plants by using extraction and distillation procedures.[18] During the Egyptian dynasty, women were involved in applied chemistry, such as the making of beer and the preparation of medicinal compounds.[19] Women have been recorded to have made major contributions to alchemy.[19] Many of which lived in Alexandria around the 1st or 2nd centuries C.E., where the gnostic tradition led to female contributions being valued. The most famous of the women alchemist, Mary the Jewess, is credited with inventing several chemical instruments, including the double boiler (bain-marie); the improvement or creation of distillation equipment of that time.[19][20] Such distillation equipment were called kerotakis (simple still) and the tribikos (a complex distillation device).[19]\n Hypatia of Alexandria (c. 350\u2013415 CE), daughter of Theon of Alexandria, was a philosopher, mathematician, and astronomer.[21][22] She is the earliest female mathematician about whom detailed information has survived.[22] Hypatia is credited with writing several important commentaries on geometry, algebra and astronomy.[15][23] Hypatia was the head of a philosophical school and taught many students.[24] In 415 CE, she became entangled in a political dispute between Cyril, the bishop of Alexandria, and Orestes, the Roman governor, which resulted in a mob of Cyril's supporters stripping her, dismembering her, and burning the pieces of her body.[24]\n \nThe early parts of the European Middle Ages, also known as the Dark Ages, were marked by the decline of the Roman Empire. The Latin West was left with great difficulties that affected the continent's intellectual production dramatically. Although nature was still seen as a system that could be comprehended in the light of reason, there was little innovative scientific inquiry.[25] The Arabic world deserves credit for preserving scientific advancements. Arabic scholars produced original scholarly work and generated copies of manuscripts from Classical periods.[26] During this period, Christianity underwent a period of resurgence, and Western civilization was bolstered as a result. This phenomenon was, in part, due to monasteries and nunneries that nurtured the skills of reading and writing, and the monks and nuns who collected and copied important writings produced by scholars of the past.[26][citation needed] As it mentioned before, convents were an important place of education for women during this period, for the monasteries and nunneries encourage the skills of reading and writing, and some of these communities provided opportunities for women to contribute to scholarly research.[26] An example is the German abbess Hildegard of Bingen (1098\u20131179 A.D), a famous philosopher and botanist, whose prolific writings include treatments of various scientific subjects, including medicine, botany and natural history (c. 1151\u201358).[27] Another famous German abbess was Hroswitha of Gandersheim (935\u20131000 A.D.)[26] that also helped encourage women to be intellectual. However, with the growth in number and power of nunneries, the all-male clerical hierarchy was not welcomed toward it, and thus it stirred up conflict by having backlash against women's advancement. That impacted many religious orders closed on women and disbanded their nunneries, and overall excluding women from the ability to learn to read and write. With that, the world of science became closed off to women, limiting women's influence in science.[26]\n Entering the 11th century, the first universities emerged. Women were, for the most part, excluded from university education.[1] However, there were some exceptions. The Italian University of Bologna allowed women to attend lectures from its inception, in 1088.[28]\n The attitude to educating women in medical fields in Italy appears to have been more liberal than in other places. The physician, Trotula di Ruggiero, is supposed to have held a chair at the Medical School of Salerno in the 11th century, where she taught many noble Italian women, a group sometimes referred to as the \"ladies of Salerno\".[20] Several influential texts on women's medicine, dealing with obstetrics and gynecology, among other topics, are also often attributed to Trotula.\n Dorotea Bucca was another distinguished Italian physician. She held a chair of philosophy and medicine at the University of Bologna for over forty years from 1390.[28][29][self-published source?][30][31] Other Italian women whose contributions in medicine have been recorded include Abella, Jacobina F\u00e9licie, Alessandra Giliani, Rebecca de Guarna, Margarita, Mercuriade (14th century), Constance Calenda, Calrice di Durisio (15th century), Constanza, Maria Incarnata and Thomasia de Mattio.[29][32]\n Despite the success of some women, cultural biases affecting their education and participation in science were prominent in the Middle Ages. For example, Saint Thomas Aquinas, a Christian scholar, wrote, referring to women, \"She is mentally incapable of holding a position of authority.\"[1]\n Margaret Cavendish, a seventeenth-century aristocrat, took part in some of the most important scientific debates of that time. She was, however, not inducted into the English Royal Society, although she was once allowed to attend a meeting. She wrote a number of works on scientific matters, including Observations upon Experimental Philosophy (1666) and Grounds of Natural Philosophy. In these works she was especially critical of the growing belief that humans, through science, were the masters of nature. The 1666 work attempted to heighten female interest in science. The observations provided a critique of the experimental science of Bacon and criticized microscopes as imperfect machines.[33]\n Isabella Cortese, an Italian alchemist, is most known for her book I secreti della signora Isabella Cortese or The Secrets of Isabella Cortese. Cortese was able to manipulate nature in order to create several medicinal, alchemy and cosmetic \"secrets\" or experiments.[34] Isabella's book of secrets belongs to a larger book of secrets that became extremely popular among the elite during the 16th century. Despite the low percentage of literate women during Cortese's era, the majority of alchemical and cosmetic \"secrets\" in the book of secrets were geared towards women. This included but was not limited to pregnancy, fertility, and childbirth.[34]\n Sophia Brahe, sister of Tycho Brahe, was a Danish Horticulturalist. Brahe was trained by her older brother in chemistry and horticulture but taught herself astronomy by studying books in German. Sophia visited her brother in the Uranienborg on numerous occasions and assisted on his project the De nova stella. Her observations lead to the discovery of the Supernova SN 1572 which helped refute the geocentric model of the universe.[35]\n Tycho Wrote the Urania Titani about his sister Sophia and her husband Erik. The Urania presented Sophia and the Titan represented Erik. Tycho used this poem in order to show his appreciation for his sister and all of her work.\n In Germany, the tradition of female participation in craft production enabled some women to become involved in observational science, especially astronomy. Between 1650 and 1710, women were 14% of German astronomers.[36] The most famous female astronomer in Germany was Maria Winkelmann. She was educated by her father and uncle and received training in astronomy from a nearby self-taught astronomer. Her chance to be a practising astronomer came when she married Gottfried Kirch, Prussia's foremost astronomer. She became his assistant at the astronomical observatory operated in Berlin by the Academy of Science. She made original contributions, including the discovery of a comet. When her husband died, Winkelmann applied for a position as assistant astronomer at the Berlin Academy \u2013 for which she had experience. As a woman \u2013 with no university degree \u2013 she was denied the post. Members of the Berlin Academy feared that they would establish a bad example by hiring a woman. \"Mouths would gape\", they said.[37]\n Winkelmann's problems with the Berlin Academy reflect the obstacles women faced in being accepted in scientific work, which was considered to be chiefly for men. No woman was invited to either the Royal Society of London nor the French Academy of Sciences until the twentieth century. Most people in the seventeenth century viewed a life devoted to any kind of scholarship as being at odds with the domestic duties women were expected to perform.\n A founder of modern botany and zoology, the German Maria Sibylla Merian (1647\u20131717), spent her life investigating nature. When she was thirteen, Sibylla began growing caterpillars and studying their metamorphosis into butterflies. She kept a \"Study Book\" which recorded her investigations into natural philosophy. In her first publication, The New Book of Flowers, she used imagery to catalog the lives of plants and insects. After her husband died, and her brief stint of living in Siewert, she and her daughter journeyed to Paramaribo for two years to observe insects, birds, reptiles, and amphibians.[38] She returned to Amsterdam and published The Metamorphosis of the Insects of Suriname, which \"revealed to Europeans for the first time the astonishing diversity of the rain forest.\"[39][40] She was a botanist and entomologist who was known for her artistic illustrations of plants and insects. Uncommon for that era, she traveled to South America and Surinam, where, assisted by her daughters, she illustrated the plant and animal life of those regions.[41]\n Overall, the Scientific Revolution did little to change people's ideas about the nature of women \u2013 more specifically \u2013 their capacity to contribute to science just as men do. According to Jackson Spielvogel, 'Male scientists used the new science to spread the view that women were by nature inferior and subordinate to men and suited to play a domestic role as nurturing mothers. The widespread distribution of books ensured the continuation of these ideas'.[42]\n Although women excelled in many scientific areas during the eighteenth century, they were discouraged from learning about plant reproduction. Carl Linnaeus' system of plant classification based on sexual characteristics drew attention to botanical licentiousness, and people feared that women would learn immoral lessons from nature's example. Women were often depicted as both innately emotional and incapable of objective reasoning, or as natural mothers reproducing a natural, moral society.[43]\n The eighteenth century was characterized by three divergent views towards women: that women were mentally and socially inferior to men, that they were equal but different, and that women were potentially equal in both mental ability and contribution to society.[44] While individuals such as Jean-Jacques Rousseau believed women's roles were confined to motherhood and service to their male partners, the Enlightenment was a period in which women experienced expanded roles in the sciences.[45]\n The rise of salon culture in Europe brought philosophers and their conversation to an intimate setting where men and women met to discuss contemporary political, social, and scientific topics.[46] While Jean-Jacques Rousseau attacked women-dominated salons as producing 'effeminate men' that stifled serious discourse, salons were characterized in this era by the mixing of the sexes.[47]\n Lady Mary Wortley Montagu defied convention by introducing smallpox inoculation through variolation to Western medicine after witnessing it during her travels in the Ottoman Empire.[48][49] In 1718 Wortley Montague had her son inoculated[49] and when in 1721 a smallpox epidemic struck England, she had her daughter inoculated.[50] This was the first such operation done in Britain.[49] She persuaded Caroline of Ansbach to test the treatment on prisoners.[50] Princess Caroline subsequently inoculated her two daughters in 1722.[49] Under a pseudonym, Wortley Montague published an article describing and advocating in favor of inoculation in September 1722.[51]\n After publicly defending forty nine theses[52] in the Palazzo Pubblico, Laura Bassi was awarded a doctorate of philosophy in 1732 at the University of Bologna.[53] Thus, Bassi became the second woman in the world to earn a philosophy doctorate after Elena Cornaro Piscopia in 1678, 54 years prior. She subsequently defended twelve additional theses at the Archiginnasio, the main building of the University of Bologna which allowed her to petition for a teaching position at the university.[53] In 1732 the university granted Bassi's professorship in philosophy, making her a member of the Academy of the Sciences and the first woman to earn a professorship in physics at a university in Europe[53] But the university held the value that women were to lead a private life and from 1746 to 1777 she gave only one formal dissertation per year ranging in topic from the problem of gravity to electricity.[52] Because she could not lecture publicly at the university regularly, she began conducting private lessons and experiments from home in the year of 1749.[52] However, due to her increase in responsibilities and public appearances on behalf of the university, Bassi was able to petition for regular pay increases, which in turn was used to pay for her advanced equipment. Bassi earned the highest salary paid by the University of Bologna of 1,200 lire.[54] In 1776, at the age of 65, she was appointed to the chair in experimental physics by the Bologna Institute of Sciences with her husband as a teaching assistant.[52]\n According to Britannica, Maria Gaetana Agnesi is \"considered to be the first woman in the Western world to have achieved a reputation in mathematics.\"[55] She is credited as the first woman to write a mathematics handbook, the Instituzioni analitiche ad uso della giovent\u00f9 italiana, (Analytical Institutions for the Use of Italian Youth). Published in 1748 it \"was regarded as the best introduction extant to the works of Euler.\"[56][57] The goal of this work was, according to Agnesi herself, to give a systematic illustration of the different results and theorems of infinitesimal calculus.[58] In 1750 she became the second woman to be granted a professorship at a European university. Also appointed to the University of Bologna she never taught there.[56][59]\n The German Dorothea Erxleben was instructed in medicine by her father from an early age[60] and Bassi's university professorship inspired Erxleben to fight for her right to practise medicine. In 1742 she published a tract arguing that women should be allowed to attend university.[61] After being admitted to study by a dispensation of Frederick the Great,[60] Erxleben received her M.D. from the University of Halle in 1754.[61] She went on to analyse the obstacles preventing women from studying, among them housekeeping and children.[60] She became the first female medical doctor in Germany.[62]\n In 1741\u201342 Charlotta Fr\u00f6lich became the first woman to be published by the Royal Swedish Academy of Sciences with three books in agricultural science. In 1748 Eva Ekeblad became the first woman inducted into that academy.[63] In 1746 Ekeblad had written to the academy about her discoveries of how to make flour and alcohol out of potatoes.[64][65] Potatoes had been introduced into Sweden in 1658 but had been cultivated only in the greenhouses of the aristocracy. Ekeblad's work turned potatoes into a staple food in Sweden, and increased the supply of wheat, rye and barley available for making bread, since potatoes could be used instead to make alcohol. This greatly improved the country's eating habits and reduced the frequency of famines.[65] Ekeblad also discovered a method of bleaching cotton textile and yarn with soap in 1751,[64] and of replacing the dangerous ingredients in cosmetics of the time by using potato flour in 1752.[65]\n \u00c9milie du Ch\u00e2telet, a close friend of Voltaire, was the first scientist to appreciate the significance of kinetic energy, as opposed to momentum. She repeated and described the importance of an experiment originally devised by Willem 's Gravesande showing the impact of falling objects is proportional not to their velocity, but to the velocity squared. This understanding is considered to have made a profound contribution to Newtonian mechanics.[66] In 1749 she completed the French translation of Newton's Philosophiae Naturalis Principia Mathematica (the Principia), including her derivation of the notion of conservation of energy from its principles of mechanics. Published ten years after her death, her translation and commentary of the Principia contributed to the completion of the scientific revolution in France and to its acceptance in Europe.[67]\n Marie-Anne Pierrette Paulze and her husband Antoine Lavoisier rebuilt the field of chemistry, which had its roots in alchemy and at the time was a convoluted science dominated by George Stahl's theory of phlogiston. Paulze accompanied Lavoisier in his lab, making entries into lab notebooks and sketching diagrams of his experimental designs. The training she had received allowed her to accurately and precisely draw experimental apparatuses, which ultimately helped many of Lavoisier's contemporaries to understand his methods and results. Paulze translated various works about phlogiston into French. One of her most important translation was that of Richard Kirwan's Essay on Phlogiston and the Constitution of Acids, which she both translated and critiqued, adding footnotes as she went along and pointing out errors in the chemistry made throughout the paper.[68] Paulze was instrumental in the 1789 publication of Lavoisier's Elementary Treatise on Chemistry, which presented a unified view of chemistry as a field. This work proved pivotal in the progression of chemistry, as it presented the idea of conservation of mass as well as a list of elements and a new system for chemical nomenclature. She also kept strict records of the procedures followed, lending validity to the findings Lavoisier published.\n The astronomer Caroline Herschel was born in Hanover but moved to England where she acted as an assistant to her brother, William Herschel. Throughout her writings, she repeatedly made it clear that she desired to earn an independent wage and be able to support herself. When the crown began paying her for her assistance to her brother in 1787, she became the first woman to do so at a time when even men rarely received wages for scientific enterprises\u00a0\u2013 to receive a salary for services to science.[69] During 1786\u201397 she discovered eight comets, the first on 1 August 1786. She had unquestioned priority as discoverer of five of the comets[69][70] and rediscovered Comet Encke in 1795.[71] Five of her comets were published in Philosophical Transactions, a packet of paper bearing the superscription, \"This is what I call the Bills and Receipts of my Comets\" contains some data connected with the discovery of each of these objects. William was summoned to Windsor Castle to demonstrate Caroline's comet to the royal family.[72] Caroline Herschel is often credited as the first woman to discover a comet; however, Maria Kirch discovered a comet in the early 1700s, but is often overlooked because at the time, the discovery was attributed to her husband, Gottfried Kirch.[73]\n Science remained a largely amateur profession during the early part of the nineteenth century. Botany was considered a popular and fashionable activity, and one particularly suitable to women. In the later eighteenth and early nineteenth centuries, it was one of the most accessible areas of science for women in both England and North America.[74][75][76]\n However, as the nineteenth century progressed, botany and other sciences became increasingly professionalized, and women were increasingly excluded.  Women's contributions were limited by their exclusion from most formal scientific education, but began to be recognized through their occasional admittance into learned societies during this period.[76][74]\n Scottish scientist Mary Fairfax Somerville carried out experiments in magnetism, presenting a paper entitled 'The Magnetic Properties of the Violet Rays of the Solar Spectrum' to the Royal Society in 1826, the second woman to do so. She also wrote several mathematical, astronomical, physical and geographical texts, and was a strong advocate for women's education. In 1835, she and Caroline Herschel were the first two women elected as Honorary Members of the Royal Astronomical Society.[77]\n English mathematician Ada, Lady Lovelace, a pupil of Somerville, corresponded with Charles Babbage about applications for his analytical engine. In her notes (1842\u201343) appended to her translation of Luigi Menabrea's article on the engine, she foresaw wide applications for it as a general-purpose computer, including composing music. She has been credited as writing the first computer program, though this has been disputed.[78]\n In Germany, institutes for \"higher\" education of women (H\u00f6here M\u00e4dchenschule, in some regions called Lyzeum) were founded at the beginning of the century.[79] The Deaconess Institute at Kaiserswerth was established in 1836 to instruct women in nursing. Elizabeth Fry visited the institute in 1840 and was inspired to found the London Institute of Nursing, and Florence Nightingale studied there in 1851.[80]\n In the US, Maria Mitchell made her name by discovering a comet in 1847, but also contributed calculations to the Nautical Almanac produced by the United States Naval Observatory. She became the first woman member of the American Academy of Arts and Sciences in 1848 and of the American Association for the Advancement of Science in 1850.\n Other notable female scientists during this period include:[15]\n The latter part of the 19th century saw a rise in educational opportunities for women. Schools aiming to provide education for girls similar to that afforded to boys were founded in the UK, including the North London Collegiate School (1850), Cheltenham Ladies' College (1853) and the Girls' Public Day School Trust schools (from 1872). The first UK women's university college, Girton, was founded in 1869, and others soon followed: Newnham (1871) and Somerville (1879).\n The Crimean War (1854\u20131856) contributed to establishing nursing as a profession, making Florence Nightingale a household name. A public subscription allowed Nightingale to establish a school of nursing in London in 1860, and schools following her principles were established throughout the UK.[80] Nightingale was also a pioneer in public health as well as a statistician.\n James Barry became the first British woman to gain a medical qualification in 1812, passing as a man. Elizabeth Garrett Anderson was the first openly female Briton to qualify medically, in 1865. With Sophia Jex-Blake, American Elizabeth Blackwell and others, Garret Anderson founded the first UK medical school to train women, the London School of Medicine for Women, in 1874.\n Annie Scott Dill Maunder was a pioneer in astronomical photography, especially of sunspots. A mathematics graduate of Girton College, Cambridge, she was first hired (in 1890) to be an assistant to Edward Walter Maunder, discoverer of the Maunder Minimum, the head of the solar department at Greenwich Observatory. They worked together to observe sunspots and to refine the techniques of solar photography. They married in 1895. Annie's mathematical skills made it possible to analyse the years of sunspot data that Maunder had been collecting at Greenwich. She also designed a small, portable wide-angle camera with a 1.5-inch-diameter (38\u00a0mm) lens. In 1898, the Maunders traveled to India, where Annie took the first photographs of the Sun's corona during a solar eclipse. By analysing the Cambridge records for both sunspots and geomagnetic storm, they were able to show that specific regions of the Sun's surface were the source of geomagnetic storms and that the Sun did not radiate its energy uniformly into space, as William Thomson, 1st Baron Kelvin had declared.[81]\n In Prussia women could go to university from 1894 and were allowed to receive a PhD. In 1908 all remaining restrictions for women were terminated.\n Alphonse Rebi\u00e8re published a book in 1897, in France, entitled Les Femmes dans la science (Women in Science) which listed the contributions and publications of women in science.[82]\n Other notable female scientists during this period include:[15][83]\n In the second half of the 19th century, a large proportion of the most successful women in the STEM fields were Russians.  Although many women received advanced training in medicine in the 1870s,[84] in other fields women were barred and had to go to western Europe\u00a0\u2013 mainly Switzerland\u00a0\u2013 in order to pursue scientific studies.  In her book about these \"women of the [eighteen] sixties\" (\u0448\u0435\u0441\u0442\u0438\u0434\u0435\u0441\u044f\u0442\u043d\u0438\u0446\u044b), as they were called, Ann Hibner Koblitz writes:[85]:\u200a11\u200a\n To a large extent, women's higher education in continental Europe was pioneered by this first generation of Russian women.  They were the first students in Z\u00fcrich, Heidelberg, Leipzig, and elsewhere.  Theirs were the first doctorates in medicine, chemistry, mathematics, and biology.\n Among the successful scientists were Nadezhda Suslova (1843\u20131918), the first woman in the world to obtain a medical doctorate fully equivalent to men's degrees; Maria Bokova-Sechenova (1839\u20131929), a pioneer of women's medical education who received two doctoral degrees, one in medicine in Z\u00fcrich and one in physiology in Vienna; Iulia Lermontova (1846\u20131919), the first woman in the world to receive a doctoral degree in chemistry; the marine biologist Sofia Pereiaslavtseva (1849\u20131903), director of the Sevastopol Biological Station and winner of the Kessler Prize of the Russian Society of Natural Scientists; and the mathematician Sofia Kovalevskaia (1850\u20131891), the first woman in 19th century Europe to receive a doctorate in mathematics and the first to become a university professor in any field.[85]\n In the later nineteenth century the rise of the women's college provided jobs for women scientists, and opportunities for education.\n Women's colleges produced a disproportionate number of women who went on for PhDs in science. \nMany coeducational colleges and universities also opened or started to admit women during this period; such institutions included just over 3000 women in 1875, by 1900 numbered almost 20,000.[83]\n An example is Elizabeth Blackwell, who became the first certified female doctor in the US when she graduated from Geneva Medical College in 1849.[86] With her sister, Emily Blackwell, and Marie Zakrzewska, Blackwell founded the New York Infirmary for Women and Children in 1857 and the first women's medical college in 1868, providing both training and clinical experience for women doctors. She also published several books on medical education for women.\n In 1876, Elizabeth Bragg became the first woman to graduate with a civil engineering degree in the United States, from the University of California, Berkeley.[87]\n Marie Sk\u0142odowska-Curie, the first woman to win a Nobel prize in 1903 (physics), went on to become a double Nobel prize winner in 1911, both for her work on radiation. She was the first person to win two Nobel prizes, a feat accomplished by only three others since then. She also was the first woman to teach at Sorbonne University in Paris.[88]\n Alice Perry is understood to be the first woman to graduate with a degree in civil engineering in the then United Kingdom of Great Britain and Ireland, in 1906 at Queen's College, Galway, Ireland.[89]\n Lise Meitner played a major role in the discovery of nuclear fission. As head of the physics section at the Kaiser Wilhelm Institute in Berlin she collaborated closely with the head of chemistry Otto Hahn on atomic physics until forced to flee Berlin in 1938. In 1939, in collaboration with her nephew Otto Frisch, Meitner derived the theoretical explanation for an experiment performed by Hahn and Fritz Strassman in Berlin, thereby demonstrating the occurrence of nuclear fission. The possibility that Fermi's bombardment of uranium with neutrons in 1934 had instead produced fission by breaking up the nucleus into lighter elements, had actually first been raised in print in 1934, by chemist Ida Noddack (co-discover of the element rhenium), but this suggestion had been ignored at the time, as no group made a concerted effort to find any of these light radioactive fission products.\n Maria Montessori was the first woman in Southern Europe to qualify as a physician.[90] She developed an interest in the diseases of children and believed in the necessity of educating those recognized to be ineducable. In the case of the latter she argued for the development of training for teachers along Froebelian lines and developed the principle that was also to inform her general educational program, which is the first the education of the senses, then the education of the intellect. Montessori introduced a teaching program that allowed defective children to read and write. She sought to teach skills not by having children repeatedly try it, but by developing exercises that prepare them.[91]\n Emmy Noether revolutionized abstract algebra, filled in gaps in relativity, and was responsible for a critical theorem about conserved quantities in physics. One notes that the Erlangen program attempted to identify invariants under a group of transformations. On 16 July 1918, before a scientific organization in G\u00f6ttingen, Felix Klein read a paper written by Emmy Noether, because she was not allowed to present the paper herself. In particular, in what is referred to in physics as Noether's theorem, this paper identified the conditions under which the Poincar\u00e9 group of transformations (now called a gauge group) for general relativity defines conservation laws.[92] Noether's papers made the requirements for the conservation laws precise. Among mathematicians, Noether is best known for her fundamental contributions to abstract algebra, where the adjective noetherian is nowadays commonly used on many sorts of objects.\n Mary Cartwright was a British mathematician who was the first to analyze a dynamical system with chaos.[93] Inge Lehmann, a Danish seismologist, first suggested in 1936 that inside the Earth's molten core there may be a solid inner core.[94] Women such as Margaret Fountaine continued to contribute detailed observations and illustrations in botany, entomology, and related observational fields. Joan Beauchamp Procter, an outstanding herpetologist, was the first woman Curator of Reptiles for the Zoological Society of London at London Zoo.\n Florence Sabin was an American medical scientist. Sabin was the first woman faculty member at Johns Hopkins in 1902, and the first woman full-time professor there in 1917.[95] Her scientific and research experience is notable. Sabin published over 100 scientific papers and multiple books.[95]\n Women moved into science in significant numbers by 1900, helped by the women's colleges and by opportunities at some of the new universities. Margaret Rossiter's books Women Scientists in America: Struggles and Strategies to 1940 and Women Scientists in America: Before Affirmative Action 1940\u20131972 provide an overview of this period, stressing the opportunities women found in separate women's work in science.[96][97]\n In 1892, Ellen Swallow Richards called for the \"christening of a new science\" \u2013 \"oekology\" (ecology) in a Boston lecture. This new science included the study of \"consumer nutrition\" and environmental education. This interdisciplinary branch of science was later specialized into what is currently known as ecology, while the consumer nutrition focus split off and was eventually relabeled as home economics,[98][99] which provided another avenue for women to study science. Richards helped to form the American Home Economics Association, which published a journal, the Journal of Home Economics, and hosted conferences. Home economics departments were formed at many colleges, especially at land grant institutions. In her work at MIT, Ellen Richards also introduced the first biology course in its history as well as the focus area of sanitary engineering.\n Women also found opportunities in botany and embryology. In psychology, women earned doctorates but were encouraged to specialize in educational and child psychology and to take jobs in clinical settings, such as hospitals and social welfare agencies.\n In 1901, Annie Jump Cannon first noticed that it was a star's temperature that was the principal distinguishing feature among different spectra.[dubious \u2013 discuss] This led to re-ordering of the ABC types by temperature instead of hydrogen absorption-line strength. Due to Cannon's work, most of the then-existing classes of stars were thrown out as redundant. Afterward, astronomy was left with the seven primary classes recognized today, in order: O, B, A, F, G, K, M;[100] that has since been extended.\n Henrietta Swan Leavitt first published her study of variable stars in 1908. This discovery became known as the \"period-luminosity relationship\" of Cepheid variables.[102] Our picture of the universe was changed forever, largely because of Leavitt's discovery.\n The accomplishments of Edwin Hubble, renowned American astronomer, were made possible by Leavitt's groundbreaking research and Leavitt's Law. \"If Henrietta Leavitt had provided the key to determine the size of the cosmos, then it was Edwin Powell Hubble who inserted it in the lock and provided the observations that allowed it to be turned\", wrote David H. and Matthew D.H. Clark in their book Measuring the Cosmos.[103]\n Hubble often said that Leavitt deserved the Nobel for her work.[104] G\u00f6sta Mittag-Leffler of the Swedish Academy of Sciences had begun paperwork on her nomination in 1924, only to learn that she had died of cancer three years earlier[105] (the Nobel prize cannot be awarded posthumously).\n In 1925, Harvard graduate student Cecilia Payne-Gaposchkin demonstrated for the first time from existing evidence on the spectra of stars that stars were made up almost exclusively of hydrogen and helium, one of the most fundamental theories in stellar astrophysics.[100][102]\n Canadian-born Maud Menten worked in the US and Germany.  Her most famous work was on enzyme kinetics together with Leonor Michaelis, based on earlier findings of Victor Henri.  This resulted in the Michaelis\u2013Menten equations. Menten also invented the azo-dye coupling reaction for alkaline phosphatase, which is still used in histochemistry. She characterised bacterial toxins from B. paratyphosus, Streptococcus scarlatina and Salmonella ssp., and conducted the first electrophoretic separation of proteins in 1944. She worked on the properties of hemoglobin, regulation of blood sugar level, and kidney function.\n World War II brought some new opportunities. The Office of Scientific Research and Development, under Vannevar Bush, began in 1941 to keep a registry of men and women trained in the sciences. Because there was a shortage of workers, some women were able to work in jobs they might not otherwise have accessed. Many women worked on the Manhattan Project or on scientific projects for the United States military services. Women who worked on the Manhattan Project included Leona Woods Marshall, Katharine Way, and Chien-Shiung Wu. It was actually Wu who confirmed Enrico Fermi's hypothesis through her earlier draft that Xe-135 impeded the B reactor from working. The adjustments made would quickly let the project resume its course.[106][107]\n Wu would later also confirm Albert Einstein's EPR Paradox in the first experimental corroboration, and prove the first violation of Parity and Charge Conjugate Symmetry, thereby laying the conceptual basis for the future Standard model of Particle Physics, and the rapid development of the new field.[108]\n Women in other disciplines looked for ways to apply their expertise to the war effort. Three nutritionists, Lydia J. Roberts, Hazel K. Stiebeling, and Helen S. Mitchell, developed the Recommended Dietary Allowance in 1941 to help military and civilian groups make plans for group feeding situations. The RDAs proved necessary, especially, once foods began to be rationed. Rachel Carson worked for the United States Bureau of Fisheries, writing brochures to encourage Americans to consume a wider variety of fish and seafood. She also contributed to research to assist the Navy in developing techniques and equipment for submarine detection.\n Women in psychology formed the National Council of Women Psychologists, which organized projects related to the war effort. The NCWP elected Florence Laura Goodenough president. In the social sciences, several women contributed to the Japanese Evacuation and Resettlement Study, based at the University of California. This study was led by sociologist Dorothy Swaine Thomas, who directed the project and synthesized information from her informants, mostly graduate students in anthropology. These included Tamie Tsuchiyama, the only Japanese-American woman to contribute to the study, and Rosalie Hankey Wax.\n In the United States Navy, female scientists conducted a wide range of research. Mary Sears, a planktonologist, researched military oceanographic techniques as head of the Hydgrographic Office's Oceanographic Unit. Florence van Straten, a chemist, worked as an aerological engineer. She studied the effects of weather on military combat. Grace Hopper, a mathematician, became one of the first computer programmers for the Mark I computer. Mina Spiegel Rees, also a mathematician, was the chief technical aide for the Applied Mathematics Panel of the National Defense Research Committee.\n Gerty Cori was a biochemist who discovered the mechanism by which glycogen, a derivative of glucose, is transformed in the muscles to form lactic acid, and is later reformed as a way to store energy. For this discovery she and her colleagues were awarded the Nobel prize in 1947, making her the third woman and the first American woman to win a Nobel Prize in science. She was the first woman ever to be awarded the Nobel Prize in Physiology or Medicine. Cori is among several scientists whose works are commemorated by a U.S. postage stamp.[109]\n Nina Byers notes that before 1976, fundamental contributions of women to physics were rarely acknowledged. Women worked unpaid or in positions lacking the status they deserved. That imbalance is gradually being redressed.[citation needed]\n In the early 1980s, Margaret Rossiter presented two concepts for understanding the statistics behind women in science as well as the disadvantages women continued to suffer. She coined the terms \"hierarchical segregation\" and \"territorial segregation.\" The former term describes the phenomenon in which the further one goes up the chain of command in the field, the smaller the presence of women. The latter describes the phenomenon in which women \"cluster in scientific disciplines.\"[110]:\u200a33\u201334\u200a\n A recent book titled Athena Unbound provides a life-course analysis (based on interviews and surveys) of women in science from early childhood interest, through university, graduate school and the academic workplace. The thesis of this book is that \"Women face a special series of gender related barriers to entry and success in scientific careers that persist, despite recent advances\".[111]\n The L'Or\u00e9al-UNESCO Awards for Women in Science were set up in 1998, with prizes alternating each year between the materials science and life sciences. One award is given for each geographical region of Africa and the Middle East, Asia-Pacific, Europe, Latin America and the Caribbean, and North America. By 2017, these awards had recognised almost 100 laureates from 30 countries.  Two of the laureates have gone on to win the Nobel Prize, Ada Yonath (2008) and Elizabeth Blackburn (2009). Fifteen promising young researchers also receive an International Rising Talent fellowship each year within this programme.\n South-African born physicist and radiobiologist Tikvah Alper(1909\u201395), working in the UK, developed many fundamental insights into biological mechanisms, including the (negative) discovery that the infective agent in scrapie could not be a virus or other eukaryotic structure.\n French virologist Fran\u00e7oise Barr\u00e9-Sinoussi performed some of the fundamental work in the identification of the human immunodeficiency virus (HIV) as the cause of AIDS, for which she shared the 2008 Nobel Prize in Physiology or Medicine.\n In July 1967, Jocelyn Bell Burnell discovered evidence for the first known radio pulsar, which resulted in the 1974 Nobel Prize in Physics for her supervisor. She was president of the Institute of Physics from October 2008 until October 2010.\n Astrophysicist Margaret Burbidge was a member of the B2FH group responsible for originating the theory of stellar nucleosynthesis, which explains how elements are formed in stars. She has held a number of prestigious posts, including the directorship of the Royal Greenwich Observatory.\n Mary Cartwright was a mathematician and student of G. H. Hardy. Her work on nonlinear differential equations was influential in the field of dynamical systems.\n Rosalind Franklin was a crystallographer, whose work helped to elucidate the fine structures of coal, graphite, DNA and viruses. In 1953, the work she did on DNA allowed Watson and Crick to conceive their model of the structure of DNA. Her photograph of DNA gave Watson and Crick a basis for their DNA research, and they were awarded the Nobel Prize without giving due credit to Franklin, who had died of cancer in 1958.\n Jane Goodall is a British primatologist considered to be the world's foremost expert on chimpanzees and is best known for her over 55-year study of social and family interactions of wild chimpanzees. She is the founder of the Jane Goodall Institute and the Roots & Shoots programme.\n Dorothy Hodgkin analyzed the molecular structure of complex chemicals by studying diffraction patterns caused by passing X-rays through crystals. She won the 1964 Nobel prize for chemistry for discovering the structure of vitamin B12, becoming the third woman to win the prize for chemistry.[112]\n Ir\u00e8ne Joliot-Curie, daughter of Marie Curie, won the 1935 Nobel Prize for chemistry with her husband Fr\u00e9d\u00e9ric Joliot for their work in radioactive isotopes leading to nuclear fission. This made the Curies the family with the most Nobel laureates to date.\n Palaeoanthropologist Mary Leakey discovered the first skull of a fossil ape on Rusinga Island and also a noted robust Australopithecine.\n Italian neurologist Rita Levi-Montalcini received the 1986 Nobel Prize in Physiology or Medicine for the discovery of Nerve growth factor (NGF). Her work allowed for a further potential understanding of different diseases such as tumors, delayed healing, malformations, and others.[113] This research led to her winning the Nobel Prize for Physiology or Medicine alongside Stanley Cohen in 1986. While making advancements in medicine and science, Rita Levi-Montalcini was also active politically throughout her life.[114] She was appointed a Senator for Life in the Italian Senate in 2001 and is the oldest Nobel laureate ever to have lived.\n Zoologist Anne McLaren conducted studied in genetics which led to advances in in vitro fertilization. She became the first female officer of the Royal Society in 331 years.\n Christiane N\u00fcsslein-Volhard received the Nobel Prize in Physiology or Medicine in 1995 for research on the genetic control of embryonic development. She also started the Christiane N\u00fcsslein-Volhard Foundation (Christiane N\u00fcsslein-Volhard Stiftung), to aid promising young female German scientists with children.\n Bertha Swirles was a theoretical physicist who made a number of contributions to early quantum theory. She co-authored the well-known textbook Methods of Mathematical Physics with her husband Sir Harold Jeffreys.\n Kay McNulty, Betty Jennings, Betty Snyder, Marlyn Wescoff, Fran Bilas and Ruth Lichterman were six of the original programmers for the ENIAC, the first general purpose electronic computer.[115]\n Linda B. Buck is a neurobiologist who was awarded the 2004 Nobel Prize in Physiology or Medicine along with Richard Axel for their work on olfactory receptors.\n Rachel Carson was a marine biologist from the United States. She is credited with being the founder of the environmental movement.[116] The biologist and activist  published Silent Spring, a work on the dangers of pesticides, in 1962. The publishing of her environmental science book led to the questioning of usage of harmful pesticides and other chemicals in agricultural settings.[116] This led to a campaign to attempt to ultimately discredit Carson. However, the federal government called for a review of DDT which concluded with DDT being banned.[117] Carson later died from cancer in 1964 at 57 years old.[117]\n Eugenie Clark, popularly known as The Shark Lady, was an American ichthyologist known for her research on poisonous fish of the tropical seas and on the behavior of sharks.[118]\n Ann Druyan is an American writer, lecturer and producer specializing in cosmology and popular science. Druyan has credited her knowledge of science to the 20 years she spent studying with her late husband, Carl Sagan, rather than formal academic training.[citation needed] She was responsible for the selection of music on the Voyager Golden Record for the Voyager 1 and Voyager 2 exploratory missions. Druyan also sponsored the Cosmos 1 spacecraft.\n Gertrude B. Elion was an American biochemist and pharmacologist, awarded the Nobel Prize in Physiology or Medicine in 1988 for her work on the differences in biochemistry between normal human cells and pathogens.\n Sandra Moore Faber, with Robert Jackson, discovered the Faber\u2013Jackson relation between luminosity and stellar dispersion velocity in elliptical galaxies. She also headed the team which discovered the Great Attractor, a large concentration of mass which is pulling a number of nearby galaxies in its direction.\n Zoologist Dian Fossey worked with gorillas in Africa from 1967 until her murder in 1985.\n Astronomer Andrea Ghez received a MacArthur \"genius grant\" in 2008 for her work in surmounting the limitations of earthbound telescopes.[119]\n Maria Goeppert Mayer was the second female Nobel Prize winner in Physics, for proposing the nuclear shell model of the atomic nucleus. Earlier in her career, she had worked in unofficial or volunteer positions at the university where her husband was a professor. Goeppert Mayer is one of several scientists whose works are commemorated by a U.S. postage stamp.[120]\n Sulamith Low Goldhaber and her husband Gerson Goldhaber formed a research team on the K meson and other high-energy particles in the 1950s.\n Carol Greider and the Australian born Elizabeth Blackburn, along with Jack W. Szostak, received the 2009 Nobel Prize in Physiology or Medicine for the discovery of how chromosomes are protected by telomeres and the enzyme telomerase.\n Rear Admiral Grace Murray Hopper developed the first computer compiler while working for the Eckert Mauchly Computer Corporation, released in 1952.\n Deborah S. Jin's team at JILA, in Boulder, Colorado, in 2003 produced the first fermionic condensate, a new state of matter.\n Stephanie Kwolek, a researcher at DuPont, invented poly-paraphenylene terephthalamide \u2013 better known as Kevlar.\n Lynn Margulis is a biologist best known for her work on endosymbiotic theory, which is now generally accepted for how certain organelles were formed.\n Barbara McClintock's studies of maize genetics demonstrated genetic transposition in the 1940s and 1950s. Before then, McClintock obtained her PhD from Cornell University in 1927. Her discovery of transposition provided a greater understanding of mobile loci within chromosomes and the ability for genetics to be fluid.[121] She dedicated her life to her research, and she was awarded the Nobel Prize in Physiology or Medicine in 1983. McClintock was the first American woman to receive a Nobel Prize that was not shared by anyone else.[121] McClintock is one of several scientists whose works are commemorated by a U.S. postage stamp.[122]\n Nita Ahuja is a renowned surgeon-scientist known for her work on CIMP in cancer, she is currently the Chief of surgical oncology at Johns Hopkins Hospital. First woman ever to be the Chief of this prestigious department.\n Carolyn Porco is a planetary scientist best known for her work on the Voyager program and the Cassini\u2013Huygens mission to Saturn. She is also known for her popularization of science, in particular space exploration.\n Physicist Helen Quinn, with Roberto Peccei, postulated Peccei-Quinn symmetry. One consequence is a particle known as the axion, a candidate for the dark matter that pervades the universe. Quinn was the first woman to receive the Dirac Medal by the International Centre for Theoretical Physics (ICTP) and the first to receive the Oskar Klein Medal.\n Lisa Randall is a theoretical physicist and cosmologist, best known for her work on the Randall\u2013Sundrum model. She was the first tenured female physics professor at Princeton University.\n Sally Ride was an astrophysicist and the first American woman, and then-youngest American, to travel to outer space. Ride wrote or co-wrote several books on space aimed at children, with the goal of encouraging them to study science.[123][124] Ride participated in the Gravity Probe B (GP-B) project, which provided more evidence that the predictions of Albert Einstein's general theory of relativity are correct.[125]\n Through her observations of galaxy rotation curves, astronomer Vera Rubin discovered the Galaxy rotation problem, now taken to be one of the key pieces of evidence for the existence of dark matter. She was the first female allowed to observe at the Palomar Observatory.\n Sara Seager is a Canadian-American astronomer who is currently a professor at the Massachusetts Institute of Technology and known for her work on extrasolar planets.\n Astronomer Jill Tarter is best known for her work on the search for extraterrestrial intelligence. Tarter was named one of the 100 most influential people in the world by Time Magazine in 2004.[126] She is the former director of SETI.[127]\n Rosalyn Yalow was the co-winner of the 1977 Nobel Prize in Physiology or Medicine (together with Roger Guillemin and Andrew Schally) for development of the radioimmunoassay (RIA) technique.\n Latin America\n Maria Nieves Garcia-Casal, the first scientist and nutritionist woman from Latin America to lead the Latin America Society of Nutrition.\n Angela Restrepo Moreno is a microbiologist from Colombia. She first gained interest in tiny organisms when she had the opportunity to view them through a microscope that belonged to her grandfather.[128] While Restrepo has a variety of research, her main area of research is fungi and their causes of diseases.[128] Her work led her to develop research on a disease caused by fungi that has only been diagnosed in Latin America but was originally found in Brazil: Paracoccidioidomycosis.[128] Research groups also developed by Restrepo have begun studying two routes: the relationship between humans, fungi, and the environment and also how the cells within the fungi work.[128]\n Along with her research, Restrepo co-founded a non-profit that is devoted to scientific research named Corporation for Biological Research (CIB).[128] Angela Restrepo Moreno was awarded the SCOPUS Prize in 2007 for her numerous publications.[128] She currently resides in Colombia and continues her research.\n Susana L\u00f3pez Charret\u00f3n was born in Mexico City, Mexico in 1957. She is a virologist whose area of study focused on the rotavirus.[129] When she initially began studying rotavirus, it had only been discovered four years earlier.[129] Charret\u00f3n's main job was to study how the virus entered cells and its ways of multiplying.[129] Because of her, and several others, work other scientists were able to learn about more details of the virus.[129] Now, her research focuses on the virus's ability to recognize the cells it infects.[129] Along with her husband, Charret\u00f3n was awarded the Carlos J. Finlay Prize for Microbiology in 2001.[129] She also received the Loreal-UNESCO prize titled \"Woman in Science\" in 2012.[129] Charret\u00f3n has also received several other awards for her research.\n Liliana Quintanar Vera is a Mexican chemist. Currently a researcher at the Department of Chemistry of the Center of Investigation and Advanced Studies, Vera's research currently focuses on neurodegenerative diseases like Parkinson's, Alzheimer's, and prion disease and also on degenerative diseases like diabetes and cataracts.[130] For this research she focused on how copper interacts with the proteins of the neurodegenerative diseases mentioned before.[131]\n Liliana's awards include the Mexican Academy of Sciences Research Prize for Science in 2017, the Marcos Moshinsky Chair award in 2016, the Fulbright Scholarship in 2014, and the L'Or\u00e9al-UNESCO For Women in Science Award in 2007.[130]\n The Nobel Prize and Prize in Economic Sciences have been awarded to women 61 times between 1901 and 2022. One woman, Marie Sklodowska-Curie, has been honored twice, with the 1903 Nobel Prize in Physics and the 1911 Nobel Prize in Chemistry. This means that 60 women in total have been awarded the Nobel Prize between 1901 and 2022. 25 women have been awarded the Nobel Prize in physics, chemistry, physiology or medicine.[4]\n Statistics are used to indicate disadvantages faced by women in science, and also to track positive changes of employment opportunities and incomes for women in science.[110]:\u200a33\u200a\n Women appear to do less well than men (in terms of degree, rank, and salary) in the fields that have been traditionally dominated by women, such as nursing. In 1991 women attributed 91% of the PhDs in nursing, and men held 4% of full professorships in nursing.[citation needed] In the field of psychology, where women earn the majority of PhDs, women do not fill the majority of high rank positions in that field.[133][citation needed]\n Women's lower salaries in the scientific community are also reflected in statistics. According to the data provided in 1993, the median salaries of female scientists and engineers with doctoral degrees were 20% less than men.[110]:\u200a35\u200a[needs update] This data can be explained[who?] as there was less participation of women in high rank scientific fields/positions and a female majority in low-paid fields/positions. However, even with men and women in the same scientific community field, women are typically paid 15\u201317% less than men.[citation needed] In addition to the gender gap, there were also salary differences between ethnicity: African-American women with more years of experiences earn 3.4% less than European-American women with similar skills, while Asian women engineers out-earn both Africans and Europeans.[134][needs update]\n Women are also under-represented in the sciences as compared to their numbers in the overall working population. Within 11% of African-American women in the workforce, 3% are employed as scientists and engineers.[clarification needed] Hispanics made up 8% of the total workers in the US, 3% of that number are scientists and engineers. Native Americans participation cannot be statistically measured.[citation needed]\n Women tend to earn less than men in almost all industries, including government and academia.[citation needed] Women are less likely to be hired in highest-paid positions.[citation needed] The data showing the differences in salaries, ranks, and overall success between the genders is often claimed[who?] to be a result of women's lack of professional experience. The rate of women's professional achievement is increasing. In 1996, the salaries for women in professional fields increased from 85% to 95% relative to men with similar skills and jobs. Young women between the age of 27 and 33 earned 98%, nearly as much as their male peers.[needs update] In the total workforce of the United States, women earn 74% as much as their male counterparts (in the 1970s they made 59% as much as their male counterparts).[110]:\u200a33\u201337\u200a[needs update]\n Claudia Goldin, Harvard concludes in A Grand Gender Convergence: Its Last Chapter \u2013 \"The gender gap in pay would be considerably reduced and might vanish altogether if firms did not have an incentive to disproportionately reward individuals who labored long hours and worked particular hours.\"[135]\n Research on women's participation in the \"hard\" sciences such as physics and computer science speaks of the \"leaky pipeline\" model, in which the proportion of women \"on track\" to potentially becoming top scientists fall off at every step of the way, from getting interested in science and maths in elementary school, through doctorate, postdoctoral, and career steps.  The leaky pipeline also applies in other fields.  In biology, for instance, women in the United States have been getting Masters degrees in the same numbers as men for two decades, yet fewer women get PhDs; and the numbers of women principal investigators have not risen.[136]\n What may be the cause of this \"leaky pipeline\" of women in the sciences?[tone] It is important to look at factors outside of academia that are occurring in women's lives at the same time they are pursuing their continued education and career search.  The most outstanding factor that is occurring at this crucial time is family formation. As women are continuing their academic careers, they are also stepping into their new role as a wife and mother.  These traditionally require at large time commitment and presence outside work.  These new commitments do not fare well for the person looking to attain tenure. That is why women entering the family formation period of their life are 35% less likely to pursue tenure positions after receiving their PhD's than their male counterparts.[137]\n In the UK, women occupied over half the places in science-related higher education courses (science, medicine, maths, computer science and engineering) in 2004\u201305.[138] However, gender differences varied from subject to subject: women substantially outnumbered men in biology and medicine, especially nursing, while men predominated in maths, physical sciences, computer science and engineering.\n In the US, women with science or engineering doctoral degrees were predominantly employed in the education sector in 2001, with substantially fewer employed in business or industry than men.[139] According to salary figures reported in 1991, women earn anywhere between 83.6 percent to 87.5 percent that of a man's salary.[needs update]  An even greater disparity between men and women is the ongoing trend that women scientists with more experience are not as well-compensated as their male counterparts.  The salary of a male engineer continues to experience growth as he gains experience whereas the female engineer sees her salary reach a plateau.[140]\n Women, in the United States and many European countries, who succeed in science tend to be graduates of single-sex schools.[110]:\u200aChapter 3\u200a[needs update]  Women earn 54% of all bachelor's degrees in the United States and 50% of those are in science.  9% of US physicists are women.[110]:\u200aChapter 2\u200a[needs update]\n In 2013, women accounted for 53% of the world's graduates at the bachelor's and master's level and 43% of successful PhD candidates but just 28% of researchers. Women graduates are consistently highly represented in the life sciences, often at over 50%. However, their representation in the other fields is inconsistent. In North America and much of Europe, few women graduate in physics, mathematics and computer science but, in other regions, the proportion of women may be close to parity in physics or mathematics. In engineering and computer sciences, women consistently trail men, a situation that is particularly acute in many high-income countries.[141]\n As of 2015, each step up the ladder of the scientific research system saw a drop in female participation until, at the highest echelons of scientific research and decision-making, there were very few women left. In 2015, the EU Commissioner for Research, Science and Innovation Carlos Moedas called attention to this phenomenon, adding that the majority of entrepreneurs in science and engineering tended to be men. In 2013, the German government coalition agreement introduced a 30% quota for women on company boards of directors.[141]\n In 2010, women made up 14% of university chancellors and vice-chancellors at Brazilian public universities  and 17% of those in South Africa in 2011.[142][143] As of 2015, in Argentina, women made up 16% of directors and vice-directors of national research centres and, in Mexico, 10% of directors of scientific research institutes at the National Autonomous University of Mexico.[144][145] In the US, numbers are slightly higher at 23%. In the EU, less than 16% of tertiary institutions were headed by a woman in 2010 and just 10% of universities. In 2011, at the main tertiary institution for the English-speaking Caribbean, the University of the West Indies, women represented 51% of lecturers but only 32% of senior lecturers and 26% of full professors . A 2018 review of the Royal Society of Britain by historians Aileen Fyfe and Camilla M\u00f8rk R\u00f8stvik produced similarly low numbers,[146] with women accounting for more than 25% of members in only a handful of countries, including Cuba, Panama and South Africa. As of 2015, the figure for Indonesia was 17%.[141][147][148]\n In life sciences, women researchers have achieved parity (45\u201355% of researchers) in many countries. In some, the balance even now tips in their favour. Six out of ten researchers are women in both medical and agricultural sciences in Belarus and New Zealand, for instance. More than two-thirds of researchers in medical sciences are women in El Salvador, Estonia, Kazakhstan, Latvia, the Philippines, Tajikistan, Ukraine and Venezuela.[141]\n There has been a steady increase in female graduates in agricultural sciences since the turn of the century. In sub-Saharan Africa, for instance, numbers of female graduates in agricultural science have been increasing steadily, with eight countries reporting a share of women graduates of 40% or more: Lesotho, Madagascar, Mozambique, Namibia, Sierra Leone, South Africa, Swaziland and Zimbabwe. The reasons for this surge are unclear, although one explanation may lie in the growing emphasis on national food security and the food industry. Another possible explanation is that women are highly represented in biotechnology. For example, in South Africa, women were underrepresented in engineering (16%) in 2004 and in 'natural scientific professions' (16%) in 2006 but made up 52% of employees working in biotechnology-related companies.[141]\n Women play an increasing role in environmental sciences and conservation biology. In fact, women played a foremost role in the development of these disciplines. Silent Spring by Rachel Carson proved an important impetus to the conservation movement and the later banning of chemical pesticides. Women played an important role in conservation biology including the famous work of Dian Fossey, who published the famous Gorillas in the Mist and Jane Goodall who studied primates in East Africa. Today women make up an increasing proportion of roles in the active conservation sector. A recent survey of those working in the Wildlife Trusts in the U.K., the leading conservation organisation in England, found that there are nearly as many women as men in practical conservation roles.[149]\n Women are consistently underrepresented in engineering and related fields. In Israel, for instance, where 28% of senior academic staff are women, there are proportionately many fewer in engineering (14%), physical sciences (11%), mathematics and computer sciences (10%) but dominate education (52%) and paramedical occupations (63%). In Japan and the Republic of Korea, women represent just 5% and 10% of engineers.[141]\n For women who are pursuing STEM major careers, these individuals often face gender disparities in the work field, especially in regards to science and engineering. It has become more common for women to pursue undergraduate degrees in science, but are continuously discredited in salary rates and higher ranking positions. For example, men show a greater likelihood of being selected for an employment position than a woman.[150]\n In Europe and North America, the number of female graduates in engineering, physics, mathematics and computer science is generally low. Women make up just 19% of engineers in Canada, Germany and the US and 22% in Finland, for example. However, 50% of engineering graduates are women in Cyprus, 38% in Denmark and 36% in the Russian Federation, for instance.[141]\n In many cases, engineering has lost ground to other sciences, including agriculture. The case of New Zealand is fairly typical. Here, women jumped from representing 39% to 70% of agricultural graduates between 2000 and 2012, continued to dominate health (80\u201378%) but ceded ground in science (43\u201339%) and engineering (33\u201327%).[141]\n In a number of developing countries, there is a sizable proportion of women engineers. At least three out of ten engineers are women, for instance, in Costa Rica, Vietnam and the United Arab Emirates (31%), Algeria (32%), Mozambique (34%), Tunisia (41%) and Brunei Darussalam (42%). In Malaysia (50%) and Oman (53%), women are on a par with men. Of the 13 sub-Saharan countries reporting data, seven have observed substantial increases (more than 5%) in women engineers since 2000, namely: Benin, Burundi, Eritrea, Ethiopia, Madagascar, Mozambique and Namibia.[141]\n Of the seven Arab countries reporting data, four observe a steady percentage or an increase in female engineers (Morocco, Oman, Palestine and Saudi Arabia). In the United Arab Emirates, the government has made it a priority to develop a knowledge economy, having recognized the need for a strong human resource base in science, technology and engineering. With just 1% of the labour force being Emirati, it is also concerned about the low percentage of Emirati citizens employed in key industries. As a result, it has introduced policies promoting the training and employment of Emirati citizens, as well as a greater participation of Emirati women in the labour force. Emirati female engineering students have said that they are attracted to a career in engineering for reasons of financial independence, the high social status associated with this field, the opportunity to engage in creative and challenging projects and the wide range of career opportunities.[141]\n An analysis of computer science shows a steady decrease in female graduates since 2000 that is particularly marked in high-income countries. Between 2000 and 2012, the share of women graduates in computer science slipped in Australia, New Zealand, the Republic of Korea and US. In Latin America and the Caribbean, the share of women graduates in computer science dropped by between 2 and 13 percentage points over this period for all countries reporting data.[141]\n There are exceptions. In Denmark, the proportion of female graduates in computer science increased from 15% to 24% between 2000 and 2012 and Germany saw an increase from 10% to 17%. These are still very low levels. Figures are higher in many emerging economies. In Turkey, for instance, the proportion of women graduating in computer science rose from a relatively high 29% to 33% between 2000 and 2012.[141]\n The Malaysian information technology (IT) sector is made up equally of women and men, with large numbers of women employed as university professors and in the private sector. This is a product of two historical trends: the predominance of women in the Malay electronics industry, the precursor to the IT industry, and the national push to achieve a 'pan-Malayan' culture beyond the three ethnic groups of Indian, Chinese and Malay. Government support for the education of all three groups is available on a quota basis and, since few Malay men are interested in IT, this leaves more room for women. Additionally, families tend to be supportive of their daughters' entry into this prestigious and highly remunerated industry, in the interests of upward social mobility. Malaysia's push to develop an endogenous research culture should deepen this trend.[141]\n In India, the substantial increase in women undergraduates in engineering may be indicative of a change in the 'masculine' perception of engineering in the country. It is also a product of interest on the part of parents, since their daughters will be assured of employment as the field expands, as well as an advantageous marriage. Other factors include the 'friendly' image of engineering in India and the easy access to engineering education resulting from the increase in the number of women's engineering colleges over the last two decades.[141]\n While women have made huge strides in the STEM fields, it is obvious that they are still underrepresented. One of the areas where women are most underrepresented in science is space flight. Out of the 556 people who have traveled to space, only 65 of them were women. This means that only 11% of astronauts have been women.[151]\n In the 1960s, the American space program was taking off. However, women were not allowed to be considered for the space program because at the time astronauts were required to be military pilots\u00a0\u2013 a profession that women were not allowed to be a part of. There were other \"practical\" reasons as well. According to General Don Flickinger of the United States Air Force, there was difficulty \"designing and fitting a space suit to accommodate their particular biological needs and functions.\"[152]\n \nDuring the early 1960s, the first American astronauts, nicknamed the Mercury Seven, were training. At the same time, William Randolph Lovelace II was interested to see if women could manage to go through the same training that the Mercury 7 undergoing at the time. Lovelace recruited thirteen female pilots, called the \"Mercury 13\", and put them through the same tests that the male astronauts took. As a result, the women actually performed better on these tests than the men of the Mercury 7 did. However, this did not convince NASA officials to allow women in space.[151] In response, congressional hearings were held to investigate discrimination against women in the program. One of the women who testified at the hearing was Jerrie Cobb, the first woman to pass Lovelace's tests.[153] During her testimony, Cobb said:[151] I find it a little ridiculous when I read in a newspaper that there is a place called Chimp College in New Mexico where they are training chimpanzees for space flight, one a female named Glenda. I think it would be at least as important to let the women undergo this training for space flight. NASA officials also had representatives present, notably astronauts John Glenn and Scott Carpenter, to testify that women are not suited for the space program. Ultimately, no action came from the hearings, and NASA did not put a woman in space until 1983.[153]\n Even though the United States did not allow women in space during the 60s or 70s, other countries did. Valentina Tereshkova, a cosmonaut from the Soviet Union, was the first woman to fly in space. Although she had no piloting experience, she flew on the Vostok 6 in 1963. Before going to space, Tereshkova was a textile worker. Although she successfully orbited the Earth 48 times, the next woman to go to space did not fly until almost twenty years later.[154]\n Sally Ride was the third woman to go to space and the first American woman in space. In 1978, Ride and five other women were accepted into the first class of astronauts that allowed women. In 1983, Ride became the first American woman in space when she flew on the Challenger for the STS-7 mission.[154]\n \nNASA has been more inclusive in recent years. The number of women in NASA's astronaut classes has steadily risen since the first class that allowed women in 1978. The most recent class was 45% women, and the class before was 50%. In 2019, the first all-female spacewalk was completed at the International Space Station.[155] The global figures mask wide disparities from one region to another. In Southeast Europe, for instance, women researchers have obtained parity and, at 44%, are on the verge of doing so in Central Asia and Latin America and the Caribbean. In the European Union, on the other hand, just one in three (33%) researchers is a woman, compared to 37% in the Arab world. Women are also better represented in sub-Saharan Africa (30%) than in South Asia (17%).[141]\n There are also wide intraregional disparities. Women make up 52% of researchers in the Philippines and Thailand, for instance, and are close to parity in Malaysia and Vietnam, yet only one in three researchers is a woman in Indonesia and Singapore. In Japan and the Republic of Korea, two countries characterized by high researcher densities and technological sophistication, as few as 15% and 18% of researchers respectively are women. These are the lowest ratios among members of the Organisation for Economic Co-operation and Development. The Republic of Korea also has the widest gap among OECD members in remuneration between men and women researchers (39%). There is also a yawning gap in Japan (29%).[141]\n Latin America has some of the world's highest rates of women studying scientific fields; it also shares with the Caribbean one of the highest proportions of female researchers: 44%. Of the 12 countries reporting data for the years 2010\u20132013, seven have achieved gender parity, or even dominate research: Bolivia (63%), Venezuela (56%), Argentina (53%), Paraguay (52%), Uruguay (49%), Brazil (48%) and Guatemala (45%). Costa Rica is on the cusp (43%). Chile has the lowest score among countries for which there are recent data (31%). The Caribbean paints a similar picture, with Cuba having achieved gender parity (47%) and Trinidad and Tobago on 44%. Recent data on women's participation in industrial research are available for those countries with the most developed national innovation systems, with the exception of Brazil and Cuba: Uruguay (47%), Argentina (29%), Colombia and Chile (26%).[141]\n As in most other regions, the great majority of health graduates are women (60\u201385%). Women are also strongly represented in science. More than 40% of science graduates are women in each of Argentina, Colombia, Ecuador, El Salvador, Mexico, Panama and Uruguay. The Caribbean paints a similar picture, with women graduates in science being on a par with men or dominating this field in Barbados, Cuba, Dominican Republic and Trinidad and Tobago.[141]\n In engineering, women make up over 30% of the graduate population in seven Latin American countries (Argentina, Colombia, Costa Rica, Honduras, Panama and Uruguay) and one Caribbean country, the Dominican Republic. There has been a decrease in the number of women engineering graduates in Argentina, Chile and Honduras.[141]\n The participation of women in science has consistently dropped since the turn of the century. This trend has been observed in all sectors of the larger economies: Argentina, Brazil, Chile and Colombia. Mexico is a notable exception, having recorded a slight increase. Some of the decrease may be attributed to women transferring to agricultural sciences in these countries. Another negative trend is the drop in female doctoral students and in the labour force. Of those countries reporting data, the majority signal a significant drop of 10\u201320 percentage points in the transition from master's to doctoral graduates.[141]\n Most countries in Eastern Europe, West and Central Asia have attained gender parity in research (Armenia, Azerbaijan, Georgia, Kazakhstan, Mongolia and Ukraine) or are on the brink of doing so (Kyrgyzstan and Uzbekistan). This trend is reflected in tertiary education, with some exceptions in engineering and computer science. Although Belarus and the Russian Federation have seen a drop over the past decade, women still represented 41% of researchers in 2013. In the former Soviet states, women are also very present in the business enterprise sector: Bosnia and Herzegovina (59%), Azerbaijan (57%), Kazakhstan (50%), Mongolia (48%), Latvia (48%), Serbia (46%), Croatia and Bulgaria (43%), Ukraine and Uzbekistan (40%), Romania and Montenegro (38%), Belarus (37%), Russian Federation (37%).[141]\n One in three researchers is a woman in Turkey (36%) and Tajikistan (34%). Participation rates are lower in Iran (26%) and Israel (21%), although Israeli women represent 28% of senior academic staff. At university, Israeli women dominate medical sciences (63%) but only a minority study engineering (14%), physical sciences (11%), mathematics and computer science (10%). There has been an interesting evolution in Iran. Whereas the share of female PhD graduates in health remained stable at 38\u201339% between 2007 and 2012, it rose in all three other broad fields. Most spectacular was the leap in female PhD graduates in agricultural sciences from 4% to 33% but there was also a marked progression in science (from 28% to 39%) and engineering (from 8% to 16%).[141]\n With the exception of Greece, all the countries of Southeast Europe were once part of the Soviet bloc. Some 49% of researchers in these countries are women (compared to 37% in Greece in 2011). This high proportion is considered a legacy of the consistent investment in education by the Socialist governments in place until the early 1990s, including that of the former Yugoslavia. Moreover, the participation of female researchers is holding steady or increasing in much of the region, with representation broadly even across the four sectors of government, business, higher education and non-profit. In most countries, women tend to be on a par with men among tertiary graduates in science. Between 70% and 85% of graduates are women in health, less than 40% in agriculture and between 20% and 30% in engineering. Albania has seen a considerable increase in the share of its women graduates in engineering and agriculture.[141]\n Women make up 33% of researchers overall in the European Union (EU), slightly more than their representation in science (32%). Women constitute 40% of researchers in higher education, 40% in government and 19% in the private sector, with the number of female researchers increasing faster than that of male researchers. The proportion of female researchers has been increasing over the last decade, at a faster rate than men (5.1% annually over 2002\u20132009 compared with 3.3% for men), which is also true for their participation among scientists and engineers (up 5.4% annually between 2002 and 2010, compared with 3.1% for men).[141]\n Despite these gains, women's academic careers in Europe remain characterized by strong vertical and horizontal segregation. In 2010, although female students (55%) and graduates (59%) outnumbered male students, men outnumbered women at the PhD and graduate levels (albeit by a small margin). Further along in the research career, women represented 44% of grade C academic staff, 37% of grade B academic staff and 20% of grade A academic staff.11 These trends are intensified in science, with women making up 31% of the student population at the tertiary level to 38% of PhD students and 35% of PhD graduates. At the faculty level, they make up 32% of academic grade C personnel, 23% of grade B and 11% of grade A. The proportion of women among full professors is lowest in engineering and technology, at 7.9%. With respect to representation in science decision-making, in 2010 15.5% of higher education institutions were headed by women and 10% of universities had a female rector.[141]\n Membership on science boards remained predominantly male as well, with women making up 36% of board members. The EU has engaged in a major effort to integrate female researchers and gender research into its research and innovation strategy since the mid-2000s. Increases in women's representation in all of the scientific fields overall indicates that this effort has met with some success; however, the continued lack of representation of women at the top level of faculties, management and science decision making indicate that more work needs to be done. The EU is addressing this through a gender equality strategy and crosscutting mandate in Horizon 2020, its research and innovation funding programme for 2014\u20132020.[141]\n In 2013, women made up the majority of PhD graduates in fields related to health in Australia (63%), New Zealand (58%) and the United States of America (73%). The same can be said of agriculture, in New Zealand's case (73%). Women have also achieved parity in agriculture in Australia (50%) and the United States (44%). Just one in five women graduate in engineering in the latter two countries, a situation that has not changed over the past decade. In New Zealand, women jumped from constituting 39% to 70% of agricultural graduates (all levels) between 2000 and 2012 but ceded ground in science (43\u201339%), engineering (33\u201327%) and health (80\u201378%). As for Canada, it has not reported sex-disaggregated data for women graduates in science and engineering in recent years. Moreover, none of the four countries mentioned here have reported recent data on the share of female researchers.[141]\n South Asia is the region where women make up the smallest proportion of researchers: 17%. This is 13 percentage points below sub-Saharan Africa. Of those countries in South Asia reporting data for 2009\u20132013, Nepal has the lowest representation of all (in head counts), at 8% (2010), a substantial drop from 15% in 2002. In 2013, only 14% of researchers (in full-time equivalents) were women in the region's most populous country, India, down slightly from 15% in 2009. The percentage of female researchers is highest in Sri Lanka (39%), followed by Pakistan: 24% in 2009, 31% in 2013. There are no recent data available for Afghanistan or Bangladesh.[141]\n Women are most present in the private non-profit sector \u2013 they make up 60% of employees in Sri Lanka \u2013 followed by the academic sector: 30% of Pakistani and 42% of Sri Lankan female researchers. Women tend to be less present in the government sector and least likely to be employed in the business sector, accounting for 23% of employees in Sri Lanka, 11% in India and just 5% in Nepal. Women have achieved parity in science in both Sri Lanka and Bangladesh but are less likely to undertake research in engineering. They represent 17% of the research pool in Bangladesh and 29% in Sri Lanka. Many Sri Lankan women have followed the global trend of opting for a career in agricultural sciences (54%) and they have also achieved parity in health and welfare. In Bangladesh, just over 30% choose agricultural sciences and health, which goes against the global trend. Although Bangladesh still has progress to make, the share of women in each scientific field has increased steadily over the past decade.[141]\n Southeast Asia presents a different picture entirely, with women basically on a par with men in some countries: they make up 52% of researchers in the Philippines and Thailand, for example. Other countries are close to parity, such as Malaysia and Vietnam, whereas Indonesia and Singapore are still around the 30% mark. Cambodia trails its neighbours at 20%. Female researchers in the region are spread fairly equally across the sectors of participation, with the exception of the private sector, where they make up 30% or less of researchers in most countries.\n The proportion of women tertiary graduates reflects these trends, with high percentages of women in science in Brunei Darussalam, Malaysia, Myanmar and the Philippines (around 60%) and a low of 10% in Cambodia. Women make up the majority of graduates in health sciences, from 60% in Laos to 81% in Myanmar \u2013 Vietnam being an exception at 42%. Women graduates are on a par with men in agriculture but less present in engineering: Vietnam (31%), the Philippines (30%) and Malaysia (39%); here, the exception is Myanmar, at 65%. In the Republic of Korea, women make up about 40% of graduates in science and agriculture and 71% of graduates in health sciences but only 18% of female researchers overall. This represents a loss in the investment made in educating girls and women up through tertiary education, a result of traditional views of women's role in society and in the home. Kim and Moon (2011) remark on the tendency of Korean women to withdraw from the labour force to take care of children and assume family responsibilities, calling it a 'domestic brain drain'.[141]\n Women remain very much a minority in Japanese science (15% in 2013), although the situation has improved slightly (13% in 2008) since the government fixed a target in 2006 of raising the ratio of female researchers to 25%. Calculated on the basis of the current number of doctoral students, the government hopes to obtain a 20% share of women in science, 15% in engineering and 30% in agriculture and health by the end of the current Basic Plan for Science and Technology in 2016. In 2013, Japanese female researchers were most common in the public sector in health and agriculture, where they represented 29% of academics and 20% of government researchers. In the business sector, just 8% of researchers were women (in head counts), compared to 25% in the academic sector. In other public research institutions, women accounted for 16% of researchers. One of the main thrusts of Abenomics, Japan's current growth strategy, is to enhance the socio-economic role of women. Consequently, the selection criteria for most large university grants now take into account the proportion of women among teaching staff and researchers.[141]\n The low ratio of women researchers in Japan and the Republic of Korea, which both have some of the highest researcher densities in the world, brings down Southeast Asia's average to 22.5% for the share of women among researchers in the region.[141]\n At 37%, the share of female researchers in the Arab States compares well with other regions. The countries with the highest proportion of female researchers are Bahrain and Sudan at around 40%. Jordan, Libya, Oman, Palestine and Qatar have percentage shares in the low twenties. The country with the lowest participation of female researchers is Saudi Arabia, even though they make up the majority of tertiary graduates, but the figure of 1.4% covers only the King Abdulaziz City for Science and Technology. Female researchers in the region are primarily employed in government research institutes, with some countries also seeing a high participation of women in private nonprofit organizations and universities.[156] With the exception of Sudan (40%) and Palestine (35%), fewer than one in four researchers in the business enterprise sector is a woman; for half of the countries reporting data, there are barely any women at all employed in this sector.[141]\n Despite these variable numbers, the percentage of female tertiary-level graduates in science and engineering is very high across the region, which indicates there is a substantial drop between graduation and employment and research. Women make up half or more than half of science graduates in all but Sudan and over 45% in agriculture in eight out of the 15 countries reporting data, namely Algeria, Egypt, Jordan, Lebanon, Sudan, Syria, Tunisia and the United Arab Emirates. In engineering, women make up over 70% of graduates in Oman, with rates of 25\u201338% in the majority of the other countries, which is high in comparison to other regions.[141]\n The participation of women is somewhat lower in health than in other regions, possibly on account of cultural norms restricting interactions between males and females. Iraq and Oman have the lowest percentages (mid-30s), whereas Iran, Jordan, Kuwait, Palestine and Saudi Arabia are at gender parity in this field. The United Arab Emirates and Bahrain have the highest rates of all: 83% and 84%.[141]\n Once Arab women scientists and engineers graduate, they may come up against barriers to finding gainful employment. These include a misalignment between university programmes and labour market demand \u2013 a phenomenon which also affects men \u2013, a lack of awareness about what a career in their chosen field entails, family bias against working in mixed-gender environments and a lack of female role models.[141][157]\n One of the countries with the smallest female labour force is developing technical and vocational education for girls as part of a wider scheme to reduce dependence on foreign labour. By 2017, the Technical and Vocational Training Corporation of Saudi Arabia is to have constructed 50 technical colleges, 50 girls' higher technical institutes and 180 industrial secondary institutes. The plan is to create training placements for about 500 000 students, half of them girls. Boys and girls will be trained in vocational professions that include information technology, medical equipment handling, plumbing, electricity and mechanics.[141]\n Just under one in three (30%) researchers in sub-Saharan Africa is a woman. Much of sub-Saharan Africa is seeing solid gains in the share of women among tertiary graduates in scientific fields. In two of the top four countries for women's representation in science, women graduates are part of very small cohorts, however: they make up 54% of Lesotho's 47 tertiary graduates in science and 60% of those in Namibia's graduating class of 149. South Africa and Zimbabwe, which have larger graduate populations in science, have achieved parity, with 49% and 47% respectively. The next grouping clusters seven countries poised at around 35\u201340% (Angola, Burundi, Eritrea, Liberia, Madagascar, Mozambique and Rwanda). The rest are grouped around 30% or below (Benin, Ethiopia, Ghana, Swaziland and Uganda). Burkina Faso ranks lowest, with women making up 18% of its science graduates.[141]\n Female representation in engineering is fairly high in sub-Saharan Africa in comparison with other regions. In Mozambique and South Africa, for instance, women make up more than 34% and 28% of engineering graduates, respectively. Numbers of female graduates in agricultural science have been increasing steadily across the continent, with eight countries reporting the share of women graduates of 40% or more (Lesotho, Madagascar, Mozambique, Namibia, Sierra Leone, South Africa, Swaziland and Zimbabwe). In health, this rate ranges from 26% and 27% in Benin and Eritrea to 94% in Namibia.[141]\n Of note is that women account for a relatively high proportion of researchers employed in the business enterprise sector in South Africa (35%), Kenya (34%), Botswana and Namibia (33%) and Zambia (31%). Female participation in industrial research is lower in Uganda (21%), Ethiopia (15%) and Mali (12%).[141]\n Beginning in the twentieth century[original research?] to present day, more and more women are becoming acknowledged for their work in science. However, women often find themselves at odds with expectations held towards them in relation to their scientific studies. For example, in 1968 James Watson questioned scientist Rosalind Franklin's place in the industry. He claimed that \"the best place for a feminist was in another person's lab\",[110]:\u200a76\u201377\u200a most often a male's research lab.[improper synthesis?] Women were and still are often critiqued of their overall presentation.[citation needed] In Franklin's situation, she was seen as lacking femininity for she failed to wear lipstick or revealing clothing.[110]:\u200a76\u201377\u200a\n Since on average most of a woman's colleagues in science are men who do not see her as a true social peer, she will also find herself left out of opportunities to discuss possible research opportunities outside of the laboratory. In Londa Schiebinger's book, Has Feminism Changed Science?, she mentions that men would have discussed their research outside of the lab, but this conversation is preceded by culturally \"masculine\" small-talk topics that, whether intentionally or not, excluded women influenced by their culture's feminine gender role from the conversation.[110]:\u200a81\u201391\u200a Consequently, this act of excluding many women from the after-hours work discussions produced a more separate work environment between the men and the women in science; as women then would converse with other women in science about their current findings and theories. Ultimately, the women's work was devalued as a male scientist was not involved in the overall research and analysis.\n According to Oxford University Press, the inequality toward women is \"endorsed within cultures and entrenched within institutions [that] hold power to reproduce that inequality\".[158] There are various gendered barriers in social networks that prevent women from working in male-dominated fields and top management jobs. Social networks are based on the cultural beliefs such as schemas and stereotypes.[158] According to social psychology studies, top management jobs are more likely to have incumbent schemas that favor \"an achievement-oriented aggressiveness and emotional toughness that is distinctly male in character\".[158] Gender stereotypes of feminine style set by men assume women to be conforming and submissive to male culture creating a sense of unqualified women for top management jobs. However, when the women try to prove their competence and power, they often faced obstacles. They are likely to be seen as dislikable and untrustworthy even when they excel at \"masculine\" tasks.[158] In addition, women's achievements are likely to be dismissed or discredited.[158] These \"untrustworthy, dislikable women\" could have very well been denied achievement from the fear men held of a woman overtaking his management position. Social networks and gender stereotypes produce many injustices that women have to experience in their workplace, as well as, the various obstacles they encounter when trying to advance in male-dominated and top management jobs. Women in professions like science, technology, and other related industries are likely to encounter these gendered barriers in their careers.[158] Based on the meritocratic explanations of gender inequality, \"as long as the people accept the mechanisms that produce unequal outcomes\", all the outcomes will be legitimated in the society.[158] When women try to deny the stereotypes and the discriminations by becoming \"competent, integrated, well-liked\", the society is more likely to look at these impressions as selfishness or \"being a whiner\".[158] However, there have been positive attempts to reduce gender discrimination in the public domain. For example, in the United States, Title IX of the Education Amendments of 1972 provides opportunities for women to achieve to a wide range of education programs and activities by prohibiting sex discrimination.[159] The law states \"No person in the United States shall, on the basis of sex, be excluded from participation in, be denied the benefits of, or be subject to discrimination under any educational program or activity receiving federal financial assistance.\"[159] Although, even with laws prohibiting gender discrimination, society and social institutions continue to minimize women's competencies and accomplishments, especially, in the workforce by dismissing or discrediting their achievements as stated above.\n While there has been a push to encourage more women to participate in science, there is less outreach to lesbian, bi, or gender nonconforming women, and gender nonconforming people more broadly.[160] Due to the lack of data and statistics of LGBTQ members involvement in the STEM field, it is unknown to what exact degree lesbian and bisexual women, gender non-conformers (transgender, nonbinary/agender, or anti-gender gender abolitionists who eschew the system altogether) are potentially even more repressed and underrepresented than their straight peers. But a general lack of out lesbian and bi women in STEM has been noted.[160][161] Reasons for under-representation of same-sex attracted women and anyone gender nonconforming in STEM fields include lack of role models in K\u201312,[160][161][162] the desire of some transgender girls and women to adopt traditional heteronormative gender roles as gender is a cultural performance and socially-determined subjective internal experience,[163][164] employment discrimination, and the possibility of sexual harassment in the workplace. Historically, women who have accepted STEM research positions for the government or the military remained in the closet due to lack of federal protections or the fact that homosexual or gender nonconforming expression was criminalized in their country.  A notable example is Sally Ride, a physicist, the first American female astronaut, and a lesbian.[165][166] Sally Ride chose not to reveal her sexuality until after her death in 2012; she purposefully revealed her sexual orientation in her obituary.[166] She has been known as the first female (and youngest) American to enter space, as well as, starting her own company, Sally Ride Science, that encourages young girls to enter the STEM field. She chose to keep her sexuality to herself because she was familiar with \"the male-dominated\" NASA's anti-homosexual policies at the time of her space travel.[166] Sally Ride's legacy continues as her company is still working to increase young girls and women's participation in the STEM fields.[167]\n In a nationwide study of LGBTQA employees in STEM fields in the United States, same-sex attracted and gender nonconforming women in engineering, earth sciences, and mathematics reported that they were less likely to be out in the workplace.[168] In general, LGBTQA people in this survey reported that, when more female or feminine gender role-identified people worked in their labs, the more accepting and safe the work environment.[168] In another study of over 30,000 LGBT employees in STEM-related federal agencies in the United States, queer women in these agencies reported feeling isolated in the workplace and having to work harder than their gender conforming male colleagues. This isolation and overachievement remained constant as they earned supervisory positions and worked their way up the ladder.[169] Gender nonconforming people in physics, particularly those identified as trans women in physics programs and labs, felt the most isolated and perceived the most hostility.[170]\n Organizations such as Lesbians Who Tech, National Organization of Gay and Lesbian Scientists and Technical Professionals (NOGLSTP), Out in Science, Technology, Engineering and Mathematics (OSTEM), Pride in STEM, and House of STEM currently provide networking and mentoring opportunities for lesbian girls and women and LGBT people interested in or currently working in STEM fields. These organizations also advocate for the rights of lesbian and bi women and gender nonconformists in STEM in education and the workplace.\n Margaret Rossiter, an American historian of science, offered three concepts to explain the reasons behind the data in statistics and how these reasons disadvantaged women in the science industry. The first concept is hierarchical segregation.[171] This is a well-known phenomenon in society, that the higher the level and rank of power and prestige, the smaller the population of females participating. The hierarchical differences point out that there are fewer women participating at higher levels of both academia and industry. Based on data collected in 1982, women earn 54 percent of all bachelor's degrees in the United States, with 50 percent of these in science. The source also indicated that this number increased almost every year.[172] There are fewer women at the graduate level; they earn 40 percent of all doctorates, with 31 percent of these in science and engineering.\n The second concept included in Rossiter's explanation of women in science is territorial segregation.[110]:\u200a34\u201335\u200a The term refers to how female employment is often clustered in specific industries or categories in industries. Women stayed at home or took employment in feminine fields while men left the home to work. Although nearly half of the civilian work force is female, women still comprise the majority of low-paid jobs or jobs that society considered feminine. Statistics show that 60 percent of white professional women are nurses, daycare workers, or schoolteachers.[173] Territorial disparities in science are often found between the 1920s and 1930s, when different fields in science were divided between men and women. \n Researchers collected the data on many differences between women and men in science. Rossiter found that in 1966, thirty-eight percent of female scientists held master's degrees compared to twenty-six percent of male scientists; but large proportions of female scientists were in environmental and nonprofit organizations.[174] During the late 1960s and 1970s, equal-rights legislation made the number of female scientists rise dramatically. The statistics from National Science Board (NSB) present the change at that time.[citation needed] The number of science degrees awarded to woman rose from seven percent in 1970 to twenty-four percent in 1985. In 1975 only 385 women received bachelor's degrees in engineering compared to 11,000 women in 1985. Elizabeth Finkel claims that even if the number of women participating in scientific fields increases, the opportunities are still limited.[citation needed]. Another researcher, Harriet Zuckerman, claims that when woman and man have similar abilities for a job, the probability of the woman getting the job is lower.[citation needed] Elizabeth Finkel agrees, saying, \"In general, while woman and men seem to be completing doctorate with similar credentials and experience, the opposition and rewards they find are not comparable. Women tend to be treated with less salary and status, many policy makers notice this phenomenon and try to rectify the unfair situation for women participating in scientific fields.\"[174]\n Despite women's tendency to perform better than men academically, there are flaws involving stereotyping, lack of information, and family influence that have been found to affect women's involvement in science. Stereotyping has an effect, because people associate characteristics such as nurturing, kind, and warm or characteristics like strong and powerful with a particular gender. These character associations lead people to stereotype that certain jobs are more suitable to a particular gender.[175] Lack of information is something that many institutions have worked hard over the years to improve by making programs such as the IFAC project[176] (Information for a choice: empowering women through learning for scientific and technological career paths) which investigated low women participation in science and technology fields at high school to university level. However, not all efforts were as successful, \"Science: it's a girl thing\" campaign, which has since been removed, received backlash for further encouraging women that they must partake in \"girly\" or \"feminine\" activities.[176] The idea being that if women are fully informed of their career choices and employability, they will be more inclined to pursue STEM field jobs. Women also struggle in the sense of lacking role models of women in science.[176] Family influence is dependent on education level, economic status, and belief system.[177] Education level of a student's parent matters, because oftentimes people who have higher education have a different opinion on education's importance than someone that does not. A parent can also be an influence in the sense that they want their children to follow in their footsteps and pursue a similar occupation, especially in women, it's been found that the mother's line of work tends to correlate with their daughters.[178] Economic status can influence what kind of higher education a student might get. Economic status may influence their education depending on whether they are a work bound student or a college bound student. A work bound student may choose a shorter career path to quickly begin making money or due to lack of time. The belief system of a household can also have a big impact on women depending on their family's religious or cultural viewpoints. There are still some countries that have certain regulations on women's occupation, clothing, and curfew that limit career choices for women. Parental influence is also relevant because people tend to want to fulfill what they could not have as a child.[177] Unfortunately, women are at such a disadvantage because not only must they overcome societal norms but then they also have to outperform men for the same recognition, studies show.[179]\n That sexism is alive and well in science is known. ...Even in the life sciences, where men and women start careers in fairly equal numbers, the number of women drops off rapidly at professorial level.On average, fewer than one in five science professors are female. Science punishes career breaks, and women who take time off to have children are immediately disadvantaged. \"The flashpoint is when you\u2019re about 35 and trying to get tenure. That can be when you\u2019re trying to have kids, and it can play a major role in why you see so much attrition at that stage,\" said Jennifer Rohn, a cell biologist at University College London. A grant may give a woman a year\u2019s grace if she has a baby, but it takes longer to get back into research projects than that.[180] A number of organizations have been set up to combat the stereotyping that may encourage girls away from careers in these areas.  In the UK The WISE Campaign (Women into Science, Engineering and Construction) and the UKRC (The UK Resource Centre for Women in SET) are collaborating to ensure industry, academia and education are all aware of the importance of challenging the traditional approaches to careers advice and recruitment that mean some of the best brains in the country are lost to science.  The UKRC and other women's networks provide female role models, resources and support for activities that promote science to girls and women. The Women's Engineering Society, a professional association in the UK, has been supporting women in engineering and science since 1919.  In computing, the British Computer Society group BCSWomen is active in encouraging girls to consider computing careers, and in supporting women in the computing workforce.\n In the United States, the Association for Women in Science is one of the most prominent organization for professional women in science.  In 2011, the Scientista Foundation was created to empower pre-professional college and graduate women in science, technology, engineering and mathematics (STEM), to stay in the career track.  There are also several organizations focused on increasing mentorship from a younger age. One of the best known groups is Science Club for Girls,[citation needed] which pairs undergraduate mentors with high school and middle school mentees. The model of that pairs undergraduate college mentors with younger students is quite popular. In addition, many young women are creating programs to boost participation in STEM at a younger level, either through conferences or competitions.\n In efforts to make women scientists more visible to the general public, the Grolier Club in New York hosted a \"landmark exhibition\" titled \"Extraordinary Women in Science & Medicine: Four Centuries of Achievement\", showcasing the lives and works of 32 women scientists in 2003.[181] The National Institute for Occupational Safety and Health (NIOSH) developed a video series highlighting the stories of female researchers at NIOSH.[182] Each of the women featured in the videos share their journey into science, technology, engineering, or math (STEM), and offers encouragement to aspiring scientists.[182] NIOSH also partners with external organizations in efforts to introduce individuals to scientific disciplines and funds several science-based training programs across the country.[183][184]\n Creative Resilience: Art by Women in Science is a multi\u2013media exhibition and accompanying publication, produced in 2021 by the Gender Section of the United Nations Educational, Scientific and Cultural Organization (UNESCO). The project aims to give visibility to women, both professionals and university students, working in science, technology, engineering and mathematics (STEM). With short biographical information and graphic reproductions of their artworks dealing with the Covid-19 pandemic and accessible online, the project provides a platform for women scientists to express their experiences, insights, and creative responses to the pandemic.[185]\n Kizzmekia Corbett, recognized as one of the leading scientists in the United States for vaccine research, is a true pioneer who is dedicated to promoting diversity and equity within her field. She is a part of a team at the National Institutes of Health that developed one of the COVID-19 vaccines that is greater than 90% effective. Given the disproportionate impact of COVID-19 on African Americans and the long history of African American and female scientists being underrecognized, it is particularly significant to acknowledge the groundbreaking contributions of Dr. Corbett. [186]\n In 2013, journalist Christie Aschwanden noted that a type of media coverage of women scientists that \"treats its subject's sex as her most defining detail\" was still prevalent. She proposed a checklist, the \"Finkbeiner test\",[188] to help avoid this approach.[189] It was cited in the coverage of a much-criticized 2013 New York Times obituary of rocket scientist Yvonne Brill that began with the words: \"She made a mean beef stroganoff\".[190] Women are often poorly portrayed in film.[191] The misrepresentation of women scientists in film, television and books can influence children to engage in gender stereotyping. This was seen in a 2007 meta-analysis conducted by Jocelyn Steinke and colleagues from Western Michigan University where, after engaging elementary school students in a Draw-a-Scientist Test, out of 4,000 participants only 28 girls drew female scientists.[192]\n A study conducted at Lund University in 2010 and 2011 analysed the genders of invited contributors to News & Views in Nature and Perspectives in Science.  It found that 3.8% of the Earth and environmental science contributions to News & Views were written by women even while the field was estimated to be 16\u201320% female in the United States.  Nature responded by suggesting that, worldwide, a significantly lower number of Earth scientists were women, but nevertheless committed to address any disparity.[193]\n In 2012, a journal article published in Proceedings of the National Academy of Sciences (PNAS) reported a gender bias among science faculty.[194] Faculty were asked to review a resume from a hypothetical student and report how likely they would be to hire or mentor that student, as well as what they would offer as starting salary.  Two resumes were distributed randomly to the faculty, only differing in the names at the top of the resume (John or Jennifer).  The male student was rated as significantly more competent, more likely to be hired, and more likely to be mentored.  The median starting salary offered to the male student was greater than $3,000 over the starting salary offered to the female student.  Both male and female faculty exhibited this gender bias.  This study suggests bias may partly explain the persistent deficit in the number of women at the highest levels of scientific fields.  Another study reported that men are favored in some domains, such as biology tenure rates, but that the majority of domains were gender-fair; the authors interpreted this to suggest that the under-representation of women in the professorial ranks was not solely caused by sexist hiring, promotion, and remuneration.[195] In April 2015 Williams and Ceci published a set of five national experiments showing that hypothetical female applicants were favored by faculty for assistant professorships over identically qualified men by a ratio of 2 to 1.[196]\n In 2014, a controversy over the depiction of pinup women on Rosetta project scientist Matt Taylor's shirt during a press conference raised questions of sexism within the European Space Agency.[197] The shirt, which featured cartoon women with firearms, led to an outpouring of criticism and an apology after which Taylor \"broke down in tears.\"[165]\n In 2015, stereotypes about women in science were directed at Fiona Ingleby, research fellow in evolution, behavior, and environment at the University of Sussex, and Megan Head, postdoctoral researcher at the Australian National University, when they submitted a paper analyzing the progression of PhD graduates to postdoctoral positions in the life sciences to the journal PLOS ONE.[198] The authors received an email on 27 March informing them that their paper had been rejected due to its poor quality.[198] The email included comments from an anonymous reviewer, which included the suggestion that male authors be added in order to improve the quality of the science and serve as a means of ensuring that incorrect interpretations of the data are not included.[198] Ingleby posted excerpts from the email on Twitter on 29 April bringing the incident to the attention of the public and media.[198] The editor was dismissed from the journal and the reviewer was removed from the list of potential reviewers.  A spokesman from PLOS apologized to the authors and said they would be given the opportunity to have the paper reviewed again.[198]\n On 9 June 2015, Nobel prize winning biochemist Tim Hunt spoke at the World Conference of Science Journalists in Seoul.  Prior to applauding the work of women scientists, he described emotional tension, saying \"you fall in love with them, they fall in love with you, and when you criticise them they cry.\"[199] Initially, his remarks were widely condemned and he was forced to resign from his position at University College London.  However, multiple conference attendees gave accounts, including a partial transcript and a partial recording, maintaining that his comments were understood to be satirical before being taken out of context by the media.[200]\n In 2016 an article published in JAMA Dermatology reported a significant and dramatic downward trend in the number of NIH-funded woman investigators in the field of dermatology and that the gender gap between male and female NIH-funded dermatology investigators was widening.  The article concluded that this disparity was likely due to a lack of institutional support for women investigators.[201]\n \nIn January 2005, Harvard University President Lawrence Summers sparked controversy at a National Bureau of Economic Research (NBER) Conference on Diversifying the Science & Engineering Workforce. Dr. Summers offered his explanation for the shortage of women in senior posts in science and engineering. He made comments suggesting the lower numbers of women in high-level science positions may in part be due to innate differences in abilities or preferences between men and women. Making references to the field and behavioral genetics, he noted the generally greater variability among men (compared to women) on tests of cognitive abilities,[202][203][204] leading to proportionally more men than women at both the lower and upper tails of the test score distributions. In his discussion of this, Summers said that \"even small differences in the standard deviation [between genders] will translate into very large differences in the available pool substantially out [from the mean]\".[205] Summers concluded his discussion by saying:[205] So my best guess, to provoke you, of what's behind all of this is that the largest phenomenon, by far, is the general clash between people's legitimate family desires and employers' current desire for high power and high intensity, that in the special case of science and engineering, there are issues of intrinsic aptitude, and particularly of the variability of aptitude, and that those considerations are reinforced by what are in fact lesser factors involving socialization and continuing discrimination. Despite his prot\u00e9g\u00e9e, Sheryl Sandberg, defending Summers' actions and Summers offering his own apology repeatedly, the Harvard Graduate School of Arts and Sciences passed a motion of \"lack of confidence\" in the leadership of Summers who had allowed tenure offers to women plummet after taking office in 2001.[205] The year before he became president, Harvard extended 13 of its 36 tenure offers to women and by 2004 those numbers had dropped to 4 of 32 with several departments lacking even a single tenured female professor.[206] This controversy is speculated to have significantly contributed to Summers resignation from his position at Harvard the following year.\n"
    },
    {
        "title": "Women in Vatican City",
        "url": "https://en.wikipedia.org/wiki/Women_in_Vatican_City",
        "content": "\n Women account for approximately 5.5% of the citizenry of Vatican City. According to the Herald Sun in March 2011, there were only 32 females out of 572 citizens issued with Vatican passports and one of them was a nun.[1] In 2013, Worldcrunch reported that there were around 30 women who were citizens of Vatican City, including two South American women, two Poles, and three from Switzerland. The majority of Vatican women at the time were from Italy.[2]\n Among the women who lived in Vatican City was one of the daughters of an electrician, who later got married and \"lost her right to live\" in the city. Another woman who lives in Vatican City was Magdalena Woli\u0144ska-Riedi, who is a Polish translator and wife of one of the Swiss Guards.[2]\n Among the women who have citizenship in Vatican City, there is one officer in the military, two teachers (one teaches in high school, the other teaches in kindergarten), and one academic. Women obtain Vatican City citizenship by marriage (as a baptized Catholic) to their husbands; however such citizenship \"lasts only for the duration of their stay\" in Vatican City.[2]\n In the past, women were not allowed to open a bank account in Vatican City, but, during the leadership of Pope John Paul II and Pope Benedict XVI, the value of women in the city was highlighted. One of Pope Benedict XVI's assistant editors and confidential adviser was a woman, Ingrid Stampa.[2] On April 21, 2013, The Telegraph reported that Pope Francis will be appointing \"more women to key Vatican\" positions. In May 2019, Francis appointed three women as consultors to the General Secretariat for the Synod of Bishops on Young People, Faith, and Vocational Discernment, marking a historic first for the Church.[3] In addition to this, L'Osservatore Romano\u00a0\u2013 the daily newspaper in Vatican City\u00a0\u2013 is now publishing supplementary pages that address women's issues.[4] Women are not allowed to be ordained to the presbyterate or episcopate, though a commission is currently studying the question of whether women can serve as un-ordained deacons.\n Women (and men) visiting St. Peter's Basilica, the Sistine Chapel or the Vatican Museums in Vatican City are expected to wear appropriate attire. Low cut or sleeveless clothing, shorts, miniskirts and hats (for men, indoors) are not allowed. Women may or may not wear the traditional \"black hat or veil\". Dress code for Papal audiences is somewhat more formal.[5][6] Women cannot wear clothing that does not cover the shoulders and the knees.[2]\n The restrictions on clothing are very strict and the Swiss Guards have the authority of the Pope to restrict access to anyone who is not following the dress code. Those who are not following the dress code can purchase lightweight ponchos outside the museum. [7]\n Protocol for private and official audiences with the Pope says that ladies should wear a black dress, without a neckline, and cover their heads with a veil, also black; no large handbags or flashy jewellery; yes to a string of pearls. It is also allowed to meet the Pope wearing typical national or regional costumes, but red is banned (reserved for cardinals' robes) and purple for its liturgical significance (colour of penance), while the \"privilege of white\" is only for Catholic queens. Raisa Gorbacheva's visit in 1989 was famous: she presented herself to the Pope in a red dress and without a veil.\n The Pope, who serves as the head of state, is elected by the College of Cardinals of the Catholic Church. The College is part of the Holy See, which forms a separate sovereign entity from Vatican City. Cardinals in the Catholic Church are required to be male, with voting Cardinals usually Bishops, and only men are eligible to be elected Pope. With that being said, however, the cardinalate is a privilege and office bestowed by the Pope; it is not a separate, fourth degree of Holy Orders, along with deacon, priest, and Bishop, though it ranks above them, so theoretically, the laws could be amended to allow for women to be Cardinals.\n Vatican City is one of two sovereign states that do not allow divorce, the other being the Philippines (see Divorce in the Philippines).\n The law of Vatican City recognizes the canon law of the Catholic Church as its primary source of law and primary reference for legal interpretation, and it adopts several Italian laws for practical purposes, such as the Italian penal code in force in 1929 with local modifications.[8] Canon 1397 \u00a72 and articles 381 to 385 of the penal code both prohibit abortion without explicitly mentioning any exception, but article 49 of the penal code lists the principle of necessity to save one's life, which removes punishment for any action that would otherwise be a crime.[9][10] The authors of the Italian penal code considered that this article allowed abortion to save the woman's life,[11][12] but the Church's interpretation of the canon law is more restrictive, allowing only indirect abortion under the principle of double effect, such as treatment for an ectopic pregnancy or cancer.[13] In these cases the procedure is aimed only at preserving the woman's life, and the death of the fetus, although foreseen, is not willed either as an end or as a means for obtaining the intended effect.[14][15]\n In September 2016, Vatican authorities approved the creation of Donne in Vaticano, the first women-only association of the Vatican. The members of the association are journalists, theologians, and economists.[16][17] It is led by founding president Tracey McClure.[18]\n \n"
    },
    {
        "title": "Federal Communications Commission",
        "url": "https://en.wikipedia.org/wiki/Federal_Communications_Commission",
        "content": "\n The Federal Communications Commission (FCC) is an independent agency of the United States government that regulates communications by radio, television, wire, satellite, and cable across the United States. The FCC maintains jurisdiction over the areas of broadband access, fair competition, radio frequency use, media responsibility, public safety, and homeland security.[4]\n The FCC was formed by the Communications Act of 1934 to replace the radio regulation functions of the previous Federal Radio Commission.[5] The FCC took over wire communication regulation from the Interstate Commerce Commission. The FCC's mandated jurisdiction covers the 50 states, the District of Columbia, and the territories of the United States. The FCC also provides varied degrees of cooperation, oversight, and leadership for similar communications bodies in other countries in North America. The FCC is funded entirely by regulatory fees. It has an estimated fiscal-2022 budget of US $388 million.[2] It has 1,482 federal employees as of July 2020.[6]\n The FCC's mission, specified in Section One of the Communications Act of 1934 and amended by the Telecommunications Act of 1996 (amendment to 47 U.S.C. \u00a7151), is to \"make available so far as possible, to all the people of the United States, without discrimination on the basis of race, color, religion, national origin, or sex, rapid, efficient, nationwide, and world-wide wire and radio communication services with adequate facilities at reasonable charges.\"\n The act furthermore provides that the FCC was created \"for the purpose of the national defense\" and \"for the purpose of promoting safety of life and property through the use of wire and radio communications.\"[4]\n The FCC is directed by five commissioners appointed by the president of the United States and confirmed by the United States Senate for five-year terms, except when filling an unexpired term. The U.S. president designates one of the commissioners to serve as chairman. No more than three commissioners may be members of the same political party. None of them may have a financial interest in any FCC-related business.[3][7]\n Commissioners may continue serving until the appointment of their replacements. However, they may not serve beyond the end of the next session of Congress following term expiration.[8] In practice, this means that commissioners may serve up to 1+1\u20442 years beyond the official term expiration listed above if no replacement is appointed. This would end on the date that Congress adjourns its annual session, generally no later than noon on January\u00a03.\n The FCC is organized into seven bureaus,[9] each headed by a \"chief\" that is appointed by the chair of the commission. Bureaus process applications for licenses and other filings, analyze complaints, conduct investigations, develop and implement regulations, and participate in hearings.\n The FCC has twelve staff offices.[9]\nThe FCC's offices provide support services to the bureaus.\n The FCC leases space in the Sentinel Square III building in northeast Washington, D.C.[13][14]\n Prior to moving to its new headquarters in October 2020, the FCC leased space in the Portals building in southwest Washington, D.C. Construction of the Portals building was scheduled to begin on March 1, 1996. In January 1996, the General Services Administration signed a lease with the building's owners, agreeing to let the FCC lease 450,000\u00a0sq\u00a0ft (42,000\u00a0m2) of space in Portals for 20 years, at a cost of $17.3\u00a0million per year in 1996 dollars. Prior to the Portals, the FCC had space in six buildings at and around 19th Street NW and M Street NW. The FCC first solicited bids for a new headquarters complex in 1989. In 1991 the GSA selected the Portals site. The FCC had wanted to move into a more expensive area along Pennsylvania Avenue.[15]\n In 1934, Congress passed the Communications Act, which abolished the Federal Radio Commission and transferred jurisdiction over radio licensing to a new Federal Communications Commission, including in it also the telecommunications jurisdiction previously handled by the Interstate Commerce Commission.[16][17]\n Title II of the Communications Act focused on telecommunications using many concepts borrowed from railroad legislation and Title III contained provisions very similar to the Radio Act of 1927.\n The initial organization of the FCC was effected July 17, 1934, in three divisions, Broadcasting, Telegraph, and Telephone. Each division was led by two of the seven commissioners, with the FCC chairman being a member of each division. The organizing meeting directed the divisions to meet on July 18, July 19, and July 20, respectively.[18]\n In 1941, the Federal Communications Commission issued the \"Report on Chain Broadcasting\" which was led by new FCC chairman James Lawrence Fly (and Telford Taylor as general counsel). The major point in the report was the breakup of the National Broadcasting Company (NBC), which ultimately led to the creation of the American Broadcasting Company (ABC), but there were two other important points. One was network option time, the culprit here being the Columbia Broadcasting System (CBS). The report limited the amount of time during the day and at what times the networks may broadcast. Previously a network could demand any time it wanted from a Network affiliate. The second concerned artist bureaus. The networks served as both agents and employers of artists, which was a conflict of interest the report rectified.[19]\n In assigning television stations to various cities after World War II, the FCC found that it placed many stations too close to each other, resulting in interference. At the same time, it became clear that the designated VHF channels, 2 through 13, were inadequate for nationwide television service.[20] As a result, the FCC stopped giving out construction permits for new licenses in October 1948, under the direction of Chairman Rosel H. Hyde. Most expected this \"Freeze\" to last six months, but as the allocation of channels to the emerging UHF technology and the eagerly awaited possibilities of color television were debated, the FCC's re-allocation map of stations did not come until April 1952, with July 1, 1952, as the official beginning of licensing new stations.\n Other FCC actions hurt the fledgling DuMont and ABC networks. American Telephone and Telegraph (AT&T) forced television coaxial cable users to rent additional radio long lines, discriminating against DuMont, which had no radio network operation. DuMont and ABC protested AT&T's television policies to the FCC, which regulated AT&T's long-line charges, but the commission took no action. The result was that financially marginal DuMont was spending as much in long-line charge as CBS or NBC while using only about 10 to 15 percent of the time and mileage of either larger network.[21]\n The FCC's \"Sixth Report & Order\" ended the Freeze. It took five years for the US to grow from 108 stations to more than 550. New stations came on line slowly, only five by the end of November 1952. The Sixth Report and Order required some existing television stations to change channels, but only a few existing VHF stations were required to move to UHF, and a handful of VHF channels were deleted altogether in smaller media markets like Peoria, Fresno, Bakersfield and Fort Wayne, Indiana to create markets which were UHF \"islands.\" The report also set aside a number of channels for the newly emerging field of educational television, which hindered struggling ABC and DuMont's quest for affiliates in the more desirable markets where VHF channels were reserved for non-commercial use.\n The Sixth Report and Order also provided for the \"intermixture\" of VHF and UHF channels in most markets; UHF transmitters in the 1950s were not yet powerful enough, nor receivers sensitive enough (if they included UHF tuners at all - they were not formally required until the 1960s All-Channel Receiver Act), to make UHF viable against entrenched VHF stations. In markets where there were no VHF stations and UHF was the only TV service available, UHF survived. In other markets, which were too small to financially support a television station, too close to VHF outlets in nearby cities, or where UHF was forced to compete with more than one well-established VHF station, UHF had little chance for success.\n Denver had been the largest U.S. city without a TV station by 1952. Senator Edwin Johnson (D-Colorado), chair of the Senate's Interstate and Foreign Commerce Committee, had made it his personal mission to make Denver the first post-Freeze station. The senator had pressured the FCC, and proved ultimately successful as the first new station (a VHF station) came on-line a remarkable ten days after the commission formally announced the first post-Freeze construction permits. KFEL (now KWGN-TV)'s first regular telecast was on July 21, 1952.[22][23]\n In 1996, Congress enacted the Telecommunications Act of 1996, in the wake of the breakup of AT&T resulting from the U.S. Department of Justice's antitrust suit against AT&T. The legislation attempted to create more competition in local telephone service by requiring Incumbent Local Exchange Carriers to provide access to their facilities for Competitive Local Exchange Carriers. This policy has thus far had limited success and much criticism.[24]\n The development of the Internet, cable services and wireless services has raised questions whether new legislative initiatives are needed as to competition in what has come to be called 'broadband' services. Congress has monitored developments but as of 2009 has not undertaken a major revision of applicable regulation. The Local Community Radio Act in the 111th Congress has gotten out of committee and will go before the house floor with bi-partisan support,[25] and unanimous support of the FCC.[26]\n By passing the Telecommunications Act of 1996, Congress also eliminated the cap on the number of radio stations any one entity could own nationwide and also substantially loosened local radio station ownership restrictions. Substantial radio consolidation followed.[27] Restrictions on ownership of television stations were also loosened.[28] Public comments to the FCC indicated that the public largely believed that the severe consolidation of media ownership had resulted in harm to diversity, localism, and competition in media, and was harmful to the public interest.[29]\n David A. Bray joined the commission in 2013 as chief information officer and quickly announced goals of modernizing the FCC's legacy information technology (IT) systems, citing 200 different systems for only 1750 people a situation he found \"perplexing\".[30][31] These efforts later were documented in a 2015 Harvard Case Study.[32][33] In 2017, Christine Calvosa replaced Bray as the acting CIO of FCC.[34]\n On January 4, 2023, the FCC voted unanimously to create a newly formed Space Bureau and Office of International Affairs within the agency, replacing the existing International Bureau. FCC chairwoman Jessica Rosenworcel explained that the move was done to improve the FCC's \"coordination across the federal government\" and to \"support the 21st-century satellite industry.\"[35] The decision to establish the Space Bureau was reportedly done to improve the agency's capacity to regulate Satellite Internet access.[36] The new bureau officially launched on April 11, 2023.[37]\n The commissioners of the FCC as of December\u00a022, 2024[update]:\n The initial group of FCC commissioners after establishment of the commission in 1934 comprised the following seven members:[18][38]\n The complete list of commissioners is available on the FCC website.[38] Frieda B. Hennock (D-NY) was the first female commissioner of the FCC in 1948.\n The FCC regulates broadcast stations, repeater stations as well as commercial broadcasting operators who operate and repair certain radiotelephone, radio and television stations. Broadcast licenses are to be renewed if the station meets the \"public interest, convenience, or necessity\".[41] The FCC's enforcement powers include fines and broadcast license revocation (see FCC MB Docket 04-232). Burden of proof would be on the complainant in a petition to deny.\n The FCC first promulgated rules for cable television in 1965, with cable and satellite television now regulated by the FCC under Title VI of the Communications Act. Congress added Title VI in the Cable Communications Policy Act of 1984, and made substantial modifications to Title VI in the Cable Television and Consumer Protection and Competition Act of 1992. Further modifications to promote cross-modal competition (telephone, video, etc.) were made in the Telecommunications Act of 1996, leading to the current regulatory structure.[42]\n Broadcast television and radio stations are subject to FCC regulations including restrictions against indecency or obscenity. The Supreme Court has repeatedly held, beginning soon after the passage of the Communications Act of 1934, that the inherent scarcity of radio spectrum allows the government to impose some types of content restrictions on broadcast license holders notwithstanding the First Amendment.[43] Cable and satellite providers are also subject to some content regulations under Title VI of the Communications Act such as the prohibition on obscenity, although the limitations are not as restrictive compared to broadcast stations.[44]\n The 1981 inauguration of Ronald Reagan as President of the United States accelerated an already ongoing shift in the FCC towards a decidedly more market-oriented stance. A number of regulations felt to be outdated were removed, most controversially the Fairness Doctrine in 1987.\n In terms of indecency fines, there was no action taken by the FCC on the case FCC v. Pacifica until 1987, about ten years after the landmark United States Supreme Court decision that defined the power of the FCC over indecent material as applied to broadcasting.[45][46]\n After the 1990s had passed, the FCC began to increase its censorship and enforcement of indecency regulations in the early 2000s to include a response to the Janet Jackson \"wardrobe malfunction\" that occurred during the halftime show of Super Bowl XXXVIII.[47]\n Then on June 15, 2006, President George W. Bush signed into law the Broadcast Decency Enforcement Act of 2005 sponsored by then-Senator Sam Brownback, a former broadcaster himself, and endorsed by Congressman Fred Upton of Michigan who authored a similar bill in the United States House of Representatives. The new law stiffens the penalties for each violation of the Act. The Federal Communications Commission will be able to impose fines in the amount of $325,000 for each violation by each station that violates decency standards. The legislation raised the fine ten times over the previous maximum of $32,500 per violation.[48][49]\n The FCC has established rules limiting the national share of media ownership of broadcast radio or television stations. It has also established cross-ownership rules limiting ownership of a newspaper and broadcast station in the same market, in order to ensure a diversity of viewpoints in each market and serve the needs of each local market.\n In the second half of 2006, groups such as the National Hispanic Media Coalition, the National Latino Media Council, the National Association of Hispanic Journalists, the National Institute for Latino Policy, the League of United Latin American Citizens (LULAC) and others held town hall meetings[50] in California, New York and Texas on media diversity as its effects Latinos and minority communities. They documented widespread and deeply felt community concerns about the negative effects of media concentration and consolidation on racial-ethnic diversity in staffing and programming.[51] At these Latino town hall meetings, the issue of the FCC's lax monitoring of obscene and pornographic material in Spanish-language radio and the lack of racial and national-origin diversity among Latino staff in Spanish-language television were other major themes.\n President Barack Obama appointed Mark Lloyd to the FCC in the newly created post of associate general counsel/chief diversity officer.[52]\n Numerous controversies have surrounded the city of license concept as the internet has made it possible to broadcast a single signal to every owned station in the nation at once, particularly when Clear Channel, now IHeartMedia, became the largest FM broadcasting corporation in the US after the Telecommunications Act of 1996 became law - owning over 1,200 stations at its peak. As part of its license to buy more radio stations, Clear Channel was forced to divest all TV stations.[citation needed]\n To facilitate the adoption of digital television, the FCC issued a second digital TV (DTV) channel to each holder of an analog TV station license. All stations were required to buy and install all new equipment (transmitters, TV antennas, and even entirely new broadcast towers), and operate for years on both channels. Each licensee was required to return one of their two channels following the end of the digital television transition.[citation needed]\n After delaying the original deadlines of 2006, 2008, and eventually February 17, 2009, on concerns about elderly and rural folk, on June 12, 2009, all full-power analog terrestrial TV licenses in the U.S. were terminated as part of the DTV transition, leaving terrestrial television available only from digital channels and a few low-power LPTV stations. To help U.S. consumers through the conversion, Congress established a federally sponsored DTV Converter Box Coupon Program for two free converters per household.[citation needed]\n The FCC regulates telecommunications services under Title II of the Communications Act of 1934. Title II imposes common carrier regulation under which carriers offering their services to the general public must provide services to all customers and may not discriminate based on the identity of the customer or the content of the communication. This is similar to and adapted from the regulation of transportation providers (railroad, airline, shipping, etc.) and some public utilities. Wireless carriers providing telecommunications services are also generally subject to Title II regulation except as exempted by the FCC.[53]\n The FCC regulates interstate telephone services under Title II. The Telecommunications Act of 1996 was the first major legislative reform since the 1934 act and took several steps to de-regulate the telephone market and promote competition in both the local and long-distance marketplace.\n The important relationship of the FCC and the American Telephone and Telegraph (AT&T) Company evolved over the decades. For many years, the FCC and state officials agreed to regulate the telephone system as a natural monopoly.[54] The FCC controlled telephone rates and imposed other restrictions under Title II to limit the profits of AT&T and ensure nondiscriminatory pricing.\n In the 1960s, the FCC began allowing other long-distance companies, namely MCI, to offer specialized services. In the 1970s, the FCC allowed other companies to expand offerings to the public.[55] A lawsuit in 1982 led by the Justice Department after AT&T underpriced other companies, resulted in the breakup of the Bell System from AT&T. Beginning in 1984, the FCC implemented a new goal that all long-distance companies had equal access to the local phone companies' customers.[56] Effective January 1, 1984, the Bell System's many member-companies were variously merged into seven independent \"Regional Holding Companies\", also known as Regional Bell Operating Companies (RBOCs), or \"Baby Bells\". This divestiture reduced the book value of AT&T by approximately 70%.[57]\n The FCC initially exempted \"information services\" such as broadband Internet access from regulation under Title II. The FCC held that information services were distinct from telecommunications services that are subject to common carrier regulation.\n However, Section 706 of the Telecommunications Act of 1996 required the FCC to help accelerate deployment of \"advanced telecommunications capability\" which included high-quality voice, data, graphics, and video, and to regularly assess its availability. In August 2015, the FCC said that nearly 55\u00a0million Americans did not have access to broadband capable of delivering high-quality voice, data, graphics and video offerings.[58]\n On February 26, 2015, the FCC reclassified broadband Internet access as a telecommunications service, thus subjecting it to Title II regulation, although several exemptions were also created. The reclassification was done in order to give the FCC a legal basis for imposing net neutrality rules (see below), after earlier attempts to impose such rules on an \"information service\" had been overturned in court.\n In 2005, the FCC formally established the following principles: To encourage broadband deployment and preserve and promote the open and interconnected nature of the public Internet, Consumers are entitled to access the lawful Internet content of their choice; Consumers are entitled to run applications and use services of their choice, subject to the needs of law enforcement; Consumers are entitled to connect their choice of legal devices that do not harm the network; Consumers are entitled to competition among network providers, application and service providers, and content providers. However, broadband providers were permitted to engage in \"reasonable network management.\"[59]\n On August 1, 2008, the FCC formally voted 3-to-2 to uphold a complaint against Comcast, the largest cable company in the US, ruling that it had illegally inhibited users of its high-speed Internet service from using file-sharing software. The FCC imposed no fine, but required Comcast to end such blocking in 2008. FCC chairman Kevin J. Martin said the order was meant to set a precedent that Internet providers, and indeed all communications companies, could not prevent customers from using their networks the way they see fit unless there is a good reason. In an interview Martin stated that \"We are preserving the open character of the Internet\" and \"We are saying that network operators can't block people from getting access to any content and any applications.\" Martin's successor, Julius Genachowski has maintained that the FCC has no plans to regulate the internet, saying: \"I've been clear repeatedly that we're not going to regulate the Internet.\"[60] The Comcast case highlighted broader issues of whether new legislation is needed to force Internet providers to maintain net neutrality, i.e. treat all uses of their networks equally. The legal complaint against Comcast related to BitTorrent, software that is commonly used for downloading larger files.[61]\n In December 2010, the FCC revised the principles from the original Internet policy statement and adopted the Open Internet Order consisting of three rules[62] regarding the Internet: Transparency. Fixed and mobile broadband providers must disclose the network management practices, performance characteristics, and terms and conditions of their broadband services; No blocking. Fixed broadband providers may not block lawful content, applications, services, or non-harmful devices; mobile broadband providers may not block lawful websites, or block applications that compete with their voice or video telephony services; and No unreasonable discrimination.\n On January 14, 2014, Verizon won their lawsuit over the FCC in the United States Court of Appeals for the District of Columbia Court. Verizon was suing over increased regulation on internet service providers on the grounds that \"even though the commission has general authority to regulate in this arena, it may not impose requirements that contravene express statutory mandates. Given that the commission has chosen to classify broadband providers in a manner that exempts them from treatment as common carriers, the Communications Act expressly prohibits the commission from nonetheless regulating them as such.\"[63]\n After these setbacks in court, in April 2014 the FCC issued a Notice of Proposed Rulemaking regarding a path forward for The Open Internet Order. On November 10, 2014, President Obama created a YouTube video[64] recommending that the FCC reclassify broadband Internet service as a telecommunications service in order to preserve net neutrality.[65][66][67]\n On February 26, 2015, the FCC ruled in favor of net neutrality by applying Title II (common carrier) of the Communications Act of 1934 and Section 706 of the Telecommunications act of 1996 to the Internet.[68][69][70]\n The rules prompted debate about the applicability of First Amendment protections to Internet service providers and edge providers. Republican commissioner Ajit Pai said the Open Internet Order \"posed a special danger\" to \"First Amendment speech, freedom of expression, [and] even freedom of association.\"[71] Democratic member and then-Chairman Tom Wheeler said in response that the rules were \"no more a plan to regulate the Internet than the First Amendment is a plan to regulate free speech. They both stand for the same concept.\"[72] According to a Washington Post poll, 81% of Americans supported net neutrality in 2014, with 81% of Democrats and 85% of Republicans saying they opposed allowing Internet providers to charge websites for faster speeds.[73]\n On March 12, 2015, the FCC released the specific details of the net neutrality rules.[74][75][76] On April 13, 2015, the FCC published the final rule on its new \"Net Neutrality\" regulations.[77][78]\n On April 27, 2017, FCC chairman Ajit Pai released a draft Notice of Proposed Rulemaking that would revise the legal foundation for the agency's Open Internet regulations. The NPRM was voted on at the May 18th Open Meeting.[79] On December 14, the commission voted 3\u20132 in favor of passing the repeal of the 2015 rules.[80] The repeal formally took effect on June 11, 2018, when the 2015 rules expired.[81][82]\n However, in April 2024, the FCC re-adopted the net neutrality rules by 3\u20132 vote, prohibiting internet service providers from blocking or limiting user access, reviving the regulations repealed in 2017.[83]\n On January 2, 2025, the United States Court of Appeals for the Sixth Circuit ruled that the FCC lacked the authority to adopt net neutrality regulations in April 2024. The court held that the regulations violate federal law because broadband Internet service providers are classified as information service providers, not telecommunications service providers.[84][85]\n When it emerged in 2006 that AT&T, BellSouth and Verizon may have broken U.S. laws by aiding the National Security Agency in possible illegal wiretapping of its customers, Congressional representatives called for an FCC investigation into whether or not those companies broke the law. The FCC declined to investigate, however, claiming that it could not investigate due to the classified nature of the program \u2013 a move that provoked the criticism of members of Congress.[citation needed]\n \"Today the watchdog agency that oversees the country's telecommunications industry refused to investigate the nation's largest phone companies' reported disclosure of phone records to the NSA\", said Rep. Edward Markey (D-Mass.) in response to the decision. \"The FCC, which oversees the protection of consumer privacy under the Communications Act of 1934, has taken a pass at investigating what is estimated to be the nation's largest violation of consumer privacy ever to occur. If the oversight body that monitors our nation's communications is stepping aside then Congress must step in.\"[86]\n The FCC regulates all non-Federal uses of radio frequency spectrum in the United States under Title III of the Communications Act of 1934. In addition to over-the-air broadcast television and radio stations, this includes commercial mobile (i.e., mobile phone) services, amateur radio, citizen's band radio, theatrical wireless microphone installations, and a very wide variety of other services. Use of radio spectrum by U.S. federal government agencies is coordinated by the National Telecommunications and Information Administration, an agency within the Department of Commerce.[citation needed]\n Commercial mobile radio service (CMRS) providers, including all mobile phone carriers, are subject to spectrum and wireless regulations under Title III (similar to broadcasters) as well as common carrier regulations under Title II (similar to wireline telephone carriers), except as provided by the FCC.[87]\n Beginning in 1994, the FCC has usually assigned commercial spectrum licenses through the use of competitive bidding, i.e., spectrum auctions. These auctions have raised tens of billions of dollars for the U.S. Treasury, and the FCC's auction approach is now widely emulated throughout the world. The FCC typically obtains spectrum for auction that has been reclaimed from other uses, such as spectrum returned by television broadcasters after the digital television transition, or spectrum made available by federal agencies able to shift their operations to other bands.[citation needed]\n Normally, any intentional radio transmission requires an FCC license pursuant to Title III. However, in recent decades the FCC has also opened some spectrum bands for unlicensed operations, typically restricting them to low power levels conducive to short-range applications. This has facilitated the development of a very wide range of common technologies from wireless garage door openers, cordless phones, and baby monitors to Wi-Fi and Bluetooth among others. However, unlicensed devices \u2014 like most radio transmission equipment \u2014 must still receive technical approval from the FCC before being sold into the marketplace, including ensuring that such devices cannot be modified by end users to increase transmit power above FCC limits.[citation needed]\n \"White spaces\" are radio frequencies that went unused after the federally mandated transformation of analog TV signals to digital. On October 15, 2008, FCC Chairman Kevin Martin announced his support for the unlicensed use of white spaces. Martin said he was \"hoping to take advantage of utilizing these airwaves for broadband services to allow for unlicensed technologies and new innovations in that space.\"[88]\n Google, Microsoft and other companies are vying for the use of this white-space to support innovation in Wi-Fi technology. Broadcasters and wireless microphone manufacturers fear that the use of white space would \"disrupt their broadcasts and the signals used in sports events and concerts.\"[89][better\u00a0source\u00a0needed] Cell phone providers such as T-Mobile US have mounted pressure on the FCC to instead offer up the white space for sale to boost competition and market leverage.\n On November 4, 2008, the FCC commissioners unanimously agreed to open up unused broadcast TV spectrum for unlicensed use.[90][91]\n Amateur radio operators in the United States must be licensed by the FCC before transmitting. While the FCC maintains control of the written testing standards, it no longer administers the exams, having delegated that function to private volunteer organizations.[92] No amateur license class requires examination in Morse code; neither the FCC nor the volunteer organizations test code skills for amateur licenses.[93]\n An FCC database provides information about the height and year built of broadcasting towers in the US.[94] It does not contain information about the structural types of towers or about the height of towers used by Federal agencies, such as most NDBs, LORAN-C transmission towers or VLF transmission facilities of the US Navy, or about most towers not used for transmission like the BREN Tower. These are instead tracked by the Federal Aviation Administration as obstructions to air navigation.\n In 2023, Andrew Tisinger criticized the FCC for ignoring international open standards, and instead choosing proprietary closed standards, or allowing communications companies to do so and implement the anticompetitive practice of vendor lock-in.[95]\n In the case of digital TV, it chose the ATSC standard, even though DVB was already in use around the world, including DVB-S satellite TV in the U.S. Unlike competing standards, the ATSC system is encumbered by numerous patents, and therefore royalties that make TV sets and DTV converters much more expensive than in the rest of the world. Additionally, the claimed benefit of better reception in rural areas is more than negated in urban areas by multipath interference, which other systems are nearly immune to. It also cannot be received while in motion for this reason, while all other systems can, even without dedicated mobile TV signals or receivers.[citation needed]\n For digital radio, the FCC chose proprietary HD Radio, which crowds the existing FM broadcast band and even AM broadcast band with in-band adjacent-channel sidebands, which create noise in other stations. This is in contrast to worldwide DAB, which uses unused TV channels in the VHF band III range. This too has patent fees, while DAB does not. While there has been some effort by iBiquity to lower them,[96] the fees for HD Radio are still an enormous expense when converting each station, and this fee structure presents a potentially high cost barrier to entry for community radio and other non-commercial educational stations when entering the HD Radio market.[97] (Under the subsidiary communications authority principle, FM stations could in theory use any in-band on-channel digital system of their choosing; a competing service, FMeXtra, briefly gained some traction in the early 21st century but has since been discontinued.)\n Satellite radio (also called SDARS by the FCC) uses two proprietary standards instead of DAB-S, which requires users to change equipment when switching from one provider to the other, and prevents other competitors from offering new choices as stations can do on terrestrial radio. Had the FCC picked DAB-T for terrestrial radio, no separate satellite receiver would have been needed at all, and the only difference from DAB receivers in the rest of the world would be the need to tune S band instead of L band.[citation needed]\n In mobile telephony, the FCC abandoned the \"any lawful device\" principle[failed verification] decided against AT&T landlines, and has instead allowed each mobile phone company to dictate what its customers can use.[98][99]\n As the public interest standard has always been important to the FCC when determining and shaping policy, so too has the relevance of public involvement in U.S. communication policy making.[100] The FCC Record is the comprehensive compilation of decisions, reports, public notices, and other documents of the FCC, published since 1986.[101][102]\n In the 1927 Radio Act, which was formulated by the predecessor of the FCC (the Federal Radio Commission), section 4(k) stipulated that the commission was authorized to hold hearings for the purpose of developing a greater understanding of the issues for which rules were being crafted. Section 4(k) stated that:\n Thus, it is clear that public consultation, or at least consultation with outside bodies was regarded as central to the commission's job from early on. Though it should not be surprising, the act also stipulated that the commission should verbally communicate with those being assigned licenses. Section 11 of the act noted:\n As early as 1927, there is evidence that public hearings were indeed held; among them, hearings to assess the expansion of the radio broadcast band.[103] At these early hearings, the goal of having a broad range of viewpoints presented was evident, as not only broadcasters, but also radio engineers and manufacturers were in attendance. Numerous groups representing the general public appeared at the hearings as well, including amateur radio operators and inventors as well as representatives of radio listeners' organizations.\n While some speakers at the 1927 hearings referred to having received \"invitations,\" Herbert Hoover's assistant observed in a letter at the time that \"the Radio Commission has sent out a blanket invitation to all people in the country who desire either to appear in person or to submit their recommendations in writing. I do not understand that the commission has sent for any particular individuals, however\" [Letter from George Akerson, assistant to Sec. Hoover, to Mrs. James T. Rourke, Box 497, Commerce Period Papers, Herbert Hoover Presidential Library (March 29, 1927)] (FN 14)[103] Including members of the general public in the discussion was regarded (or at least articulated) as very important to the commission's deliberations. In fact, FCC commissioner Bellows noted at the time that \"it is the radio listener we must consider above everyone else.\"[103] Though there were numerous representatives of the general public at the hearing, some expressing their opinions to the commission verbally, overall there was not a great turnout of everyday listeners at the hearings.\n Though not a constant fixture of the communications policy-making process, public hearings were occasionally organized as a part of various deliberation processes as the years progressed. For example, seven years after the enactment of the Radio Act, the Communications Act of 1934 was passed, creating the FCC. That year the federal government's National Recovery Agency (associated with the New Deal period) held public hearings as a part of its deliberations over the creation of new broadcasting codes.[104]\n A few years later [when?], the FCC held hearings to address early cross-ownership issues; specifically, whether newspaper companies owning radio stations was in the public interest.[105] These \"newspaper divorcement hearings\" were held between 1941 and 1944, though it appears that these hearings were geared mostly towards discussion by industry stakeholders. Around the same time, the commission held hearings as a part of its evaluation of the national television standard,[106] and in 1958 held additional hearings on the television network broadcasting rules.[107] Though public hearings were organized somewhat infrequently, there was an obvious public appeal. In his now famous \"vast wasteland\" speech in 1961, FCC chairman Newton Minow noted that the commission would hold a \"well advertised public hearing\" in each community to assure broadcasters were serving the public interest,[108] clearly a move to reconnect the commission with the public interest (at least rhetorically).\n On September 5, 2023, commissioner Nathan Simington held a public forum on the tech-focused social news site, Hacker News.[109]\n"
    },
    {
        "title": "Criminology",
        "url": "https://en.wikipedia.org/wiki/Criminology",
        "content": "\n 1800s: Martineau \u00b7 Tocqueville\u00a0\u00b7\u00a0Marx \u00b7\u00a0Spencer \u00b7 Le Bon  \u00b7 Ward \u00b7 Pareto \u00b7\u00a0T\u00f6nnies \u00b7 Veblen \u00b7\u00a0Simmel \u00b7 Durkheim \u00b7\u00a0Addams \u00b7\u00a0Mead \u00b7 Weber \u00b7\u00a0Du Bois \u00b7\u00a0Mannheim \u00b7 Elias\n Criminology (from Latin crimen, 'accusation', and Ancient Greek -\u03bb\u03bf\u03b3\u03af\u03b1, -logia, from \u03bb\u03cc\u03b3\u03bf\u03c2 logos, 'word, reason') is the interdisciplinary study of crime and deviant behaviour.[1] Criminology is a multidisciplinary field in both the behavioural and social sciences, which draws primarily upon the research of sociologists, political scientists, economists, legal sociologists, psychologists, philosophers, psychiatrists, social workers, biologists, social anthropologists, scholars of law and jurisprudence, as well as the processes that define administration of justice and the criminal justice system.\n The interests of criminologists include the study of nature of crime and criminals, origins of criminal law, etiology of crime, social reaction to crime, and the functioning of law enforcement agencies and the penal institutions. It can be broadly said that criminology directs its inquiries along three lines: first, it investigates the nature of criminal law and its administration and conditions under which it develops; second, it analyzes the causation of crime and the personality of criminals; and third, it studies the control of crime and the rehabilitation of offenders. Thus, criminology includes within its scope the activities of legislative bodies, law-enforcement agencies, judicial institutions, correctional institutions and educational, private and public social agencies.\n Modern academic criminology has direct roots in the 19th-century Italian School of \"criminal anthropology\", which according to the historian Mary Gibson \"caused a radical refocusing of criminological discussion throughout Europe and the United States from law to the criminal. While this 'Italian School' was in turn attacked and partially supplanted in countries such as France by 'sociological' theories of delinquency, they retained the new focus on the criminal.\"[2] According to Gibson, the term criminology was most likely coined in 1885 by Italian law professor Raffaele Garofalo as Criminologia\u00a0[it].[2] In the late 19th century, French anthropologist Paul Topinard used the analogous French term Criminologie\u00a0[fr].[3]\n Criminology grew substantially as a discipline in the first quarter of the twentieth century. From 1900 through to 2000 this field of research underwent three significant phases in the United States: (1) Golden Age of Research (1900\u20131930) which has been described as a multiple-factor approach, (2) Golden Age of Theory (1930\u20131960) which endeavored to show the limits of systematically connecting criminological research to theory, and (3) a 1960\u20132000 period, which was seen as a significant turning point for criminology.[4]\n There were three main schools of thought in early criminological theory, spanning the period from the mid-18th century to the mid-twentieth century: Classical, Positivist, and Chicago. These schools of thought were superseded by several contemporary paradigms of criminology, such as the sub-culture, control, strain, labelling, critical criminology, cultural criminology, postmodern criminology, feminist criminology, Queer criminology, and others discussed below.\n The Classical school arose in the mid-18th century and reflects ideas from utilitarian philosophy.[5] Cesare Beccaria,[6] author of On Crimes and Punishments (1763\u201364), Jeremy Bentham (inventor of the panopticon), and other early criminological philosophers proposed ideas including:[7]\n This school developed during a major reform in penology when society began designing prisons for the sake of extreme punishment. This period also saw many legal reforms, the French Revolution, and the development of the legal system in the United States.[9]\n The Positivist school argues criminal behaviour comes from internal and external factors out of the individual's control. Its key method of thought is that criminals are born as criminals and not made into them;[10] this school of thought also supports theory of nature in the debate between nature versus nurture. They also argue that criminal behavior is innate and within a person. Philosophers within this school applied the scientific method to study human behavior. Positivism comprises three segments: biological, psychological and social positivism.[11]\n Psychological Positivism is the concept that criminal acts or the people doing said crimes do them because of internal factors driving them.\n Social Positivism, which is often referred to as Sociological Positivism, discusses the thought process that criminals are produced by society. This school claims that low income levels, high poverty/unemployment rates, and poor educational systems create and motivate criminals.[12]\n The notion of having a criminal personality is achieved from the school of thought of psychological positivism. It essentially means that parts of an individual's personality have traits that align with many of those possessed by criminals, such as neuroticism, anti-social tendencies, aggressive behaviors, and other factors. There is evidence of correlation, but not causation, between these personality traits and criminal actions.[13][14][15][16][17][18][19][20]\n Cesare Lombroso (1835\u20131909), an Italian sociologist working in the late 19th century, is often called \"the father of criminology\".[21] He was one of the key contributors to biological positivism and founded the Italian school of criminology.[22] Lombroso took a scientific approach, insisting on empirical evidence for studying crime.[23]  He suggested physiological traits such as the measurements of cheekbones or hairline, or a cleft palate could indicate \"atavistic\" criminal tendencies. This approach, whose influence came via the theory of phrenology and by Charles Darwin's theory of evolution, has been superseded. Enrico Ferri, a student of Lombroso, believed social as well as biological factors played a role, and believed criminals should not be held responsible when factors causing their criminality were beyond their control. Criminologists have since rejected Lombroso's biological theories since control groups were not used in his studies.[24][25]\n Sociological positivism suggests societal factors such as poverty, membership of subcultures, or low levels of education can predispose people to crime. Adolphe Quetelet used data and statistical analysis to study the relationship between crime and sociological factors. He found age, gender, poverty, education, and alcohol consumption were important factors to crime.[26] Lance Lochner performed three different research experiments, each one supporting education reduces crime.[27] Rawson W. Rawson used crime statistics to suggest a link between population density and crime rates, with crowded cities producing more crime.[28] Joseph Fletcher and John Glyde read papers to the Statistical Society of London on their studies of crime and its distribution.[29] Henry Mayhew used empirical methods and an ethnographic approach to address social questions and poverty, and gave his studies in London Labour and the London Poor.[30] \u00c9mile Durkheim viewed crime as an inevitable aspect of a society with uneven distribution of wealth and other differences among people.\n Differential association (sub-cultural) posits that people learn crime through association. This theory was advocated by Edwin Sutherland, who focused on how \"a person becomes delinquent because of an excess of definitions favorable to violation of law over definitions unfavorable to violation of law.\"[31] Associating with people who may condone criminal conduct, or justify crime under specific circumstances makes one more likely to take that view, under his theory. Interacting with this type of \"antisocial\" peer is a major cause of delinquency. Reinforcing criminal behavior makes it chronic. Where there are criminal subcultures, many individuals learn crime, and crime rates swell in those areas.[32]\n The Chicago school arose in the early twentieth century, through the work of Robert E. Park, Ernest Burgess, and other urban sociologists at the University of Chicago.  In the 1920s, Park and Burgess identified five concentric zones that often exist as cities grow, including the \"zone of transition\", which was identified as the most volatile and subject to disorder.  In the 1940s, Henry McKay and Clifford R. Shaw focused on juvenile delinquents, finding that they were concentrated in the zone of transition. The Chicago School was a school of thought developed that blames social structures for human behaviors. This thought can be associated or used within criminology, because it essentially takes the stance of defending criminals and criminal behaviors. The defense and argument lies in the thoughts that these people and their acts are not their faults but they are actually the result of society (i.e. unemployment, poverty, etc.), and these people are actually, in fact, behaving properly.[33]\n Chicago school sociologists adopted a social ecology approach to studying cities and postulated that urban neighborhoods with high levels of poverty often experience a breakdown in the social structure and institutions, such as family and schools.  This results in social disorganization, which reduces the ability of these institutions to control behavior and creates an environment ripe for deviant behavior.\n Other researchers suggested an added social-psychological link.  Edwin Sutherland suggested that people learn criminal behavior from older, more experienced criminals with whom they may associate.\n Theoretical perspectives used in criminology include psychoanalysis, functionalism, interactionism, Marxism, econometrics, systems theory, postmodernism, behavioural genetics, personality psychology, evolutionary psychology, etc.\n This theory is applied to a variety of approaches within the bases of criminology in particular and in sociology more generally as a conflict theory or structural conflict perspective in sociology and sociology of crime. As this perspective is itself broad enough, embracing as it does a diversity of positions.[34]\n Social disorganization theory is based on the work of Henry McKay and Clifford R. Shaw of the Chicago School.[35]  Social disorganization theory postulates that neighborhoods plagued with poverty and economic deprivation tend to experience high rates of population turnover.[36] This theory suggests that crime and deviance is valued within groups in society, 'subcultures' or 'gangs'. These groups have different values to the social norm. These neighborhoods also tend to have high population heterogeneity.[36] With high turnover, informal social structure often fails to develop, which in turn makes it difficult to maintain social order in a community.\n Since the 1950s, social ecology studies have built on the social disorganization theories.  Many studies have found that crime rates are associated with poverty, disorder, high numbers of abandoned buildings, and other signs of community deterioration.[36][37] As working and middle-class people leave deteriorating neighborhoods, the most disadvantaged portions of the population may remain. William Julius Wilson suggested a poverty \"concentration effect\", which may cause neighborhoods to be isolated from the mainstream of society and become prone to violence.[38]\n Strain theory, also known as Mertonian Anomie, advanced by American sociologist Robert Merton, suggests that mainstream culture, especially in the United States, is saturated with dreams of opportunity, freedom, and prosperity\u2014as Merton put it, the American Dream. Most people buy into this dream, and it becomes a powerful cultural and psychological motivator. Merton also used the term anomie, but it meant something slightly different for him than it did for Durkheim.  Merton saw the term as meaning a dichotomy between what society expected of its citizens and what those citizens could actually achieve.  Therefore, if the social structure of opportunities is unequal and prevents the majority from realizing the dream, some of those dejected will turn to illegitimate means (crime) in order to realize it. Others will retreat or drop out into deviant subcultures (such as gang members, or what he calls \"hobos\"). Robert Agnew developed this theory further to include types of strain which were not derived from financial constraints. This is known as general strain theory.[39]\n Following the Chicago school and strain theory, and also drawing on Edwin Sutherland's idea of differential association, sub-cultural theorists focused on small cultural groups fragmenting away from the mainstream to form their own values and meanings about life.\n Albert K. Cohen tied anomie theory with Sigmund Freud's reaction formation idea, suggesting that delinquency among lower-class youths is a reaction against the social norms of the middle class.[40]  Some youth, especially from poorer areas where opportunities are scarce, might adopt social norms specific to those places that may include \"toughness\" and disrespect for authority.  Criminal acts may result when youths conform to norms of the deviant subculture.[41]\n Richard Cloward and Lloyd Ohlin suggested that delinquency can result from a differential opportunity for lower class youth.[42]  Such youths may be tempted to take up criminal activities, choosing an illegitimate path that provides them more lucrative economic benefits than conventional, over legal options such as minimum wage-paying jobs available to them.[42]\n Delinquency tends to occur among the lower-working-class males who have a lack of resources available to them and live in impoverished areas, as mentioned extensively by Albert Cohen (Cohen, 1965). Bias has been known to occur among law enforcement agencies, where officers tend to place a bias on minority groups, without knowing for sure if they had committed a crime or not.\n British sub-cultural theorists focused more heavily on the issue of class, where some criminal activities were seen as \"imaginary solutions\" to the problem of belonging to a subordinate class. A further study by the Chicago school looked at gangs and the influence of the interaction of gang leaders under the observation of adults.\n Sociologists such as Raymond D. Gastil have explored the impact of a Southern culture of honor on violent crime rates.[43]\n Another approach is made by the social bond or social control theory. Instead of looking for factors that make people become criminal, these theories try to explain why people do not become criminal. Travis Hirschi identified four main characteristics: \"attachment to others\", \"belief in moral validity of rules\", \"commitment to achievement\", and \"involvement in conventional activities\".[44]  The more a person features those characteristics, the less likely he or she is to become deviant (or criminal).  On the other hand, if these factors are not present, a person is more likely to become a criminal. Hirschi expanded on this theory with the idea that a person with low self-control is more likely to become criminal. As opposed to most criminology theories, these do not look at why people commit crime but rather why they do not commit crime.[45]\n A simple example: Someone wants a big yacht but does not have the means to buy one.  If the person cannot exert self-control, he or she might try to get the yacht (or the means for it) in an illegal way, whereas someone with high self-control will (more likely) either wait, deny themselves of what want or seek an intelligent intermediate solution, such as joining a yacht club to use a yacht by group consolidation of resources without violating social norms.\n Social bonds, through peers, parents, and others can have a countering effect on one's low self-control.  For families of low socio-economic status, a factor that distinguishes families with delinquent children, from those who are not delinquent, is the control exerted by parents or chaperonage.[46] In addition, theorists such as David Matza and Gresham Sykes argued that criminals are able to temporarily neutralize internal moral and social-behavioral constraints through techniques of neutralization.\n Psychoanalysis is a psychological theory (and therapy) which regards the unconscious mind, repressed memories and trauma, as the key drivers of behavior, especially deviant behavior.[47] Sigmund Freud talks about how the unconscious desire for pain relates to psychoanalysis in his essay, Beyond the Pleasure Principle,.[47] Freud suggested that unconscious impulses such as 'repetition compulsion' and a 'death drive' can dominate a person's creativity, leading to self-destructive behavior. Phillida Rosnick, in the article Mental Pain and Social Trauma, posits a difference in the thoughts of individuals suffering traumatic unconscious pain which corresponds to them having thoughts and feelings which are not reflections of their true selves. There is enough correlation between this altered state of mind and criminality to suggest causation.[48] Sander Gilman, in the article Freud and the Making of Psychoanalysis, looks for evidence in the physical mechanisms of the human brain and the nervous system and suggests there is a direct link between an unconscious desire for pain or punishment and the impulse to commit crime or deviant acts.[49]\n Symbolic interactionism draws on the phenomenology of Edmund Husserl and George Herbert Mead, as well as subcultural theory and conflict theory.[50] This school of thought focused on the relationship between state, media, and conservative-ruling elite and other less powerful groups. The powerful groups had the ability to become the \"significant other\" in the less powerful groups' processes of generating meaning. The former could to some extent impose their meanings on the latter; therefore they were able to \"label\" minor delinquent youngsters as criminal. These youngsters would often take the label on board, indulge in crime more readily, and become actors in the \"self-fulfilling prophecy\" of the powerful groups. Later developments in this set of theories were by Howard Becker and Edwin Lemert, in the mid-20th century.[51] Stanley Cohen developed the concept of \"moral panic\" describing the societal reaction to spectacular, alarming social phenomena (e.g. post-World War 2 youth cultures like the Mods and Rockers in the UK in 1964, AIDS epidemic and football hooliganism).\n Labeling theory refers to an individual who is labeled by others in a particular way. The theory was studied in great detail by Becker.[52] It was originally derived from sociology, but is regularly used in criminological studies. When someone is given the label of a criminal they may reject or accept it and continue to commit crime. Even those who initially reject the label can eventually accept it as the label becomes more well known, particularly among their peers. This stigma can become even more profound when the labels are about deviancy, and it is thought that this stigmatization can lead to deviancy amplification. Malcolm Klein conducted a test which showed that labeling theory affected some youth offenders but not others.[53]\n At the other side of the spectrum, criminologist Lonnie Athens developed a theory about how a process of brutalization by parents or peers that usually occurs in childhood results in violent crimes in adulthood. Richard Rhodes' Why They Kill describes Athens' observations about domestic and societal violence in the criminals' backgrounds. Both Athens and Rhodes reject the genetic inheritance theories.[54]\n Rational choice theory is based on the utilitarian, classical school philosophies of Cesare Beccaria, which were popularized by Jeremy Bentham.  They argued that punishment, if certain, swift, and proportionate to the crime, was a deterrent for crime, with risks outweighing possible benefits to the offender. In Dei delitti e delle pene (On Crimes and Punishments, 1763\u20131764), Beccaria advocated a rational penology. Beccaria conceived of punishment as the necessary application of the law for a crime; thus, the judge was simply to confirm his or her sentence to the law. Beccaria also distinguished between crime and sin, and advocated against the death penalty, as well as torture and inhumane treatments, as he did not consider them as rational deterrents.\n This philosophy was replaced by the positivist and Chicago schools and was not revived until the 1970s with the writings of James Q. Wilson, Gary Becker's 1965 article Crime and Punishment[55] and George Stigler's 1970 article The Optimum Enforcement of Laws.[56] Rational choice theory argues that criminals, like other people, weigh costs or risks and benefits when deciding whether to commit crime and think in economic terms.[57] They will also try to minimize risks of crime by considering the time, place, and other situational factors.[57]\n Becker, for example, acknowledged that many people operate under a high moral and ethical constraint but considered that criminals rationally see that the benefits of their crime outweigh the cost, such as the probability of apprehension and conviction, severity of punishment, as well as their current set of opportunities. From the public policy perspective, since the cost of increasing the fine is marginal to that of the cost of increasing surveillance, one can conclude that the best policy is to maximize the fine and minimize surveillance.\n With this perspective, crime prevention or reduction measures can be devised to increase the effort required to commit the crime, such as target hardening.[58] Rational choice theories also suggest that increasing risk and likelihood of being caught, through added surveillance, law enforcement presence, added street lighting, and other measures, are effective in reducing crime.[58]\n One of the main differences between this theory and Bentham's rational choice theory, which had been abandoned in criminology, is that if Bentham considered it possible to completely annihilate crime (through the panopticon), Becker's theory acknowledged that a society could not eradicate crime beneath a certain level. For example, if 25% of a supermarket's products were stolen, it would be very easy to reduce this rate to 15%, quite easy to reduce it until 5%, difficult to reduce it under 3% and nearly impossible to reduce it to zero (a feat which the measures required would cost the supermarket so much that it would outweigh the benefits). This reveals that the goals of utilitarianism and classical liberalism have to be tempered and reduced to more modest proposals to be practically applicable.\n Such rational choice theories, linked to neoliberalism, have been at the basics of crime prevention through environmental design and underpin the Market Reduction Approach to theft [59] by Mike Sutton, which is a systematic toolkit for those seeking to focus attention on \"crime facilitators\" by tackling the markets for stolen goods[60] that provide motivation for thieves to supply them by theft.[61]\n Routine activity theory, developed by Marcus Felson and Lawrence Cohen, draws upon control theories and explains crime in terms of crime opportunities that occur in everyday life.[62] A crime opportunity requires that elements converge in time and place including a motivated offender, suitable target or victim, and lack of a capable guardian.[63] A guardian at a place, such as a street, could include security guards or even ordinary pedestrians who would witness the criminal act and possibly intervene or report it to law enforcement.[63] Routine activity theory was expanded by John Eck, who added a fourth element of \"place manager\" such as rental property managers who can take nuisance abatement measures.[64]\n Biosocial criminology is an interdisciplinary field that aims to explain crime and antisocial behavior by exploring both biological factors and environmental factors. While contemporary criminology has been dominated by sociological theories, biosocial criminology also recognizes the potential contributions of fields such as behavioral genetics, personality psychology, and evolutionary psychology.[65] Various theoretical frameworks such as evolutionary neuroandrogenic theory have sought to explain trends in criminality through the lens of evolutionary biology. Specifically, they seek to explain why criminality is so much higher in men than in women and why young men are most likely to exhibit criminal behavior.[66] See also: genetics of aggression.\n Aggressive behavior has been associated with abnormalities in three principal regulatory systems in the body: serotonin systems, catecholamine systems, and the hypothalamic-pituitary-adrenocortical axis. Abnormalities in these systems also are known to be induced by stress, either severe, acute stress or chronic low-grade stress.[67]\n Biosocial approaches remain very controversial within the scientific field.[68][69]\n In 1968, young British sociologists formed the National Deviance Conference (NDC) group. The group was restricted to academics and consisted of 300 members. Ian Taylor, Paul Walton and Jock Young \u2013 members of the NDC \u2013 rejected previous explanations of crime and deviance. Thus, they decided to pursue a new Marxist criminological approach.[70] In The New Criminology, they argued against the biological \"positivism\" perspective represented by Lombroso, Hans Eysenck and Gordon Trasler.[71]\n According to the Marxist perspective on crime, \"defiance is normal \u2013 the sense that men are now consciously involved ... in assuring their human diversity.\" Thus Marxists criminologists argued in support of society in which the facts of human diversity, be it social or personal, would not be criminalized.[72] They further attributed the processes of crime creation not to genetic or psychological facts, but rather to the material basis of a given society.[73]\n State crime is a distinct field of crimes that is studied by Marxist criminology, which considers these crimes to be some of the most costly to society in terms of overall harm/injury. In a Marxist framework, genocides, environmental degradation, and war are not crimes that occur out of contempt for one's fellow man, but are crimes of power. They continue systems of control and hegemony which allow state crime and state-corporate crime, along with state-corporate non-profit criminals, to continue governing people.[74]\n Convict criminology is a school of thought in the realm of criminology. Convict criminologists have been directly affected by the criminal justice system, oftentimes having spent years inside the prison system. Researchers in the field of convict criminology such as John Irwin and Stephan Richards argue that traditional criminology can better be understood by those who lived in the walls of a prison.[75] Martin Leyva argues that \"prisonization\" oftentimes begins before prison, in the home, community, and schools.[76]\n According to Rod Earle, Convict Criminology started in the United States after the major expansion of prisons in the 1970s, and the U.S. still remains the main focus for those who study convict criminology.[77]\n Queer criminology is a field of study that focuses on LGBT individuals and their interactions with the criminal justice system. The goals of this field of study are as follows:\n Legitimacy of Queer criminology:\n The value of pursuing criminology from a queer theorist perspective is contested; some believe that it is not worth researching and not relevant to the field as a whole, and as a result is a subject that lacks a wide berth of research available. On the other hand, it could be argued that this subject is highly valuable in highlighting how LGBT individuals are affected by the criminal justice system. This research also has the opportunity to \"queer\" the curriculum of criminology in educational institutions by shifting the focus from controlling and monitoring LGBT communities to liberating and protecting them.[78]\n As more and more people identify as something other than heterosexual, queer criminology continues to grow in relevance. At the same time, in jurisdictions such as Russia, Uganda, and Ghana, governments have become even more punitive through laws that expand the criminalisation of LGBTQ+ conduct, relationships, and organising.[79] 'Digiqueer criminology' has emerged as a sub discipline of queer criminology and aims to deepen understanding of the relationship between digital technology, LGBTQ+ identity, and justice.[79][80]\n Cultural criminology views crime and its control within the context of culture.[81][82] Ferrell believes criminologists can examine the actions of criminals, control agents, media producers, and others to construct the meaning of crime.[82] He discusses these actions as a means to show the dominant role of culture.[82] Kane adds that cultural criminology has three tropes; village, city street, and media, in which males can be geographically influenced by society's views on what is broadcast and accepted as right or wrong.[83] The village is where one engages in available social activities. Linking the history of an individual to a location can help determine social dynamics.[83] The city street involves positioning oneself in the cultural area. This is full of those affected by poverty, poor health and crime, and large buildings that impact the city but not neighborhoods.[83] Mass media gives an all-around account of the environment and the possible other subcultures that could exist beyond a specific geographical area.[83]\n It was later that Naegler and Salman introduced feminist theory to cultural criminology and discussed masculinity and femininity, sexual attraction and sexuality, and intersectional themes.[84] Naegler and Salman believed that Ferrell's mold was limited and that they could add to the understanding of cultural criminology by studying women and those who do not fit Ferrell's mold.[84] Hayward would later add that not only feminist theory, but green theory as well, played a role in the cultural criminology theory through the lens of adrenaline, the soft city, the transgressive subject, and the attentive gaze.[81] The adrenaline lens deals with rational choice and what causes a person to have their own terms of availability, opportunity, and low levels of social control.[81] The soft city lens deals with reality outside of the city and the imaginary sense of reality: the world where transgression occurs, where rigidity is slanted, and where rules are bent.[81] The transgressive subject refers to a person who is attracted to rule-breaking and is attempting to be themselves in a world where everyone is against them.[81] The attentive gaze is when someone, mainly an ethnographer, is immersed into the culture and interested in lifestyle(s) and the symbolic, aesthetic, and visual aspects. When examined, they are left with the knowledge that they are not all the same, but come to a settlement of living together in the same space.[81] Through it all, sociological perspective on cultural criminology theory attempts to understand how the environment an individual is in determines their criminal behavior.[82]\n Relative deprivation involves the process where an individual measures his or her own well-being and materialistic worth against that of other people and perceive that they are worse off in comparison.[85] When humans fail to obtain what they believe they are owed, they can experience anger or jealousy over the notion that they have been wrongly disadvantaged.\n Relative deprivation was originally utilized in the field of sociology by Samuel A. Stouffer, who was a pioneer of this theory. Stouffer revealed that soldiers fighting in World War II measured their personal success by the experience in their units rather than by the standards set by the military.[86] Relative deprivation can be made up of societal, political, economic, or personal factors which create a sense of injustice. It is not based on absolute poverty, a condition where one cannot meet a necessary level to maintain basic living standards. Rather, relative deprivation enforces the idea that even if a person is financially stable, he or she can still feel relatively deprived. The perception of being relatively deprived can result in criminal behavior and/or morally problematic decisions.[87] Relative deprivation theory has increasingly been used to partially explain crime as rising living standards can result in rising crime levels. In criminology, the theory of relative deprivation explains that people who feel jealous and discontent of others might turn to crime to acquire the things that they can not afford.\n Rural criminology is the study of crime trends outside of metropolitan and suburban areas. Rural criminologists have used social disorganization and routine activity theories. The FBI Uniform Crime Report shows that rural communities have significantly different crime trends as opposed to metropolitan and suburban areas. The crime in rural communities consists predominantly of narcotic related crimes such as the production, use, and trafficking of narcotics. Social disorganization theory is used to examine the trends involving narcotics.[88] Social disorganization leads to narcotic use in rural areas because of low educational opportunities and high unemployment rates. Routine activity theory is used to examine all low-level street crimes such as theft.[89] Much of the crime in rural areas is explained through routine activity theory because there is often a lack of capable guardians in rural areas.[citation needed]\n Public criminology is a strand within criminology closely tied to ideas associated with \"public sociology\", focused on disseminating criminological insights to a broader audience than academia. Advocates of public criminology argue that criminologists should be \"conducting and disseminating research on crime, law, and deviance in dialogue with affected communities.\"[90] Its goal is for academics and researchers in criminology to provide their research to the public in order to inform public decisions and policymaking. This allows criminologists to avoid the constraints of traditional criminological research.[91] In doing so, public criminology takes on many forms, including media and policy advising as well as activism, civic-oriented education, community outreach, expert testimony, and knowledge co-production.[92]\n Both the positivist and classical schools take a consensus view of crime: that a crime is an act that violates the basic values and beliefs of society.  Those values and beliefs are manifested as laws that society agrees upon. However, there are two types of laws:\n There have been moves in contemporary criminological theory to move away from liberal pluralism, culturalism, and postmodernism by introducing the universal term \"harm\" into the criminological debate as a replacement for the legal term \"crime\".[93][94]\n Areas of study in criminology include:\n"
    },
    {
        "title": "Fastpitch softball",
        "url": "https://en.wikipedia.org/wiki/Fastpitch_softball",
        "content": "Fastpitch softball, or simply fastpitch, is a form of softball played by both women and men. While the teams are most often segregated by sex, coed fast-pitch leagues also exist. Considered the most competitive form of softball, fastpitch is the format played at the Olympic Games. Softball was on the International Olympic Committee (IOC) program in 1996, 2000, 2004, 2008, and 2020. \n The fastpitch style is also used in college softball and other international competition. It is the form used in the American Women's Professional Fastpitch league, a women's professional league whose inaugural season began in June 2022.[1][2]\n Pitchers throw the ball with an underhand motion at speeds up to 77 miles per hour (124\u00a0km/h) for women. Monica Abbott set this record while playing with the Chicago Bandits in June of 2012.[3]\n The pitching style of fastpitch is different from that of slowpitch softball. Pitchers in fast-pitch softball usually throw the ball using a \"windmill\" type of movement. In this style of pitching, the pitcher begins with the arm at the hip. A common way to be taught how to pitch is using the motions, 'repel', 'rock', 'kick', 'drag', 'toss'. The pitcher then brings the ball in a circular motion over the head, completes the circle back down at the hip, and snaps the hand. A \"modified\" fast pitch is identical to a \"windmill\" pitch except the arm is not brought over the head in a full windmill motion, but instead is brought behind the body and is then thrust directly forward for the release. Another type of pitching movement is the \"figure 8\".  With this style, the ball is not brought over the head at all but down and behind the body and back in one smooth motion tracing out a figure eight.  There are many different pitches which can be thrown, including a two-seam fastball, four-seam fastball, changeup, two different riseballs, two dropballs, curveball, offspeed, screwball, knuckleball and more. These pitches can be taught in many different styles, depending on the pitching coach's method and the player's abilities.\n Catching is also a very important part of fast pitch softball.  Without a fast-paced catcher, the pitcher will not succeed. The catcher needs to be able to recognize the batters, their hitting style, and the right pitches to call. If there is a bad pitch that hits the ground, the catcher needs to block it so runs do not score, and runners do not advance on the bases. Catchers can also \"frame\" pitches by pulling the ball towards the center of the plate to convince the umpires to call the pitch a strike.  Catchers are protected by a chest guard, helmet, mouth guard, leg protectors, and a specialized mitt.[4] This is due to the proximity of the batters to the catcher. Catchers are responsible for throwing runners out when they try to steal bases, meaning that a catcher must have a strong arm and a quick throw. The catcher is the brains of the team, and carries it as a whole.[5]\n The game of fastpitch softball is similar to baseball, and includes stealing bases and bunting. Unlike baseball, however, there is no \"leading off\" \u2013 the baserunner can only leave the base when the pitcher releases the ball. Most leagues use the \"dropped third strike\" rule, which allows the batter to attempt an advance to first base when the catcher fails to catch the third strike.[6]\n Fast pitch softball became a very popular sport in the US during the 1930s and 1940s. Commercial and semi-pro leagues sprang up all over the country in large cities and small towns alike. Both men's and women's leagues were popular and it was not unusual for both to be playing on the same night in a \"double-header\". Because of the speed of these games, they were very popular with spectators.  \nDuring those years, the women's games were popular and considered fun to watch but the real draws were the men's games. Pitchers that could hurl the ball in excess of 85\u00a0mph at a batter 46 feet away could strike out 15 to 20 batters a game. To make things even more difficult, the underhand delivery meant the ball was rising as it approached the plate and a talented pitcher could make the ball perform some baffling aerobatics on its journey to the batter's box.  \n The Amateur Softball Association (ASA) was formed in 1934 and held a National tournament each year to determine the best softball team in America. Soon there were state and regional tournaments all over the country selecting teams to vie for the coveted National Championship. Competition was fierce with teams competing not only on the field but in recruiting the best \"fire baller\" around. It was not unusual for a talented pitcher to be recruited by the winning team after his team was eliminated from a tournament. It was rumored that some of these \"amateurs\" were making fair living from playing softball.\n Fast pitch softball started to lose popularity in the mid-50s for a variety of reasons.[7] More and more families were getting television in their homes and so games drew smaller crowds. More teams were starting to play \"slow  pitch\" with its greater emphasis on fielding. Although men's fast pitch softball is still played, the game is now mostly played by women.\n One of the most important milestones in softball and American softball history occurred when the Amateur Softball Association sent the Connecticut Brakettes of Stratford, Connecticut, to compete in the International Softball Federation's (ISF) Women's World championship in 1965. The Brakettes were the ASA's first women's softball team and they finished the competition with a record of 8\u20133 and a silver medal.[8] After the championship, the Brakettes travelled to many locations around the world to serve as ambassadors for the sport. During the trip, the coaches and players held softball clinics to give a diverse group of people a better understanding of softball.[8]\n The number of Division I softball teams in the US grew from 222 in 1997 to 277 in 2007. The number of youth teams also increased from 73,567 in 1995 to 86,049 that same year.[9]\n Currently, there are 286 National Collegiate Athletic Association (NCAA) Division 1 (D1) softball colleges across the nation.[10] This provides an opportunity for competitors to play Division 1 softball in every state across America. 64 teams compete in the NCAA tournament, only 16 teams make it to the Super Regionals and 8 teams compete in the College Softball World Series, hosted each year in May and June in Oklahoma City.[10]\n As the worldwide participation in softball continued to grow, the sport was eventually included in the 1996 Summer Olympic Games in Atlanta, Georgia, US.\n In America, the ASA responded by developing a coaching pool consisting of the best coaches in the country along with a selection committee which would recruit the most talented US softball players. The selection committee was responsible for making the final cuts to decide which players would compete for the US team at all international competitions throughout the year. The strategy was successful as the United States won their first olympic gold medal in softball against China with a 3\u20130 win. This success was followed by two more olympic gold medals and seven World Championships.\n In July 2005, IOC members voted 52\u201352 (with one abstention), to remove softball (along with baseball) from the Olympic program after the 2008 Olympic Games. Softball and baseball needed a majority vote to stay. The two sports were the first to be cut since polo in 1936. One of the reasons softball was considered for elimination from the Olympics was because there was not enough global participation and not enough depth of talent worldwide to merit Olympic status. In the three Summer Olympics which included a softball competition, four countries won medals: the United States, Australia, China and Japan.\n In response to the expressed concern that there was not enough talent depth worldwide, the ISF began to introduce the game in places where softball is not traditionally played. For example, the US team donated equipment and hosted coaching clinics in the Middle East, Africa and Europe. The US team's Jessica Mendoza has also delivered equipment and conducted clinics in other countries such as Brazil, the Czech Republic and South Africa.\n After softball's elimination from 2012 Olympics became a possibility, the ASA created the World Cup of Softball in Oklahoma City in 2005. This event allows the top countries in the world to compete on a yearly basis. The 2005 World Cup of Softball drew over 18,000 fans around the world for a competition between the top five international softball teams.\n The World Cup of Softball was later established as one of the premier events for the sport of softball. At the second World Cup of Softball, the attendance record was broken and the television ratings were higher than in any previous US Softball event on ESPN and ESPN2. Fastpitch softball, however, was added to the 2020 Summer Olympics.\n Many countries have expressed interest in the addition of softball especially to the Olympic games, with collegiate softball and semi-professional games growing.[11]\n Olympic level softball was on the Summer Olympic program from 1996 to 2008. The sport of softball was later removed from the programme for 2012 and 2016, but was slated to be re-added for the 2020 Summer Olympics.[12] Along with softball, the International Olympic Committee (IOC) had added baseball, skateboarding, karate, sports climbing, and surfing to the Summer Olympic games.[13] The IOC expressed consideration of the youth focus and increasing interest in the newly added sports.\n Originally scheduled to take place from July to August 2020, the 2020 Summer Olympics was postponed in March 2020 and pushed forward to 2021 as a result of the COVID-19 pandemic.\n In August 2021, the IOC announced that softball would not be part of the 2024 Paris Olympics.[14]\n Softball pitchers at any level require a level of accuracy with their pitches.[15] An accurate pitch is achieved through different techniques which help the pitcher to maintain a certain consistency of body weight and balance. The pitch starts with the grip and ends with a follow through after the ball is released from the hand of the pitcher. An accurate pitch requires skill in six areas: the grip, stance, windup, stride, release and follow through.\n A perfect grip depends on which pitch the pitcher is throwing. For a normal fastball, it is beneficial to hold the ball firmly with your fingertips, but not so that the ball is pushed deep into your palm of your hand. In fastpitch softball, there are several  types of grips for various pitches. There is not one correct way to hold or throw a pitch, it all depends on the pitcher. Some grips are easier for individuals due to the size of their hand while others find certain grips more difficult. This is why younger pitchers who have not developed into their normal hand size start with more basic grips such as the fastball. The main pitches involved in fastpitch softball are as follows: fastball, change-up, curveball, drop ball, and rise ball. Other pitches include both the drop curve, and well as the drop screw, and the backdoor curve.\n The pitcher's stance is also important when pitching.  In different types of competitions, different rules concerning the stance apply. In college, professional, and Olympic games, pitchers must place both feet on the rubber when starting the pitch. Other competitions require having only one foot on the rubber. The pitcher's feet are placed at a distance that is not larger than the width of the shoulders, with either one foot or both feet on the rubber. A common stance for pitchers is to have the ball of the foot on the same side as the throwing arm (also called the pivot foot) on the front edge of the rubber, and the toes of the alternate foot shoulder width apart and toward the middle or back side of the rubber.\n A stride is performed from the beginning stance by rocking back, shifting the weight back to gain momentum and then moving the weight forward onto the ball of the pivot foot.  The pitcher then pushes off the rubber with the pivot foot, pivoting that foot in a 30 to 40 degree, clockwise angle as the opposite leg moves out into a stride.  The stride leg must land along the \"power line\", which means that the pitcher's body is in line with the plate, with the pitcher's hips facing the third base line. The angle of a pitch can be altered in different ways through the stride.  If the pitcher is aiming for the outside corner of the plate, the pitcher will pivot, stride with the opposite foot, and land slightly outside of the \"power line\"; if the inside corner is aimed, the pitcher will land slightly inside. If the movement is very subtle, the batter will probably not be able to notice the change of angle.  In most leagues during the pitcher's delivery, the pivot foot must drag along the ground in order for the pitch to be legal.  If the drag foot lifts off the ground, an illegal pitch will be called for crow hopping.  In this case, the batter is awarded with one ball to the count, and all base runners advance to the next base.[16] In international play \u2013 and in most men's leagues \u2013 the pitcher is allowed to jump with the pivot foot. A crow hop here is considered legal.\n The windup is performed with the throwing arm and happens prior to the pitcher releasing the ball. Throughout the entire pitch, the upper body should remain upright, rather than bent over. The pitcher's throwing arm begins at the hip. Some pitchers move the throwing arm back as they shift their weight back, but it is not necessary throw an effective pitch. From the hip, the throwing arm moves up in a circle, brushing the ear, and returning to the hip prior to the release. The pitcher's arm should remain tight to the body to keep control of the pitch. This can be performed because the pitcher's lower body is pivoting in a straight line.\n The release is one of the most important motions for the effectiveness of the pitch. The release technique consists of wrist-snapping and allowing the ball to roll off the fingertips when arm reaches the hip.  If the snap also twists the wrist, the ball will move laterally or up and down, which can benefit the pitcher by confusing the batter.[17] The follow through is the last motion of a pitch. This is the point when the pitcher bends the throwing arm at the elbow, and the hand moves upward, finishing in front of the pitcher's face.\n Windmill pitching consists of three phases. The first phase, or \"wind up\" involves initiation of the motion until the top of the back swing (TOB).  The second phase lasts from the top of the back swing until the instant of stride foot contact (SFC).  The third phase occurs between the SFC and the instant of ball release (REL).[18]\n Knowledge of the kinematic parameters of these phases is critical for physicians, physical therapists, and athletic trainers to devise better diagnostic and rehabilitative protocols that are specific to the athletes. A previous study by Alexander et al. demonstrated that the majority of kinematic parameters have low magnitudes and vary among pitchers during the windup phase.[19] In a follow-up study, Werner et al. examined the parameters of the second and third phases, which they termed the \"delivery phase\". They found 7 specific parameters of pitching mechanics that correlate to risk of shoulder injury:  shoulder abduction, shoulder flexion, knee flexion angle at SFC, stride length, stride angle, and elbow and hip angles at REL. Monitoring these kinematics would aid in reducing shoulder stress. Mean shoulder abduction and shoulder flexion angles at SFC were 155\u00b0 \u00b116\u00b0 and 168\u00b0 \u00b135\u00b0, respectively. As the stride foot contacted the ground, the knee demonstrated a mean value of 27\u00b0\u00b19\u00b0 of flexion. Stride length averaged 89% \u00b111% of body height. Stride position varied between subjects, with a mean value of \u22123 \u00b114\u00a0cm; this indicates that when the foot contacted the ground, on average it landed slightly to the first-base side of home plate for right-handed pitchers, and to the third-base side for left-handers. The elbow flexion angle was 18\u00b0 \u00b19\u00b0 and the lower trunk (hip) angle moved toward a closed position of 52\u00b0\u00b118\u00b0 at REL. This indicated that greater degrees of shoulder abduction at SFC and greater stride angle decreased the magnitude of shoulder compression force. Conversely, longer stride, open REL hip position, and greater degrees of elbow flexion at REL and of shoulder and knee flexion at SFC all increase shoulder compression force.[18]\n Normative ranges for kinematic parameters have been established for an elite population of windmill pitchers. Specific pitching mechanic parameters correlate with clinically significant injury patterns. Interventions that take into account the aforementioned data could decrease shoulder forces, thus translating to lower rates of time-loss injury in this group of athletes.[18]\nThe fastest pitch on record was thrown by Eddie Feigner of \"The King and His Court\" who was clocked at \u207f90\u00a0mph.[20]  Feigner was at his best in the 1950s, and it is doubtful if this figure is correct.\n The fastpitch swing can be broken down into seven components: 1. Stance 2. Grip 3. Bat Position 4. Shift of Weight 5. Hand Position 6. \"Squishing the Bug\" 7. Follow Through and Finish. There are many drills as well that can help aid in the advancing of one's mechanisms including hitting of a tee and soft toss drills. The swing must be very concise and compact since the ball is coming very fast and there is little time to react.[21]\n Hitting is also an important aspect of softball. Hitters will approach the plate based on a very thought out line up assigned by the coach. They will have a helmet and a bat made out of a specific composite material based on the hitter's preference and contact styles. An important technique to remember about hitting is the launch angle that the ball takes off of the bat at contact. The launch angle for a ground ball is anything less than 10\u00b0 while a line drive is 10\u201325\u00b0. These launch angles are best for slap hitters or contact hitters, and a line drive is better than a ground ball. Fly balls come off the bat at a launch angle of 25\u201350\u00b0 and these are good for power hitters. Fly balls are typically what a home run is made of. A pop up's launch angle is anything greater than 50\u00b0. There is never really an apparent reason to hit a fly ball, the best case scenario would be to foul it off out of play in a battle at the plate. It is always better to hit a line drive than a ground ball, especially as the defensive game gets more competitive. It is always best to find a gap rather than hit it right at the defensive players.[22]\n Associations which support fastpitch softball include:\n \nEach association plays under its own official rules.\n USA Softball was created in 1933 by the American Softball Association (ASA). It is still owned and operated by the ASA, and includes the USA Men's, Women's, Junior Boys' and Junior Girls' National Team. In addition, it also oversees 150,000 amateur teams nationwide. There are 15 players on the USA Softball roster along with 3 replacement players.[24]  USA Softball offers recreational, league, tournament and National Championship play for fast pitch softball.[24]\n Before the 1996 Summer Olympics, the USA Softball National Team Selection Committee was established to make the final cuts for the team that would represent the US at every international competition. The selected 1996 United States softball team won the olympic gold medal with a 3\u20130 victory over China in the 1996 games.[25]\n The United States women's national softball team won three consecutive gold medals at the 1996, 2000, and 2004 Summer Olympic Games. In the 2008 olympic games, the gold medal was won by Japan after they defeated the United States. The US team received the silver medal.  Softball was recognized as an official sport in the 2020 Summer Olympics.  Japan won the gold medal, defeating the United States 2\u20130.[26]\n"
    },
    {
        "title": "Grace Hopper",
        "url": "https://en.wikipedia.org/wiki/Grace_Murray_Hopper",
        "content": "\n Grace Brewster Hopper (n\u00e9e\u00a0Murray; December 9, 1906 \u2013 January 1, 1992) was an American computer scientist, mathematician, and United States Navy rear admiral.[1] She was a pioneer of computer programming. Hopper was the first to devise the theory of machine-independent programming languages, and used this theory to develop the FLOW-MATIC programming language and COBOL, an early high-level programming language still in use today. She was also one of the first programmers on the Harvard Mark I computer. She is credited with writing the first computer manual, \"A Manual of Operation for the Automatic Sequence Controlled Calculator.\"\n Before joining the Navy, Hopper earned a Ph.D. in both mathematics and mathematical physics from Yale University and was a professor of mathematics at Vassar College. She left her position at Vassar to join the United States Navy Reserve during World War II. Hopper began her computing career in 1944 as a member of the Harvard Mark I team, led by Howard H. Aiken. In 1949, she joined the Eckert\u2013Mauchly Computer Corporation and was part of the team that developed the UNIVAC I computer. At Eckert\u2013Mauchly she managed the development of one of the first COBOL compilers.\n She believed that programming should be simplified with an English-based computer programming language. Her compiler converted English terms into machine code understood by computers. By 1952, Hopper had finished her program linker (originally called a compiler), which was written for the A-0 System.[2][3][4][5] In 1954, Eckert\u2013Mauchly chose Hopper to lead their department for automatic programming, and she led the release of some of the first compiled languages like FLOW-MATIC. In 1959, she participated in the CODASYL consortium, helping to create a machine-independent programming language called COBOL language, which was based on English words. Hopper promoted the use of the language throughout the 60s.\n The U.S. Navy Arleigh Burke-class guided-missile destroyer USS\u00a0Hopper was named for her, as was the Cray XE6 \"Hopper\" supercomputer at NERSC,[6] and the Nvidia GPU architecture \"Hopper\".[7][8] During her lifetime, Hopper was awarded 40 honorary degrees from universities across the world. A college at Yale University was renamed in her honor.  In 1991, she received the National Medal of Technology. On November 22, 2016, she was posthumously awarded the Presidential Medal of Freedom by President Barack Obama.[9] In 2024, the Institute of Electrical and Electronics Engineers (IEEE) dedicated a marker in honor of Grace Hopper at the University of Pennsylvania for her role in inventing the A-0 compiler during her time as a Lecturer in the School of Engineering, citing her inspirational impact on young engineers.[10][11]\n Grace Brewster Murray was born in New York City. She was the eldest of three children. Her parents, Walter Fletcher Murray and Mary Campbell Van Horne, were of Scottish and Dutch descent, and attended West End Collegiate Church.[12] Her great-grandfather, Alexander Wilson Russell, an admiral in the US Navy, fought in the Battle of Mobile Bay during the Civil War.[12]:\u200a2\u20133\u200a\n Grace was very curious as a child; this was a lifelong trait. At the age of seven, she decided to determine how an alarm clock worked and dismantled seven alarm clocks before her mother realized what she was doing (she was then limited to one clock).[13] Later in life, she was known for keeping a clock that ran backward, she explained, \"Humans are allergic to change. They love to say, 'We've always done it this way.' I try to fight that. That's why I have a clock on my wall that runs counterclockwise.\"[14] For her preparatory school education, she attended the Hartridge School in Plainfield, New Jersey. Grace was initially rejected for early admission to Vassar College at age 16 (because her test scores in Latin were too low), but she was admitted the next year. She graduated Phi Beta Kappa from Vassar in 1928 with a bachelor's degree in mathematics and physics and earned her master's degree at Yale University in 1930.\n In 1930, Grace Murray married New York University professor Vincent Foster Hopper (1906\u20131976); they divorced in 1945.[15][16] She did not marry again and retained his surname.\n In 1934, Hopper earned a Ph.D. in mathematics from Yale[17] under the direction of \u00d8ystein Ore.[15][18] Her dissertation, \"New Types of Irreducibility Criteria\",[19] was published that same year.[20] She began teaching mathematics at Vassar in 1931, and was promoted to associate professor in 1941.[21]\n Hopper tried to be commissioned in the Navy early in World War II, however she was turned down. At age 34, she was too old to enlist and her weight-to-height ratio was too low. She was also denied on the basis that her job as a mathematician and mathematics professor at Vassar College was valuable to the war effort.[22] During the war in 1943, Hopper obtained a leave of absence from Vassar and was sworn into the United States Navy Reserve; she was one of many women who volunteered to serve in the WAVES.\n She had to get an exemption to be commissioned; she was 15 pounds (6.8\u00a0kg) below the Navy minimum weight of 120 pounds (54\u00a0kg). She reported in December and trained at the Naval Reserve Midshipmen's School at Smith College in Northampton, Massachusetts. Hopper graduated first in her class in 1944, and was assigned to the Bureau of Ships Computation Project at Harvard University as a lieutenant, junior grade. She served on the Mark I computer programming staff headed by Howard H. Aiken.\n Hopper and Aiken co-authored three papers on the Mark I, also known as the Automatic Sequence Controlled Calculator. Hopper's request to transfer to the regular Navy at the end of the war was declined due to her advanced age of 38. She continued to serve in the Navy Reserve. Hopper remained at the Harvard Computation Lab until 1949, turning down a full professorship at Vassar in favor of working as a research fellow under a Navy contract at Harvard.[23]\n In 1949, Hopper became an employee of the Eckert\u2013Mauchly Computer Corporation as a senior mathematician and joined the team developing the UNIVAC I.[21] Hopper also served as UNIVAC director of Automatic Programming Development for Remington Rand. The UNIVAC was the first known large-scale electronic computer to be on the market in 1951, and was more competitive at processing information than the Mark I.[24]\n When Hopper recommended the development of a new programming language that would use entirely English words, she \"was told very quickly that [she] couldn't do this because computers didn't understand English.\" Still, she persisted. \"It's much easier for most people to write an English statement than it is to use symbols\", she explained. \"So I decided data processors ought to be able to write their programs in English, and the computers would translate them into machine code.\"[25]\n Her idea was not accepted for three years. In the meantime, she published her first paper on the subject, compilers, in 1952. In the early 1950s, the company was taken over by the Remington Rand corporation, and it was while she was working for them that her original compiler work was done. The program was known as the A compiler and its first version was A-0.[26]:\u200a11\u200a\n In 1952, she had an operational link-loader, which at the time was referred to as a compiler. She later said that \"Nobody believed that\", and that she \"had a running compiler and nobody would touch it. They told me computers could only do arithmetic.\"[27]\n In 1954 Hopper was named the company's first director of automatic programming.[21] Beginning in 1954, Hopper's work was influenced by the Laning and Zierler system, which was the first compiler to accept algebraic notation as input.[28] Her department released some of the first compiler-based programming languages, including MATH-MATIC and FLOW-MATIC.[21]\n Hopper said that her compiler A-0, \"translated mathematical notation into machine code. Manipulating symbols was fine for mathematicians but it was no good for data processors who were not symbol manipulators. Very few people are really symbol manipulators. If they are, they become professional mathematicians, not data processors. It's much easier for most people to write an English statement than it is to use symbols. So I decided data processors ought to be able to write their programs in English, and the computers would translate them into machine code. That was the beginning of COBOL, a computer language for data processors. I could say 'Subtract income tax from pay' instead of trying to write that in octal code or using all kinds of symbols. COBOL is the major language used today in data processing.\"[29]\n In the spring of 1959, computer experts from industry and government were brought together in a two-day conference known as the Conference on Data Systems Languages (CODASYL). Hopper served as a technical consultant to the committee, and many of her former employees served on the short-term committee that defined the new language COBOL (an acronym for COmmon Business-Oriented Language). The new language extended Hopper's FLOW-MATIC language with some ideas from the IBM equivalent, COMTRAN.  Hopper's belief that programs should be written in a language that was close to English (rather than in machine code or in languages close to machine code, such as assembly languages) was captured in the new business language, and COBOL went on to be the most ubiquitous business language to date.[30] Among the members of the committee that worked on COBOL was Mount Holyoke College alumna Jean E. Sammet.[31]\n From 1967 to 1977, Hopper served as the director of the Navy Programming Languages Group in the Navy's Office of Information Systems Planning and was promoted to the rank of captain in 1973. She developed validation software for COBOL and its compiler as part of a COBOL standardization program for the entire Navy.[23]\n In the 1970s, Hopper advocated for the Defense Department to replace large, centralized systems with networks of small, distributed computers. Any user on any computer node could access common databases on the network.[26]:\u200a119\u200a She developed the implementation of standards for testing computer systems and components, most significantly for early programming languages such as FORTRAN and COBOL. The Navy tests for conformance to these standards led to significant convergence among the programming language dialects of the major computer vendors. In the 1980s, these tests (and their official administration) were assumed by the National Bureau of Standards (NBS), known today as the National Institute of Standards and Technology (NIST).\n In accordance with Navy attrition regulations, Hopper retired from the Naval Reserve with the rank of commander at age 60 at the end of 1966.[32] She was recalled to active duty in August 1967 for a six-month period that turned into an indefinite assignment. She again retired in 1971 but was again asked to return to active duty in 1972. She was promoted to captain in 1973 by Admiral Elmo R. Zumwalt Jr.[33]\n After Republican Representative Philip Crane saw her on a March 1983 segment of 60 Minutes, he championed a joint resolution to promote Hopper to commodore on the retired list; the resolution was referred to, but not reported out of, the Senate Armed Services Committee.[34] Hopper was instead promoted to commodore on December 15, 1983, via the Appointments Clause by President Ronald Reagan.[35][33][36][37][38] She remained on active duty for several years beyond mandatory retirement by special approval of Congress.[39] Effective November 8, 1985, the rank of commodore was renamed rear admiral (lower half) and Hopper became one of the Navy's few female admirals.\n After a career that spanned more than 42 years, Hopper retired from the Navy on August 14, 1986.[40] At the time, she was the oldest serving member of the Navy. At a celebration held in Boston on the USS\u00a0Constitution to commemorate her retirement, Hopper was awarded the Defense Distinguished Service Medal, the highest non-combat decoration awarded by the Department of Defense.[41]\n At the time of her retirement, she was the oldest active-duty commissioned officer in the United States Navy (79 years, eight months and five days), and had her retirement ceremony aboard the oldest commissioned ship in the United States Navy (188 years, 9 months, 23 days).[42]\n After her retirement from the Navy, Hopper was hired as a senior consultant to Digital Equipment Corporation (DEC). Hopper was initially offered a position by Rita Yavinsky, but she insisted on going through the typical formal interview process. She then proposed in jest that she would be willing to accept a position which made her available on alternating Thursdays, exhibited at their museum of computing as a pioneer, in exchange for a generous salary and unlimited expense account. Instead, she was hired as a full-time Principal Corporate Consulting Engineer, a tech-track SVP-equivalent. In this position, Hopper represented the company at industry forums, serving on various industry committees, along with other obligations.[43] She retained that position until her death at age 85 in 1992.\n At DEC Hopper served primarily as a goodwill ambassador. She lectured widely about the early days of computing, her career, and on efforts that computer vendors could take to make life easier for their users. She visited most of Digital's engineering facilities, where she generally received a standing ovation at the conclusion of her remarks. Although no longer a serving officer, she always wore her Navy full dress uniform to these lectures contrary to U.S. Department of Defense policy.[44] In 2016 Hopper received the Presidential Medal of Freedom, the nation's highest civilian honor, in recognition of her remarkable contributions to the field of computer science.\n \"The most important thing I've accomplished, other than building the compiler\", she said, \"is training young people. They come to me, you know, and say, 'Do you think we can do this?' I say, 'Try it.' And I back 'em up. They need that. I keep track of them as they get older and I stir 'em up at intervals so they don't forget to take chances.\"[45]\n Throughout much of her later career, Hopper was much in demand as a speaker at various computer-related events. She was well known for her lively and irreverent speaking style, as well as a rich treasury of early war stories. She also received the nickname \"Grandma COBOL\".[46]\n While Hopper was working on a Mark II Computer at Harvard University in 1947,[47] her associates discovered a moth that was stuck in a relay and impeding the operation of the computer. Upon extraction, the insect was affixed to a log sheet for that day with the notation, \"First actual case of bug being found\". While neither she nor her crew members mentioned the exact phrase, \"debugging\", in their log entries, the case is held as a historical instance of \"debugging\" a computer and Hopper is credited with popularizing the term in computing. For many decades, the term \"bug\" for a malfunction had been in use in several fields before being applied to computers.[48][49] The remains of the moth can be found taped into the group's log book at the Smithsonian Institution's National Museum of American History in Washington, D.C.[47]\n Hopper became known for her nanoseconds visual aid. People (such as generals and admirals) used to ask her why satellite communication took so long.  She started handing out pieces of wire that were just under one foot long\u201411.8 inches (30\u00a0cm)\u2014the distance that light travels in one nanosecond. She gave these pieces of wire the metonym \"nanoseconds\".[38] She was careful to tell her audience that the length of her nanoseconds was actually the maximum distance the signals would travel in a vacuum, and that signals would travel more slowly through the actual wires that were her teaching aids. Later she used the same pieces of wire to illustrate why computers had to be small to be fast. At many of her talks and visits, she handed out \"nanoseconds\" to everyone in the audience, contrasting them with a coil of wire 984 feet (300 meters) long,[50] representing a microsecond. Later, while giving these lectures while working for DEC, she passed out packets of pepper, calling the individual grains of ground pepper picoseconds.[51]\n Jay Elliot described Hopper as appearing to be \"'all Navy', but when you reach inside, you find a 'Pirate' dying to be released\".[52]\n On New Year's Day 1992, Hopper died in her sleep of natural causes at her home in Arlington County, Virginia;[53] she was 85 years of age. She was interred with full military honors in Arlington National Cemetery.[54]\n Her legacy was an inspiring factor in the creation of the Grace Hopper Celebration of Women in Computing.[112] Held yearly, this conference is designed to bring the research and career interests of women in computing to the forefront.[113]\n"
    },
    {
        "title": "Tenley Albright",
        "url": "https://en.wikipedia.org/wiki/Tenley_Albright",
        "content": "\n Tenley Emma Albright (born July 18, 1935) is an American former figure skater and surgeon. She is the 1956 Olympic champion, the 1952 Olympic silver medalist, the 1953 and 1955 World Champion, the 1953 and 1955 North American champion, and the 1952\u20131956 U.S. national champion. Albright is also a graduate of Harvard Medical School. In 2015, she was inducted into the National Women's Hall of Fame.[2]\n Albright was born in Newton, Massachusetts.[3] Her father was a surgeon who wanted his daughter to be active in sports. Each winter, he would flood an area behind his house to create a skating rink for Albright and her friends. She was able to cut figure eights into the ice by the time she was nine years old.[citation needed] In 1947, she contracted a mild case of polio; as figure skating historian James R. Hines put it, \"Skating provided much needed physical therapy\".[4] Her illness left her muscles \u201cweak and withered\u201d. She started training at the Skating Club of Boston as part of her rehabilitation. She found her rehabilitation \u201cexhilarating\u201d. She would later say, \"Did you ever notice how many athletes my age once had polio? I think it's because being paralyzed makes you aware of your muscles and you never want to let them go unused again\u201d.[5][3]\n Albright had two coaches in her career: Willie Frick and Maribel Vinson.[4] She won the silver medal at the 1952 Olympics. She won her first World title in 1953, silver in 1954, a second gold medal in 1955, and her fourth medal, silver, in 1956.[6] She was the first American female skater to win a world title.[7] In 1955 she recorded a triple\u2014 winning the US, North American and World Championships that year. She managed to do this while enrolled as a full-time pre-med student at Radcliffe College.[3]\n She won the US Nationals Novice Championships at the age of 13 and the US Junior Championships at the age of 14, and then won five consecutive national titles starting at age 16.[4][3] In 1956, while training for the Olympics, Albright fell due to a rut in the ice and cut her right ankle joint to the bone with her left skate.[8] The cut was stitched by her father, a surgeon.[8][9] At the 1956 Winter Olympics in Cortina d'Ampezzo, Italy, she became the first American female skater to win an Olympic gold medal.[10][7]\n Albright retired from competitive skating after 1956 but remained attached to figure skating as a sports functionary.[9] In 1982 she became a vice president of the U.S. Olympic Committee.[1]\n A graduate of The Winsor School in Boston, Albright entered Radcliffe College in 1953 as a pre-med student,[9] and focused on completing her education after the 1956 Olympics.[10] She graduated from Harvard Medical School in 1961 at the age of 24,[11] went on to become a surgeon,[10][12] and she practiced for 23 years, continuing as a faculty member and lecturer at Harvard Medical School. For a while, she chaired the Board of Regents of the National Library of Medicine at the National Institutes of Health. As a director, she has served both not-for-profits such as The Whitehead Institute for Biomedical Research and the Woods Hole Oceanographic Institution and for-profit enterprises such as West Pharmaceutical Services, Inc., and State Street Bank and Trust Company.[13]\n In 1976 she served as the chief physician for the US Winter Olympic team. The American Academy of Achievement presented her with a Golden Plate Award in 1976.[14] Her accomplishments earned her an induction into the International Women's Sports Hall of Fame in 1983.[11]\n Tenley Albright was born to Hollis Albright, a prominent Boston surgeon.[12] Albright was married to Tudor Gardiner, a lawyer and son of William Tudor Gardiner, from 1962 to 1976. From 1981-2021 she was married to former Ritz-Carlton hotel owner Gerald Blakeley, who shared her association with Woods Hole and was chair of The Morehouse School of Medicine. He died on July 2, 2021.[15]\n"
    },
    {
        "title": "Women in Chad",
        "url": "https://en.wikipedia.org/wiki/Women_in_Chad",
        "content": "\n Women in Chad, a landlocked country in Central Africa, are the mainstay of its predominantly rural-based economy and they outnumber the men.[4] Chad is a country with diverse and rich cultural practices, such as male beauty pageants (judged by women) and long-kept-secret hair products.[5][6] Despite their numbers in the general population, there are very few women in governmental positions and gender equality is far from being a reality in Chad. Chad is rated by the World Bank as the third least gender equal  country in Africa.[7] Additionally, there are few women who attain higher education, and many who receive a college degree do so outside of the country.\n \nWomen face  discrimination and violence. Female genital mutilation, while technically illegal, is still widely practiced.[8] Child marriage and adolescent pregnancy are commonly practiced, although some policies have been implemented to combat them. Extrajudicial killings, beatings, torture, and rape were committed by security forces and other abuses with \"near total\" impunity.[9][10][11] Amnesty International has reported that \"the widespread insecurity in eastern Chad had particularly severe consequences for women, who suffered grave human rights abuses, including rape, during attacks on villages\"  by Janjawid militia from Sudan.[11]  Despite government's efforts, overall educational levels remained low at the end of the first decade of independence. In 1971, about 99 percent of women over the age of fifteen could not read, write, or speak French, which at the time was the only official national language; literacy in Arabic stood at 7.8 percent. In 1982, the overall literacy rate stood at about 15 percent.[12]\n Major problems have hindered the development of Chadian education since independence. Financing for education has been very limited. Limited facilities and personnel also have made it difficult for the education system to provide adequate instruction. Overcrowding is a major problem, with some classes having up to 100 students, many of whom are repeaters. In the years just after independence, many primary school teachers had only marginal qualifications. On the secondary level, the situation was even worse.[12]\n In 2004, 39.6 percent of children aged 5 to 14 were attending school. Educational opportunities for girls are limited, mainly due to cultural traditions. Fewer girls enroll in secondary school than boys, primarily due to early marriage. In 1999, 54.0 percent of children who started primary school reached grade 5.[13]\n There is a wide range of cultural traditions among the many ethnic groups within Chad.\n Chebe powder is a hair-care product used by rural women of the Basara Arab ethnic group in Chad. It is made from ground up natural herbs and ingredients, some of which are derived from plants unique to the region.[14] These Bassara Arab women use Chebe as a protective styling agent, and credit the product as the secret to their famously long and healthy hair. The ground-up powder is roasted then mixed with hair oil or animal fat. It is applied liberally to hair several times per month, to hydrate it.[5] The hair is then braided for further protection. The frequent Chebe application and hair-braiding is also a community bonding event for the women in these rural Basara groups. Some western beauty companies have used Chebe powder as a selling point in their hair-care products, but it is unclear whether or not these products contain genuine Chebe.[15]\n Facial scarring has been a common practice in Chad and the surrounding region for many decades. The nature of the scarring is extremely varied throughout the region, so there is no single style followed.[16] Some are opposed to facial scarring especially on children, as children are often scarred as infants. Much of the pushback has also stemmed from the AIDS epidemic, as some religious leaders have refused to use a fresh blade with each new client.\n A now relatively out-dated practice in the Sara tribe was to wear decorative facial jewelry that stretched the lips and skin.[17]\n Traditional women's clothing styles in Chad can be separated into two general cultural categories: that of Muslim women and that of non-Muslim women.[18] Muslim women in Chad typically dress similarly to Muslim women around the globe. This consists of wearing robes that cover the body and a hijab and/or niq\u0101b. A traditional style for Non-Muslim women is to wear a short-sleeved shirt, along with a pagne (or body wrap) which covers the upper and lower parts of the body, and decorative scarf called a lafai. Sandals are a common footwear worn by Chadian women.\n Gerewol (var. Guerewol, Gu\u00e9rewol) is an annual cultural festival of the semi-nomadic Wodaabe people across Niger and Chad.[19] It takes place each year at the end of the rainy season in September. The festival is centered around a courtship ritual beauty pageant, but unlike traditional western pageants the competitors are male. The men adorn themselves in dramatic makeup and flashy garments to attract the attentions of female suitors, and the women are responsible for judging their beauty.[20][21][6]\n In Chad, the mean age of women at their first marriage is 16, but experts believe the median age is likely even lower.[22][23] Traditionally, girls in Chad are seen as women once they reach reproductive age, and immediately become candidates for marriage to bachelors in their respective communities. According to the World Fertility Report in 2013, the average (mean) age of women in Chad at their first pregnancy was 18.2, but the median age for this figure is also expected to be somewhat lower.[22][23] The structure of promoting marriage is perpetuated by religious organizations, as over 73% of the population is Catholic or Muslim.[24] The tradition of women marrying young, combined with a culture that promotes having large families with many children, has caused Chad's population to grow at an accelerated rate compared to developed countries with lower fertility rates. Despite being one of the poorest countries in the world (per capita), Chad has the 5th highest fertility/birth rate, at an average of 5.8 births per woman.[22]\n Polygamy is legal in Chad, and it is estimated that over a third of women live in polygamous marriages. Men in Chad often take multiple wives in order to maximize the number of offspring they can produce, because having large families is seen as virtuous, in traditional culture. Ordinance No. 03/INT/SUR of 1961 states that polygamy is legal as long as \"spouses do not renounce it when signing the marriage contract\".[25] Knowledge about and access to contraceptives is scarce, which also contributes to high levels of poverty and child mortality, resulting from families' inability to provide for their children. Polygamy is, however, not limited to rural areas or remote communities; former President Idriss D\u00e9by publicly had multiple wives.[26] During his presidency, he married Hinda D\u00e9by Itno who eventually was designated as the First Lady out of his eight known wives.[26][27]\n Because girls are often married once they reach reproductive age in Chad, adolescent pregnancy is much more common than in other places. In Chad, the rate of childbirth for girls under the age of 15 is 47.8 out of 1,000, and nearly 14% of women in Chad have a child by this age.[24] In addition to the potential financial problems faced by young mothers in poor countries with high fertility rates, medical experts warn against the increased physical risks associated with adolescent pregnancy\u2014especially in regions with limited access to healthcare such as rural Chad.[24]\n Chad has a large number of nomadic and isolated communities which makes it much more difficult to ensure consistent healthcare access for all members of the population.[28] This problem is exacerbated by nation-wide financial troubles that forced the government to cut the per-capita healthcare spending by nearly 20% from 2014 to 2015.[29] Of the working healthcare facilities in Chad, only 18% are able to provide emergency neonatal and obstetric care (EmONC).[28]\n In Nigeria\u2014one of Chad's neighboring countries\u2014the maternal mortality rate among adolescents (ages 15\u201319) is more than double the maternal mortality rate of adult mothers (ages 20\u201334).[30] Although it has decreased in recent years, Chad still has the second highest maternal mortality rate of any African country, topped on.ly by South Sudan.[31]  According to the data collected by the World Bank in 2015, 54.7% of pregnant Chadian women were receiving prenatal care from a skilled health professional. This is a significant increase from 38.9% in 2004, but Chad is still far underperforming in this area as compared to neighboring countries of Niger, Sudan, and Libya which reported rates closer to 80 or 90 percent.[31]\n It is difficult for women in Chad's rural communities to access healthcare and education regarding safe sex practices, as well as information on what proper prenatal care is needed during pregnancy. A weak education system is partially responsible for the lack of knowledge around contraceptives and women's healthcare practices in Chad, as very few women between the ages of 15 and 24 are literate. This age range is crucial to the discussion of contraception because traditionally, females in Chad are seen as women once they reach childbearing age (typically around 13\u201316 years old).[30]\n Chad is a source and destination country for children subjected to trafficking in persons, specifically conditions of forced labor and forced prostitution. The country's trafficking problem is primarily internal and frequently involves parents entrusting children to relatives or intermediaries in return for promises of education, apprenticeship, goods, or money; selling or bartering children into involuntary domestic servitude or herding, as a means of survival by families seeking to reduce the number of mouths to feed.[32]\n Underage Chadian girls travel to larger towns in search of work, where some are subsequently subjected to prostitution. Some girls are compelled to marry against their will, only to be forced by their husbands into involuntary domestic servitude or agricultural labor. In past reporting periods, traffickers transported children from Cameroon and the CAR to Chad's oil producing regions for commercial sexual exploitation; it is unknown whether this practice persisted in 2009.[32]\n 60 percent of Chadian women had been subjected to female genital mutilation in 1995. The procedure is a traditional rite of passage as a girl moves into adulthood and it is followed regardless of religious orientation. It is equally common amongst Muslims, Christians and animists. Those who attain adulthood without being mutilated generally avoid it for life.[33] Over 80 percent of the girls in Chad who suffered genital mutilation were cut between the age of 5 and 14.[34]\n In 2012, the World Economic Forum ranked Chad as the third worst country on the continent for gender inequality in their Global Gender Gap Report, with an index score of 0.58 (1 is complete equality and 0 is complete inequality).[7]\n After President Idriss D\u00e9by was killed on April 20, 2021, there was a coup which led to the dismissal of the National Assembly.[35][36] The ruling military junta is now responsible for the functions previously handled by the National Assembly. Prior to its dissolution, the National Assembly consisted of 188 members, 28 of whom were women.\n When the military junta took power, they named 93 people who would act as the interim parliament\u2014called the Conseil Militaire de Transition (CMT)\u2014until the next election cycle. The members were chosen according to a list of quotas which required that at least 30% of members must come from the preceding National Assembly, 30% must be women, and 30% must be youths.[37] The list of all CMT members can be found at this link.[38]\n The presence of quotas for women in government is a relatively new practice, and is part of a greater women's rights and gender equality movement that has grown in popularity over the last decade.[39] The goal of complete gender equality is still far from realized in Chad, and it is still rare to find women in highly influential positions in government. There is a deep cultural history around the woman's role as a mother, which has made the project of gender equality much more difficult to implement.\n See linked pages for more information.\n Fatim\u00e9 Kimto (died May 21, 2015) was a Chadian politician. She was the first woman to serve in a cabinet position in the country's history. She was first named to the cabinet in 1982, becoming the Minister of Social Affairs and Women; she remained in the post until 1984. Her last post was as the Minister of Civil Service, Labor, and Employment.\n Achta Ton\u00e9 Gossingar (1 July 1941 \u2013  23 November 2011) was the first female minister in the government of Idriss D\u00e9by; she went on to become a public health advocate, working in preventing death rates from AIDS and maternal death before her death in a plane crash in 2011.\n Elise Loum (born 1956, Chad) was a vice-president of the African Union's Pan-African Parliament from 2004 to 2009.\n Bourkou Louise Kabo (5 July 1934 \u2013 13 June 2019) was a Chadian politician. She was the first woman to be elected to the National Assembly of Chad.\n Kalthouma Nguembang was a Chadian politician, who was an early member of the Chadian Progressive Party (PPT). She was elected to the National Assembly of Chad in 1968, but was later imprisoned by Fran\u00e7ois Tombalbaye who accused her of plotting against him.\n See linked pages for more information.\n Lydie Beaassemda (born c. 1967) is a Chadian politician. Since 2 May 2021, she has been Minister of Higher Education and Research under Chad's Transitional Military Council. She is also a leading member of the Party for Democracy and Full Independence (PDI), founded by her father, and she has headed the party since his death in 2018.\n Amina Priscille Longoh (born 1991) is a Chadian humanitarian organizer and politician. She has served in the government of Chad as Minister of Women and the Protection of early childhood since July 2020. She is one of the main women's rights activists in Chad.\n Hinda D\u00e9by Itno (born 5 April 1980) is a former Chadian First Lady who served from 2005 until the death of her husband, President Idriss D\u00e9by, in April 2021. She has been an advocate for ending the AIDS epidemic in Chad, and was named a UNAIDS \"Special Ambassador\" for the Prevention of HIV and the Protection and Health of Adolescents.[40]\n Aziza Baroud (born August 4, 1965) is a Chadian politician who has served in various senior government positions such as Minister for Public Health & National Solidarity, and Minister for the Economy. Baroud has been the Permanent Representative to the United Nations since 2019, and is on the Religions for Peace Secretary General's Advisory Council.[41] In 1989, she received a master's degree in applied economics from the Universit\u00e9 Paris Dauphine.\n Chad has signed and ratified the Convention on the Elimination of All Forms of Discrimination against Women, the Convention against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment, the Convention on the Rights of the Child and the Optional Protocol to the Convention on the Rights of the Child on the Sale of Children, Child Prostitution and Child Pornography.[42][43][44][45]\n External Aid\n Several external organizations, including the UN, have offered their assistance to Chad in the spirit of improving equality and human rights. One example of this is the African Development Bank Group's project for Girls Education and Women's Literacy (PEFAF).[46]\n Recent Policy\n On March 14, 2015, the President of Chad launched a campaign to end child marriage by outlawing the forceful marriage of minors under the age of 18, making such a marriage punishable by 5\u201310 years in prison, along with major fines.[47] This decree has not completely ended child marriage, but it is a positive indication of the country's commitment to reduce harm against women and girls.\n SENAFET stands for the Semaine Nationale des Femmes Tchadienne or in English, National Chadian Women's week. The theme of the 2022 SENAFET is \u201cl\u2019autonomisation de la femme tchadienne passe par son implication dans la vie politique, \u00e9conomique et sociale du pays,\u201d which translates to \u201cGiving the Chadian woman autonomy through her involvement in political, economic, and societal life in Chad.\"[48]\n Goals of SENAFET:\n"
    },
    {
        "title": "Nellie Bly",
        "url": "https://en.wikipedia.org/wiki/Nellie_Bly",
        "content": "\n Elizabeth Cochrane Seaman (born Elizabeth Jane Cochran; May 5, 1864 \u2013 January 27, 1922), better known by her pen name Nellie Bly, was an American journalist who was widely known for her record-breaking trip around the world in 72 days in emulation of Jules Verne's fictional character Phileas Fogg and an expos\u00e9 in which she worked undercover to report on a mental institution from within.[1] She pioneered her field and launched a new kind of investigative journalism.[2]\n Elizabeth Jane Cochran was born May 5, 1864,[3] in \"Cochran's Mills\", now part of Burrell Township, Armstrong County, Pennsylvania.[4][5][6] Her father, Michael Cochran, born about 1810, started as a laborer and mill worker before buying the local mill and most of the land surrounding his family farmhouse.  He later became a merchant, postmaster, and associate justice at Cochran's Mills (named after him) in Pennsylvania.  Michael married twice.  He had 10 children with his first wife, Catherine Murphy, and five more children, including Elizabeth Cochran, his thirteenth daughter, with his second wife, Mary Jane Kennedy.[7] Michael Cochran died in 1870, when Elizabeth was 6.[8]\n As a young girl, Elizabeth often was called \"Pink\" because she so frequently wore that color. As she became a teenager, she wanted to portray herself as more sophisticated, and she dropped the nickname and changed her surname to \"Cochrane\".[9] In 1879, she enrolled at Indiana Normal School (now Indiana University of Pennsylvania) for one term but was forced to drop out due to lack of funds.[10] In 1880, Cochrane's mother moved her family to Allegheny City, which was later annexed by the City of Pittsburgh.[11]\n In 1885, a column in the Pittsburgh Dispatch titled \"What Girls Are Good For\" stated that girls were principally for birthing children and keeping house. This prompted Elizabeth to write a response under the pseudonym \"Lonely Orphan Girl\".[12][11][13] The editor, George Madden, was impressed with her passion and ran an advertisement asking the author to identify herself. When Cochran introduced herself to the editor, he offered her the opportunity to write a piece for the newspaper, again under the pseudonym \"Lonely Orphan Girl\".[13] Her first article for the Dispatch, titled \"The Girl Puzzle\", argued that not all women would marry and that what was needed were better jobs for women.[14]\n Her second article, \"Mad Marriages\", was about how divorce affected women. In it, she argued for reform of divorce laws.[15][failed verification] \"Mad Marriages\" was published under the byline of Nellie Bly, rather than \"Lonely Orphan Girl\" because, at the time,[14] it was customary for female journalists to use pen names to conceal their gender so that readers would not discredit them. The editor chose \"Nellie Bly\", after the African-American title character in the popular song \"Nelly Bly\" by Stephen Foster.[16] Cochrane originally intended that her pseudonym be \"Nelly Bly\", but her editor wrote \"Nellie\" by mistake, and the error stuck.[17] Madden was impressed again and offered her a full-time job.[11]\n As a writer, Nellie Bly focused her early work for the Pittsburgh Dispatch on the lives of working women, writing a series of investigative articles on female factory workers. However, the newspaper soon received complaints from factory owners about her writing, and she was reassigned to women's pages to cover fashion, society, and gardening, the usual role for female journalists, and she became dissatisfied. Still only 21, she was determined \"to do something no girl has done before.\"[18] She then traveled to Mexico to serve as a foreign correspondent, spending nearly half a year reporting on the lives and customs of the Mexican people. Her dispatches later were published in book form as Six Months in Mexico.[15] In one report, she protested the imprisonment of a local journalist for criticizing the Mexican government, then a dictatorship under Porfirio D\u00edaz.[19] When Mexican authorities learned of Bly's report, they threatened her with arrest, prompting her to flee the country. Safely home, she accused D\u00edaz of being a tyrannical czar suppressing the Mexican people and controlling the press.[11]\n Burdened again with theater and arts reporting, Bly left the Pittsburgh Dispatch in 1887 for New York City. Bly faced rejection after rejection as news editors would not consider hiring a woman.[20] Penniless after four months, she talked her way into the offices of Joseph Pulitzer's newspaper, the New York World, and took an undercover assignment for which she agreed to feign insanity to investigate reports of brutality and neglect at the Women's Lunatic Asylum on Blackwell's Island, now named Roosevelt Island.[21]\n It was not easy for Bly to be admitted to the asylum: she first decided to check herself into a boarding house called \"Temporary Homes for Females\". She stayed up all night to give herself the wide-eyed look of a disturbed woman and began making accusations that the other boarders were insane. Bly told the assistant matron: \"There are so many crazy people about, and one can never tell what they will do.\"[22] She refused to go to bed and eventually scared so many of the other boarders that the police were called to take her to the nearby courthouse. Once examined by a police officer, a judge, and a doctor, Bly was taken to Bellevue Hospital for a few days, then after evaluation was sent by boat to Blackwell's Island.[22]\n Committed to the asylum, Bly experienced the deplorable conditions firsthand. After ten days, the asylum released Bly at The World's behest. Her report, published October 9, 1887[23] and later in book form as Ten Days in a Mad-House, caused a sensation, prompted the asylum to implement reforms, and brought her lasting fame.[24] Nellie Bly had a significant impact on American culture and shed light on the experiences of marginalized women beyond the bounds of the asylum as she ushered in the era of stunt girl journalism.[20]\n In 1893, Bly used the celebrity status she had gained from her asylum reporting skills to schedule an exclusive interview with the allegedly insane serial killer Lizzie Halliday.[25]\n Biographer Brooke Kroeger argues:\n In 1888, Bly suggested to her editor at the New York World that she take a trip around the world, attempting to turn the fictional Around the World in Eighty Days (1873) into fact for the first time. A year later, at 9:40\u00a0a.m. on November 14, 1889, and with two days' notice,[27][clarification needed] she boarded the Augusta Victoria, a steamer of the Hamburg America Line,[28] and began her 24,898 mile (40,070 kilometer) journey.\n To sustain interest in the story, the World organized a \"Nellie Bly Guessing Match\" in which readers were asked to estimate Bly's arrival time to the second, with the Grand Prize consisting at first of a trip to Europe and, later on, spending money for the trip.[29][30] During her travels around the world, Bly went through England, France (where she met Jules Verne in Amiens), Brindisi, the Suez Canal, Colombo (in Ceylon), the Straits Settlements of Penang and Singapore, Hong Kong, and Japan.\n Just over seventy-two days after her departure from Hoboken, Bly was back in New York. She had circumnavigated the globe, traveling alone for almost the entire journey.[28] Bly's journey was a world record, though it only stood for a few months, until George Francis Train completed the journey in 67 days.[31]\n After the fanfare of her trip around the world, Bly quit reporting and took a lucrative job writing serial novels for publisher Norman Munro's weekly New York Family Story Paper. The first chapters of Eva The Adventuress, based on the real-life trial of Eva Hamilton, appeared in print before Bly returned to New York. Between 1889 and 1895 she wrote eleven novels. As few copies of the paper survived, these novels were thought lost until 2021, when author David Blixt announced the discovery of 11 lost novels in Munro's British weekly The London Story Paper.[32] In 1893, though still writing novels, she returned to reporting for the World.\n In 1895, Bly married millionaire manufacturer Robert Seaman.[33] Bly was 31 and Seaman was 73 when they married.[34] Due to her husband's failing health, she left journalism and succeeded her husband as head of the Iron Clad Manufacturing Co., which made steel containers such as milk cans and boilers. Seaman died in 1904.[35]\n That same year, Iron Clad began manufacturing the steel barrel that was the model for the 55-gallon oil drum still in widespread use in the United States. There have been claims that Bly invented the barrel,[35] but the inventor was registered as Henry Wehrhahn (U.S. Patents 808,327 and 808,413).[36]\n Bly was also an inventor in her own right, receiving U.S. patent 697,553 for a novel milk can and U.S. patent 703,711 for a stacking garbage can, both under her married name of Elizabeth Cochrane Seaman. For a time, she was one of the leading women industrialists in the United States. But her negligence, and embezzlement by a factory manager, resulted in the Iron Clad Manufacturing Co. going bankrupt.[37]\n According to biographer Brooke Kroeger:\n She ran her company as a model of social welfare, replete with health benefits and recreational facilities. But Bly was hopeless at understanding the financial aspects of her business and ultimately lost everything. Unscrupulous employees bilked the firm of hundreds of thousands of dollars, troubles compounded by protracted and costly bankruptcy litigation.[26] Back in reporting, she covered the Woman Suffrage Procession of 1913 for the New York Evening Journal. Her article's headline was \"Suffragists Are Men's Superiors\" and in its text she accurately predicted that women in the United States would be given the right to vote in 1920.[38]\n Bly wrote stories on Europe's Eastern Front during World War I.[39] Bly was the first woman and one of the first foreigners to visit the war zone between Serbia and Austria. She was arrested when she was mistaken for a British spy.[40]\n On January 27, 1922, Bly died of pneumonia at St. Mark's Hospital, New York City, aged 57.[26] She was interred at Woodlawn Cemetery in The Bronx, New York City.[41]\n In 1998, Bly was inducted into the National Women's Hall of Fame.[42]\n Bly was one of four journalists honored with a US postage stamp in a \"Women in Journalism\" set in 2002.[43][44]\n In 2019, the Roosevelt Island Operating Corporation put out an open call for artists to create a Nellie Bly Memorial art installation on Roosevelt Island.[45] The winning proposal, The Girl Puzzle by Amanda Matthews, was announced on October 16, 2019.[46] The Girl Puzzle opened to the public in December, 2021.[47]\n The New York Press Club confers an annual Nellie Bly Cub Reporter journalism award to acknowledge the best journalistic effort by an individual with three years or fewer of professional experience. In 2020, it was awarded to Claudia Irizarry Aponte, of THE CITY.[48]\n Bly was the subject of the 1946 Broadway musical Nellie Bly by Johnny Burke and Jimmy Van Heusen. The show ran for 16 performances.[49]:\u200a310\u200a\n During the 1990s, playwright Lynn Schrichte wrote and toured Did You Lie, Nellie Bly?, a one-woman show about Bly.[50]\n An opera based on 10 Days in a Madhouse premiered in Philadelphia, PA in September 2023. The music was by Rene Orth and the libretto by Hannah Moscovitch.[51]\n Bly has been portrayed in the films The Adventures of Nellie Bly (1981),[52] 10 Days in a Madhouse (2015),[53] and Escaping the Madhouse: The Nellie Bly Story (2019).[54] In 2019, the Center for Investigative Reporting released Nellie Bly Makes the News, a short animated biographical film.[55] A fictionalized version of Bly as a mouse named Nellie Brie appears as a central character in the animated children's film An American Tail: The Mystery of the Night Monster.[56]\n Anne Helm appeared as Nellie Bly in the November 21, 1960, Tales of Wells Fargo TV episode \"The Killing of Johnny Lash\".[citation needed] Julia Duffy appeared as Bly in the July 10, 1983 Voyagers! episode \"Jack's Back\".[citation needed] The character of Lana Winters (Sarah Paulson) in American Horror Story: Asylum is inspired by Bly's experience in the asylum.[57]\n Bly was also a subject of Season 2 Episode 5 of The West Wing in which First Lady Abbey Bartlet dedicates a memorial in Pennsylvania in honor of Nellie Bly and convinces the president to mention her and other female historic figures during his weekly radio address.[58]\n Bly has been the subject of two episodes of the Comedy Central series Drunk History. The second-season episode \"New York City\" featured her undercover exploits in the Blackwell's Island asylum,[59] while the third-season episode \"Journalism\" retold the story of her race around the world against Elizabeth Bisland.[60]\n On May 5, 2015, the Google search engine produced an interactive \"Google Doodle\" for Bly; for the \"Google Doodle\" Karen O wrote, composed, and recorded an original song about Bly, and Katy Wu created an animation set to Karen O's music.[61]\n Nellie\u2019s story was adapted into a Doctor Who audio drama by Big Finish Productions, released on 8 September 2021.  The Perils of Nellie Bly was the second story in a three story box set, and was written by Sarah Ward.[62] Nellie was portrayed by actress Sydney Feder.[citation needed]\n Bly has been featured as the protagonist of novels by David Blixt,[63] Marshall Goldberg,[64] Dan Jorgensen,[65] Carol McCleary,[66] Pearry Reginald Teo, Maya Rodale,[67] Christine Converse [68] and Louisa Treger [69] David Blixt also appeared on a March 10, 2021 episode of the podcast Broads You Should Know as a Nellie Bly expert.[70]\n A fictionalized account of Bly's around-the-world trip was used in the 2010 comic book Julie Walker Is The Phantom published by Moonstone Books (Story: Elizabeth Massie, art: Paul Daly, colors: Stephen Downer).[71]\n Bly is one of 100 women featured in the first version of the book Good Night Stories for Rebel Girls written by Elena Favilli & Francesca Cavallo.[72]\n The board game Round the World with Nellie Bly created in 1890 is named in recognition of her trip.[73]\n The Nellie Bly Amusement Park in Brooklyn, New York City, was named after her, taking as its theme Around the World in Eighty Days. The park reopened in 2007[74] under new management, renamed \"Adventurers Amusement Park\".[75]\n A large species of tarantula from Ecuador, Pamphobeteus nellieblyae Sherwood et al., 2022, was named in her honour by arachnologists.[76]\n A fireboat named Nellie Bly operated in Toronto, Canada, in the first decade of the 20th century.[77] From early in the twentieth century until 1961, the Pennsylvania Railroad operated an express train named the Nellie Bly on a route between New York and Atlantic City, bypassing Philadelphia.\n Within her lifetime, Nellie Bly published three non-fiction books (compilations of her newspaper reportage) and one novel in book form.\n Between 1889 and 1895, Nellie Bly also penned twelve novels for The New York Family Story Paper. Thought lost, these novels were not collected in book form until their re-discovery in 2021.[78]\n"
    },
    {
        "title": "Women in Turkmenistan",
        "url": "https://en.wikipedia.org/wiki/Women_in_Turkmenistan",
        "content": "\n Women in Turkmenistan comprise 50.8% of the country's population.[1] They generally have assigned roles in society and reduced rights compared to men. Study of women's rights in the country is made difficult by government censorship and lack of reliable, official data.\n Under Article 18 of the Constitution of Turkmenistan, women are guaranteed equal rights to men.[2] In practice, however, they face routine political and societal discrimination. Women are not allowed to form independent women's organizations, and all such organizations must be registered under the Women's Union of Turkmenistan.[3] They make up 16.8% of the country's Parliament. Most women work in the home, as mothers or homemakers, or in the markets as sellers.[4] Though the country's Islamic roots are several centuries old, Turkmen women never were and are not required to wear a face covering.\n Turkmenistan is a country in Central Asia. Throughout the centuries, the territory of present-day Turkmenistan has been subjected to numerous civilizations, such as Persian empires, the conquest of Alexander the Great, Muslims, Mongols, Turkic peoples, and Russians. Throughout most of the 20th century it was part of the Soviet Union, until its fall in 1991. As with other former Soviet states, in the 1990s the economy collapsed and the country experienced social problems. Today, Turkmenistan is about half urban and half rural; its population is largely Muslim (89%), but there is also a significant Eastern Orthodox minority.[5] The total fertility rate is 2.09 children born/woman (2015 est.).[5]\n During the Soviet period, women assumed responsibility for the observance of some Muslim rites to protect their husbands' careers.[6] Many women entered the work force out of economic necessity, a factor that disrupted some traditional family practices and increased the incidence of divorce.[6] At the same time, educated urban women entered professional services and careers.[6] After the collapse of the Soviet Union, however, traditional values began to reassert themselves. This had led to increasing numbers of women confined to the home and dependent on their male counterparts.[7]\n Cooking is the main field of work for women. Neighbours or relatives sometimes arrive unasked to assist in housework, or they may bring their own household tasks to work on together and socialize. Food preparation is done in the open air as well as in in-house kitchens. Tasks such as smoking meat and popping corn, are done by men and often turn into a social opportunity.[8][full citation needed]\n Men and women might sit and eat in one place, but they are segregated during social occasions. Some women carry on the practice of wearing a ya\u015fmak, head scarf, in the initial year after they are wed. The wife clenches the corner of her scarf in her teeth to show a significant barrier toward the male guests and to show respect to her parents-in-law. The scarf also stops her from communicating. The wife may stop covering her head with a ya\u015fmak after a year of her wedding, after the birth of her first born, or by a decision within the family.[9][full citation needed]\n Women are garbed in ankle-length garments of silk or velvet, which are commonly a mix of bright oranges, purples, yellows, blues, and greens. The necklines are embellished with elaborate gold-thread needlework that drops down, decorating the neckline right to the navel.[10] Richly decorated headwear, jewelry and embroidery accents are a part of their routine.[11][full citation needed] Face covering is not required by law.[4]\n Manufacturing of ketene, a homespun silk, persists largely as a cottage skill. Garments prepared from ketene are worn by both men and women. Costumes made from ketene are used as a customary bridal dress.[11][full citation needed] The embroidery on the garments reveal various patterns that are exclusively known as a family hallmark, distinguishing the family of its maker.[11][full citation needed] Skilled Turkmen women use antique weaving looms known as tara, which were adopted in the ancient times.[12][full citation needed]\n Turkmen teachers and healthcare workers are primarily women.[13] Job cuts in both these sectors, however, have led to a dramatic rise in unemployment for both men and women.\n Article 25 of the Constitution of Turkmenistan requires mutual consent for marriage.[2] Both individuals must also be above the age of 18.\nTurkmen marriages feature numerous unique customs and rituals. Turkmen wedding dresses are often richly decorated and covered in silver-colored pendants which are believed to drive away evil spirits.[14]\n Women comprise 16.8% of the Turkmen Parliament, and domestic law guarantees women the right to political participation. However, the actual participation of women in government is stifled by the curtailing of nongovernmental organizations (NGOs).\n"
    },
    {
        "title": "Women in the Sahrawi Arab Democratic Republic",
        "url": "https://en.wikipedia.org/wiki/Women_in_Western_Sahara",
        "content": "Women in the Sahrawi Arab Democratic Republic are women who were born in, who live in, or are from the Sahrawi Arab Democratic Republic (SADR, also romanized with Saharawi) in the region of the Western Sahara or the Sahrawi refugee camps. In Sahrawi society, the women share responsibilities at every level of its community and social organization.[1] Article 41 of the Constitution of the Sahrawi Arab Democratic Republic ensures that the state will pursue \"the promotion of women and [their] political, social and cultural participation, in the construction of society and the country's development\".\n Clothing worn by the Sahrawis include the robe called as the Daraa. Women wear the headscarf known as the melhfa.[2][3]\n The Polisario Front, a national liberation movement that is recognized by the United Nations as the representative of the people of Western Sahara since 1979, has attempted to modernize the society of the Sahrawi refugee camps that was set up in Tindouf Province, Algeria from 1975 to 1976, through emphasis on education, eradication of tribalism and emancipation of women. The role of Sahrawi women was central already in pre-colonial and colonial life, but was strengthened further during the war years (1975\u20131991), when Sahrawi women ran most of the camps' administration, while the men were fighting at the front.[4] This together with literacy- and professional education classes produced major advances in the role of women in Sahrawi society. The return of large numbers of Sahrawi men since the cease fire in 1991 may have slowed this development according to some observers, but women still run a majority of the camps' administration,[5] and the Sahrawi women's union UNMS is very active in promoting their role. The role of women in camps was enhanced by their shouldering of the main responsibility for the camps and government bureaucracy during the war years, as virtually the entire male population was enrolled in the Polisario army.[6]\n In the 2012 election for seats in the Sahrawi National Council, 35% of the parliamentarians were women. In the legislative election in 2008 women gained 34.61% of seats, thanks in part to a quota system.[7]\n In Sahrawi culture, the role of the poet is not gendered and both men and women are recognised as accomplished poets.[8] Poets such as Al Khadra and Hadjatu Aliat Swelm compose works inspired by the Sahrawi struggle, as well as life in Sahrawi camps.[9] Women are the only ones in Hassani poetry (the tradition Sahrawi poetry is part of) to compose the tabr'a genre.[10]\n In musical life, women traditionally play the ardin (plural irdiwen), which is a small lap harp with between nine and fourteen strings.[10]\n"
    },
    {
        "title": "Satellite television",
        "url": "https://en.wikipedia.org/wiki/Satellite_TV",
        "content": "Satellite television is a service that delivers television programming to viewers by relaying it from a communications satellite orbiting the Earth directly to the viewer's location.[1] The signals are received via an outdoor parabolic antenna commonly referred to as a satellite dish and a low-noise block downconverter.\n A satellite receiver decodes the desired television program for viewing on a television set. Receivers can be external set-top boxes, or a built-in television tuner. Satellite television provides a wide range of channels and services. It is usually the only television available in many remote geographic areas without terrestrial television or cable television service. Different receivers are required for the two types. Some transmissions and channels are unencrypted and therefore free-to-air, while many other channels are transmitted with encryption. Free-to-view channels are encrypted but not charged-for, while pay television requires the viewer to subscribe and pay a monthly fee to receive the programming.[2]\n Modern systems signals are relayed from a communications satellite on the X band (8\u201312\u00a0GHz) or Ku band (12\u201318\u00a0GHz) frequencies requiring only a small dish less than a meter in diameter.[3] The first satellite TV systems were a now-obsolete type known as television receive-only. These systems received weaker analog signals transmitted in the C-band (4\u20138\u00a0GHz) from FSS type satellites, requiring the use of large 2\u20133-meter dishes. Consequently, these systems were nicknamed \"big dish\" systems, and were more expensive and less popular.[4] Early systems used analog signals, but modern ones use digital signals which allow transmission of the modern television standard high-definition television, due to the significantly improved spectral efficiency of digital broadcasting. As of 2022, Star One D2 from Brazil is the only remaining satellite broadcasting in analog signals.[5][6]\n The satellites used for broadcasting television are usually in a geostationary orbit 36,000\u00a0km (22,000\u00a0mi) above the earth's equator. The advantage of this orbit is that the satellite's orbital period equals the rotation rate of the Earth, so the satellite appears at a fixed position in the sky. Thus the satellite dish antenna which receives the signal can be aimed permanently at the location of the satellite and does not have to track a moving satellite. A few systems instead use a highly elliptical orbit with inclination of +/\u221263.4 degrees and an orbital period of about twelve hours, known as a Molniya orbit.\n Satellite television, like other communications relayed by satellite, starts with a transmitting antenna located at an uplink facility.[7] Uplink satellite dishes are very large, as much as 9 to 12 meters (30 to 40 feet) in diameter.[7] The increased diameter results in more accurate aiming and increased signal strength at the satellite.[7] The uplink dish is pointed toward a specific satellite and the uplinked signals are transmitted within a specific frequency range, so as to be received by one of the transponders tuned to that frequency range aboard that satellite.[8] The transponder re-transmits the signals back to Earth at a different frequency (a process known as translation, used to avoid interference with the uplink signal), typically in the 10.7-12.7\u00a0GHz band, but some still transmit in the C-band (4\u20138\u00a0GHz), Ku-band (12\u201318\u00a0GHz), or both.[7] The leg of the signal path from the satellite to the receiving Earth station is called the downlink.[9]\n A typical satellite has up to 32 Ku-band or 24 C-band transponders, or more for Ku/C hybrid satellites. Typical transponders each have a bandwidth between 27 and 50\u00a0MHz. Each geostationary C-band satellite needs to be spaced 2\u00b0 longitude from the next satellite to avoid interference; for Ku the spacing can be 1\u00b0. This means that there is an upper limit of 360/2 = 180 geostationary C-band satellites or 360/1 = 360 geostationary Ku-band satellites. C-band transmission is susceptible to terrestrial interference while Ku-band transmission is affected by rain (as water is an excellent absorber of microwaves at this particular frequency). The latter is even more adversely affected by ice crystals in thunder clouds. On occasion, sun outage will occur when the sun lines up directly behind the geostationary satellite to which the receiving antenna is pointed.[10]\n The downlink satellite signal, quite weak after traveling the great distance (see path loss), is collected with a parabolic receiving dish, which reflects the weak signal to the dish's focal point.[11] Mounted on brackets at the dish's focal point is a device called a feedhorn or collector.[12] The feedhorn is a section of waveguide with a flared front-end that gathers the signals at or near the focal point and conducts them to a probe or pickup connected to a low-noise block downconverter (LNB).[13] The LNB amplifies the signals and downconverts them to a lower block of intermediate frequencies (IF), usually in the L-band.[13]\n The original C-band satellite television systems used a low-noise amplifier (LNA) connected to the feedhorn at the focal point of the dish.[14] The amplified signal, still at the higher microwave frequencies, had to be fed via very expensive low-loss 50-ohm impedance gas filled hardline coaxial cable with relatively complex N-connectors to an indoor receiver or, in other designs, a downconverter (a mixer and a voltage-tuned oscillator with some filter circuitry) for downconversion to an intermediate frequency.[14] The channel selection was controlled typically by a voltage tuned oscillator with the tuning voltage being fed via a separate cable to the headend, but this design evolved.[14]\n Designs for microstrip-based converters for amateur radio frequencies were adapted for the 4\u00a0GHz C-band.[15] Central to these designs was concept of block downconversion of a range of frequencies to a lower, more easily handled IF.[15]\n The advantages of using an LNB are that cheaper cable can be used to connect the indoor receiver to the satellite television dish and LNB, and that the technology for handling the signal at L-band and UHF was far cheaper than that for handling the signal at C-band frequencies.[16] The shift to cheaper technology from the hardline and N-connectors of the early C-band systems to the cheaper and simpler 75-ohm cable and F-connectors allowed the early satellite television receivers to use, what were in reality, modified UHF television tuners which selected the satellite television channel for down conversion to a lower intermediate frequency centered on 70\u00a0MHz, where it was demodulated.[16] This shift allowed the satellite television DTH industry to change from being a largely hobbyist one where only small numbers of systems costing thousands of US dollars were built, to a far more commercial one of mass production.[16]\n In the United States, service providers use the intermediate frequency ranges of 950\u20132150\u00a0MHz to carry the signal from the LNBF at the dish down to the receiver. This allows for the transmission of UHF signals along the same span of coaxial wire at the same time. In some applications (DirecTV AU9-S and AT-9), ranges of the lower B-band[ambiguous] and 2250\u20133000\u00a0MHz, are used. Newer LNBFs in use by DirecTV, called SWM (Single Wire Multiswitch), are used to implement single cable distribution and use a wider frequency range of 2\u20132150\u00a0MHz.[citation needed]\n The satellite receiver or set-top box demodulates and converts the signals to the desired form (outputs for television, audio, data, etc.).[17] Often, the receiver includes the capability to selectively unscramble or decrypt the received signal to provide premium services to some subscribers; the receiver is then called an integrated receiver/decoder or IRD.[18] Low-loss cable (e.g. RG-6, RG-11, etc.) is used to connect the receiver to the LNBF or LNB.[13] RG-59 is not recommended for this application as it is not technically designed to carry frequencies above 950\u00a0MHz, but may work in some circumstances, depending on the quality of the coaxial wire, signal levels, cable length, etc.[13]\n A practical problem relating to home satellite reception is that an LNB can basically only handle a single receiver.[19] This is because the LNB is translating two different circular polarizations (right-hand and left-hand) and, in the case of K-band, two different frequency bands (lower and upper) to the same frequency range on the cable.[19] Depending on which frequency and polarization a transponder is using, the satellite receiver has to switch the LNB into one of four different modes in order to receive a specific \"channel\".[19] This is handled by the receiver using the DiSEqC protocol to control the LNB mode.[19] If several satellite receivers are to be attached to a single dish, a so-called multiswitch will have to be used in conjunction with a special type of LNB.[19] There are also LNBs available with a multi-switch already integrated.[19] This problem becomes more complicated when several receivers are to use several dishes (or several LNBs mounted in a single dish) pointing to different satellites.[19]\n A common solution for consumers wanting to access multiple satellites is to deploy a single dish with a single LNB and to rotate the dish using an electric motor. The axis of rotation has to be set up in the north\u2013south direction and, depending on the geographical location of the dish, have a specific vertical tilt. Set up properly the motorized dish when turned will sweep across all possible positions for satellites lined up along the geostationary orbit directly above the equator. The dish will then be capable of receiving any geostationary satellite that is visible at the specific location, i.e. that is above the horizon. The DiSEqC protocol has been extended to encompass commands for steering dish rotors.[citation needed]\n There are five major components in a satellite system: the programming source, the broadcast center, the satellite, the satellite dish, and the receiver. \"Direct broadcast\" satellites used for transmission of satellite television signals are generally in geostationary orbit 37,000\u00a0km (23,000\u00a0mi) above the earth's equator.[20] The reason for using this orbit is that the satellite circles the Earth at the same rate as the Earth rotates, so the satellite appears at a fixed point in the sky. Thus satellite dishes can be aimed permanently at that point, and do not need a tracking system to turn to follow a moving satellite. A few satellite TV systems use satellites in a Molniya orbit, a highly elliptical orbit with inclination of +/-63.4 degrees and an orbital period of about twelve hours.\n Satellite television, like other communications relayed by satellite, starts with a transmitting antenna located at an uplink facility.[20] Uplink facilities transmit the signal to the satellite over a narrow beam of microwaves, typically in the C-band frequency range due to its resistance to rain fade.[20] Uplink satellite dishes are very large, often as much as 9 to 12 metres (30 to 40 feet) in diameter[20] to achieve accurate aiming and increased signal strength at the satellite, to improve reliability.[20] The uplink dish is pointed toward a specific satellite and the uplinked signals are transmitted within a specific frequency range, so as to be received by one of the transponders tuned to that frequency range aboard that satellite.[20] The transponder then converts the signals to Ku band, a process known as \"translation,\" and transmits them back to earth to be received by home satellite stations.[20]\n The downlinked satellite signal, weaker after traveling the great distance (see path loss), is collected by using a rooftop parabolic receiving dish (\"satellite dish\"), which reflects the weak signal to the dish's focal point.[21] Mounted on brackets at the dish's focal point is a feedhorn[21] which passes the signals through a waveguide to a device called a low-noise block converter (LNB) or low noise converter (LNC) attached to the horn.[21] The LNB amplifies the weak signals, filters the block of frequencies in which the satellite television signals are transmitted, and converts the block of frequencies to a lower frequency range in the L-band range.[21] The signal is then passed through a coaxial cable into the residence to the satellite television receiver, a set-top box next to the television.\n The reason for using the LNB to do the frequency translation at the dish is so that the signal can be carried into the residence using cheap coaxial cable. To transport the signal into the house at its original Ku band microwave frequency would require an expensive waveguide, a metal pipe to carry the radio waves.[16] The cable connecting the receiver to the LNB are of the low loss type RG-6, quad shield RG-6, or RG-11.[22] RG-59 is not recommended for this application as it is not technically designed to carry frequencies above 950\u00a0MHz, but will work in many circumstances, depending on the quality of the coaxial wire.[22] The shift to more affordable technology from the 50\u00a0ohm impedance cable and N-connectors of the early C-band systems to the cheaper 75\u00a0ohm technology and F-connectors allowed the early satellite television receivers to use, what were in reality, modified UHF television tuners which selected the satellite television channel for down conversion to another lower intermediate frequency centered on 70\u00a0MHz where it was demodulated.[16]\n An LNB can only handle a single receiver.[19] This is due to the fact that the LNB is mapping two different circular polarisations \u2013 right hand and left hand \u2013 and in the case of the Ku-band two different reception bands \u2013 lower and upper \u2013 to one and the same frequency band on the cable, and is a practical problem for home satellite reception.[19] Depending on which frequency a transponder is transmitting at and on what polarisation it is using, the satellite receiver has to switch the LNB into one of four different modes in order to receive a specific desired program on a specific transponder.[19] The receiver uses the DiSEqC protocol to control the LNB mode, which handles this.[19] If several satellite receivers are to be attached to a single dish a so-called multiswitch must be used in conjunction with a special type of LNB.[19] There are also LNBs available with a multi-switch already integrated.[19] This problem becomes more complicated when several receivers use several dishes or several LNBs mounted in a single dish are aimed at different satellites.[19]\n The set-top box selects the channel desired by the user by filtering that channel from the multiple channels received from the satellite, converts the signal to a lower intermediate frequency, decrypts the encrypted signal, demodulates the radio signal and sends the resulting video signal to the television through a cable.[22] To decrypt the signal the receiver box must be \"activated\" by the satellite company. If the customer fails to pay their monthly bill the box is \"deactivated\" by a signal from the company, and the system will not work until the company reactivates it. Some receivers are capable of decrypting the received signal itself. These receivers are called integrated receiver/decoders or IRDs.[22]\n Analog television which was distributed via satellite was usually sent scrambled or unscrambled in NTSC, PAL, or SECAM television broadcast standards. The analog signal is frequency modulated and is converted from an FM signal to what is referred to as baseband. This baseband comprises the video signal and the audio subcarrier(s). The audio subcarrier is further demodulated to provide a raw audio signal.\n Later signals were digitized television signals or multiplex of signals, typically QPSK. In general, digital television, including that transmitted via satellites, is based on open standards such as MPEG and DVB-S/DVB-S2 or ISDB-S.[citation needed]\n The conditional access encryption/scrambling methods include NDS, BISS, Conax, Digicipher, Irdeto, Cryptoworks, DG Crypt, Beta digital, SECA Mediaguard, Logiways, Nagravision, PowerVu, Viaccess, Videocipher, and VideoGuard. Many conditional access systems have been compromised.\n An event called sun outage occurs when the sun lines up directly behind the satellite in the field of view of the receiving satellite dish.[23] This happens for about a 10-minute period daily around midday, twice every year for a two-week period in the spring and fall around the equinox. During this period, the sun is within the main lobe of the dish's reception pattern, so the strong microwave noise emitted by the sun on the same frequencies used by the satellite's transponders drowns out reception.[23]\n Direct-to-home (DTH) can either refer to the communications satellites themselves that deliver service or the actual television service. Most satellite television customers in developed television markets get their programming through a direct broadcast satellite (DBS) provider.[24] Signals are transmitted using Ku band (12 to 18\u00a0GHz) and are completely digital which means it has high picture and stereo sound quality.[3]\n Programming for satellite television channels comes from multiple sources and may include live studio feeds.[25] The broadcast center assembles and packages programming into channels for transmission and, where necessary, encrypts the channels. The signal is then sent to the uplink[26] where it is transmitted to the satellite. With some broadcast centers, the studios, administration and up-link are all part of the same campus.[27] The satellite then translates and broadcasts the channels.[28]\n Most systems use the DVB-S standard for transmission.[24] With pay television services, the data stream is encrypted and requires proprietary reception equipment. While the underlying reception technology is similar, the pay television technology is proprietary, often consisting of a conditional-access module and smart card. This measure assures satellite television providers that only authorized, paying subscribers have access to pay television content but at the same time can allow free-to-air channels to be viewed even by the people with standard equipment available in the market.\n Some countries operate satellite television services which can be received for free, without paying a subscription fee. This is called free-to-air satellite television. Germany is likely the leader in free-to-air with approximately 250 digital channels (including 83 HDTV channels and various regional channels) broadcast from the Astra 19.2\u00b0E satellite constellation.[29] These are not marketed as a DBS service, but are received in approximately 18 million homes, as well as in any home using the Sky Deutschland commercial DBS system. All German analogue satellite broadcasts ceased on 30 April 2012.[30][31]\n The United Kingdom has approximately 160 digital channels (including the regional variations of BBC channels, ITV channels, Channel 4 and Channel 5) that are broadcast without encryption from the Astra 28.2\u00b0E satellite constellation, and receivable on any DVB-S receiver (a DVB-S2 receiver is required for certain high definition television services). Most of these channels are included within the Sky EPG, and an increasing number within the Freesat EPG.\n India's national broadcaster, Doordarshan, promotes a free-to-air DBS package as \"DD Free Dish\", which is provided as in-fill for the country's terrestrial transmission network. It is broadcast from GSAT-15 at 93.5\u00b0E and contains about 80 FTA channels.\n While originally launched as backhaul for their digital terrestrial television service, a large number of French channels are free-to-air on satellites at 5\u00b0W, and have recently been announced as being official in-fill for the DTT network.\n In North America (United States, Canada and Mexico) there are over 80 FTA digital channels available on Galaxy 19 (with the majority being ethnic or religious in nature). Other FTA satellites include AMC-4, AMC-6, Galaxy 18, and Satmex 5. A company called GloryStar promotes FTA religious broadcasters on Galaxy 19.\n Satellite TV has seen a decline in consumers since the 2010s due to the cord-cutting trend where people are shifting towards internet-based streaming television and free over-the-air television.[32]\n The term television receive-only, or TVRO, arose during the early days of satellite television reception to differentiate it from commercial satellite television uplink and downlink operations (transmit and receive). This was the primary method of satellite television transmissions before the satellite television industry shifted, with the launch of higher powered DBS satellites in the early 1990s which transmitted their signals on the Ku band frequencies.[4][33] Satellite television channels at that time were intended to be used by cable television networks rather than received by home viewers.[34] Early satellite television receiver systems were largely constructed by hobbyists and engineers. These early TVRO systems operated mainly on the C-band frequencies and the dishes required were large; typically over 3 meters (10\u00a0ft) in diameter.[35] Consequently, TVRO is often referred to as \"big dish\" or \"Big Ugly Dish\" (BUD) satellite television.\n TVRO systems were designed to receive analog and digital satellite feeds of both television or audio from both C-band and Ku-band transponders on FSS-type satellites.[36][37] The higher frequency Ku-band systems tend to resemble DBS systems and can use a smaller dish antenna because of the higher power transmissions and greater antenna gain. TVRO systems tend to use larger rather than smaller satellite dish antennas, since it is more likely that the owner of a TVRO system would have a C-band-only setup rather than a Ku band-only setup. Additional receiver boxes allow for different types of digital satellite signal reception, such as DVB/MPEG-2 and 4DTV.\n The narrow beam width of a normal parabolic satellite antenna means it can only receive signals from a single satellite at a time.[38] Simulsat or the Vertex-RSI TORUS, is a quasi-parabolic satellite earthstation antenna that is capable of receiving satellite transmissions from 35 or more C- and Ku-band satellites simultaneously.[39]\n In 1945 British science fiction writer Arthur C. Clarke proposed a worldwide communications system which would function by means of three satellites equally spaced apart in Earth orbit.[40][41] This was published in the October 1945 issue of the Wireless World magazine and won him the Franklin Institute's Stuart Ballantine Medal in 1963.[42][43]\n The first satellite relayed communication was achieved early on in the space age, after the first relay test was conducted by Pioneer 1 and the first radio broadcast by SCORE at the end of 1958, after at the beginning of the year Sputnik I became the first satellite in history.\n The first public satellite television signals from Europe to North America were relayed via the Telstar satellite over the Atlantic ocean on 23 July 1962, although a test broadcast had taken place almost two weeks earlier on 11 July.[45] The signals were received and broadcast in North American and European countries and watched by over 100 million.[45] Launched in 1962, the Relay 1 satellite was the first satellite to transmit television signals from the US to Japan.[46] The first geosynchronous communication satellite, Syncom 2, was launched on 26 July 1963.[47] The subsequent first geostationary Syncom 3, orbiting near the International Date Line, was used to telecast the 1964 Olympic Games from Tokyo to the United States.[48][49]\n The world's first commercial communications satellite, called Intelsat I and nicknamed \"Early Bird\", was launched into geosynchronous orbit on April 6, 1965.[50] The first national network of television satellites, called Orbita, was created by the Soviet Union in October 1967, and was based on the principle of using the highly elliptical Molniya satellite for rebroadcasting and delivering of television signals to ground downlink stations.[51]\n The first domestic satellite to carry television transmissions was Canada's geostationary Anik 1, which was launched on 9 November 1972.[54]\n ATS-6, the world's first experimental educational and direct broadcast satellite (DBS), was launched on 30 May 1974.[55] It transmitted at 860\u00a0MHz using wideband FM modulation and had two sound channels. The transmissions were focused on the Indian subcontinent but experimenters were able to receive the signal in Western Europe using home constructed equipment that drew on UHF television design techniques already in use.[56]\n The first in a series of Soviet geostationary satellites to carry direct-to-home television, Ekran 1, was launched on 26 October 1976.[57] It used a 714\u00a0MHz UHF downlink frequency so that the transmissions could be received with existing UHF television technology rather than microwave technology.[58]\n The satellite television industry developed in the US from the cable television industry as communication satellites were being used to distribute television programming to remote cable television headends. Home Box Office (HBO), Turner Broadcasting System (TBS), and Christian Broadcasting Network (CBN, later The Family Channel) were among the first to use satellite television to deliver programming. Taylor Howard of San Andreas, California, became the first person to receive C-band satellite signals with his home-built system in 1976.[59]\n In the US, PBS, a non-profit public broadcasting service, began to distribute its television programming by satellite in 1978.[60]\n In 1979, Soviet engineers developed the Moskva (or Moscow) system of broadcasting and delivering of TV signals via satellites. They launched the Gorizont communication satellites later that same year. These satellites used geostationary orbits.[61] They were equipped with powerful on-board transponders, so the size of receiving parabolic antennas of downlink stations was reduced to 4 and 2.5 metres.[61] On October 18, 1979, the Federal Communications Commission (FCC) began allowing people to have home satellite earth stations without a federal government license.[62] The front cover of the 1979 Neiman-Marcus Christmas catalogue featured the first home satellite TV stations on sale for $36,500.[63] The dishes were nearly 20 feet (6.1\u00a0m) in diameter[64] and were remote controlled.[65] The price went down by half soon after that, but there were only eight more channels.[66] The Society for Private and Commercial Earth Stations (SPACE), an organisation which represented consumers and satellite TV system owners, was established in 1980.[67]\n Early satellite television systems were not very popular due to their expense and large dish size.[68] The satellite television dishes of the systems in the late 1970s and early 1980s were 10 to 16 feet (3.0 to 4.9\u00a0m) in diameter,[69] made of fibreglass or solid aluminum or steel,[70] and in the United States cost more than $5,000, sometimes as much as $10,000.[71] Programming sent from ground stations was relayed from eighteen satellites in geostationary orbit located 22,300 miles (35,900\u00a0km) above the Earth.[72][73]\n By 1980, satellite television was well established in the US and Europe. On 26 April 1982, the first satellite channel in the UK, Satellite Television Ltd. (later Sky One), was launched.[74] Its signals were transmitted from the ESA's Orbital Test Satellites.[74] Between 1981 and 1985, TVRO systems' sales rates increased as prices fell. Advances in receiver technology and the use of gallium arsenide FET technology enabled the use of smaller dishes. Five hundred thousand systems, some costing as little as $2000, were sold in the US in 1984.[71][75] Dishes pointing to one satellite were even cheaper.[76] People in areas without local broadcast stations or cable television service could obtain good-quality reception with no monthly fees.[71][73] The large dishes were a subject of much consternation, as many people considered them eyesores, and in the US most condominiums, neighborhoods, and other homeowner associations tightly restricted their use, except in areas where such restrictions were illegal.[4] These restrictions were altered in 1986 when the Federal Communications Commission ruled all of them illegal.[68] A municipality could require a property owner to relocate the dish if it violated other zoning restrictions, such as a setback requirement, but could not outlaw their use.[68] The necessity of these restrictions would slowly decline as the dishes got smaller.[68]\n Originally, all channels were broadcast in the clear (ITC) because the equipment necessary to receive the programming was too expensive for consumers. With the growing number of TVRO systems, the program providers and broadcasters had to scramble their signal and develop subscription systems.\n In October 1984, the U.S. Congress passed the Cable Communications Policy Act of 1984, which gave those using TVRO systems the right to receive signals for free unless they were scrambled, and required those who did scramble to make their signals available for a reasonable fee.[73][77] Since cable channels could prevent reception by big dishes, other companies had an incentive to offer competition.[78] In January 1986, HBO began using the now-obsolete VideoCipher II system to encrypt their channels.[69] Other channels used less secure television encryption systems. The scrambling of HBO was met with much protest from owners of big-dish systems, most of which had no other option at the time for receiving such channels, claiming that clear signals from cable channels would be difficult to receive.[79] Eventually HBO allowed dish owners to subscribe directly to their service for $12.95 per month, a price equal to or higher than what cable subscribers were paying, and required a descrambler to be purchased for $395.[79] This led to the attack on HBO's transponder Galaxy 1 by John R. MacDougall in April 1986.[79] One by one, all commercial channels followed HBO's lead and began scrambling their channels.[80] The Satellite Broadcasting and Communications Association (SBCA) was founded on December 2, 1986, as the result of a merger between SPACE and the Direct Broadcast Satellite Association (DBSA).[75]\n Videocipher II used analog scrambling on its video signal and Data Encryption Standard\u2013based encryption on its audio signal. VideoCipher II was defeated, and there was a black market for descrambler devices which were initially sold as \"test\" devices.[80]\n By 1987, nine channels were scrambled, but 99 others were available free-to-air.[77] While HBO initially charged a monthly fee of $19.95, soon it became possible to unscramble all channels for $200 a year.[77] Dish sales went down from 600,000 in 1985 to 350,000 in 1986, but pay television services were seeing dishes as something positive since some people would never have cable service, and the industry was starting to recover as a result.[77] Scrambling also led to the development of pay-per-view events.[77] On November 1, 1988, NBC began scrambling its C-band signal but left its Ku band signal unencrypted in order for affiliates to not lose viewers who could not see their advertising.[81] Most of the two million satellite dish users in the United States still used C-band.[81] ABC and CBS were considering scrambling, though CBS was reluctant due to the number of people unable to receive local network affiliates.[81] The piracy on satellite television networks in the US led to the introduction of the Cable Television Consumer Protection and Competition Act of 1992. This legislation enabled anyone caught engaging in signal theft to be fined up to $50,000 and to be sentenced to a maximum of two years in prison.[82] A repeat offender can be fined up to $100,000 and be imprisoned for up to five years.[82]\n Satellite television had also developed in Europe but it initially used low power communication satellites and it required dish sizes of over 1.7 metres. On 11 December 1988, however, Luxembourg launched Astra 1A, the first satellite to provide medium power satellite coverage to Western Europe.[83] This was one of the first medium-powered satellites, transmitting signals in Ku band and allowing reception with small dishes (90\u00a0cm).[83] The launch of Astra beat the winner of the UK's state Direct Broadcast Satellite licence holder, British Satellite Broadcasting, to the market.\n Commercial satellite broadcasts have existed in Japan since 1992 led by NHK which is influential in the development of regulations and has access to government funding for research. Their entry into the market was protected by the Ministry of Posts and Telecommunications (MPT) resulting in the WOWOW channel that is encrypted and can be accessed from NHK dishes with a decoder.[84]\n In the US in the early 1990s, four large cable companies launched PrimeStar, a direct broadcasting company using medium power satellites. The relatively strong transmissions allowed the use of smaller (90\u00a0cm) dishes. Its popularity declined with the 1994 launch of the Hughes DirecTV and Dish Network satellite television systems.\n Digital satellite broadcasts began in 1994 in the United States through DirecTV using the DSS format. They were launched (with the DVB-S standard) in South Africa, Middle East, North Africa and Asia-Pacific in 1994 and 1995, and in 1996 and 1997 in European countries including France, Germany, Spain, Portugal, Italy and the Netherlands, as well as Japan, North America and Latin America. Digital DVB-S broadcasts in the United Kingdom and Ireland started in 1998. Japan started broadcasting with the ISDB-S standard in 2000.\n On March 4, 1996, EchoStar introduced Digital Sky Highway (Dish Network) using the EchoStar 1 satellite.[85] EchoStar launched a second satellite in September 1996 to increase the number of channels available on Dish Network to 170.[85] These systems provided better pictures and stereo sound on 150\u2013200 video and audio channels, and allowed small dishes to be used. This greatly reduced the popularity of TVRO systems. In the mid-1990s, channels began moving their broadcasts to digital television transmission using the DigiCipher conditional access system.[86]\n In addition to encryption, the widespread availability, in the US, of DBS services such as PrimeStar and DirecTV had been reducing the popularity of TVRO systems since the early 1990s. Signals from DBS satellites (operating in the more recent Ku band) are higher in both frequency and power (due to improvements in the solar panels and energy efficiency of modern satellites) and therefore require much smaller dishes than C-band, and the digital modulation methods now used require less signal strength at the receiver than analog modulation methods.[87] Each satellite also can carry up to 32 transponders in the Ku band, but only 24 in the C band, and several digital subchannels can be multiplexed (MCPC) or carried separately (SCPC) on a single transponder.[88] Advances in noise reduction due to improved microwave technology and semiconductor materials have also had an effect.[88] However, one consequence of the higher frequencies used for DBS services is rain fade where viewers lose signal during a heavy downpour. C-band satellite television signals are less prone to rain fade.[89]\n In a return to the older (but proven) technologies of satellite communication, the current DBS-based satellite providers in the US (Dish Network and DirecTV) are now utilizing additional capacity on the Ku-band transponders of existing FSS-class satellites, in addition to the capacity on their own existing fleets of DBS satellites in orbit. This was done in order to provide more channel capacity for their systems, as required by the increasing number of High-Definition and simulcast local station channels. The reception of the channels carried on the Ku-band FSS satellite's respective transponders has been achieved by both DirecTV & Dish Network issuing to their subscribers dishes twice as big in diameter (36\") than the previous 18\" (& 20\" for the Dish Network \"Dish500\") dishes the services used initially, equipped with 2 circular-polarized LNBFs (for reception of 2 native DBS satellites of the provider, 1 per LNBF), and 1 standard linear-polarized LNB for reception of channels from an FSS-type satellite. These newer DBS/FSS-hybrid dishes, marketed by DirecTV and Dish Network as the \"SlimLine\" and \"SuperDish\" models respectively, are now the current standard for both providers, with their original 18\"/20\" single or dual LNBF dishes either now obsolete, or only used for program packages, separate channels, or services only broadcast over the providers' DBS satellites.\n On 29 November 1999 US President Bill Clinton signed the Satellite Home Viewer Improvement Act (SHVIA).[90] The act allowed Americans to receive local broadcast signals via direct broadcast satellite systems for the first time.[90]\n The 1963 Radio Regulations of the International Telecommunication Union (ITU) defined a \"broadcasting satellite service\" as a \"space service in which signals transmitted or retransmitted by space stations, or transmitted by reflection from objects in orbit around the Earth, are intended for direct reception by the general public.\"[91]\n In the 1970s some states grew concerned that external broadcasting could alter the cultural or political identity of a state leading to the New World Information and Communication Order (NWICO) proposal. However, satellite broadcasts can not be restricted on a per-state basis due to the limitations of the technology. Around the time the MacBride report was released, satellite broadcasting was being discussed at the UN Committee on the Peaceful Uses of Outer Space (COPUOS) where most of the members supported prior consent restrictions for broadcasting in their territories, but some argued this would violate freedom of information. The parties were unable to reach a consensus on this and in 1982 submitted UNGA Res 37/92 (\"DBS Principles\") to the UN General Assembly which was adopted by a majority vote, however, most States capable of DBS voted against it. The \"DBS Principles\" resolution is generally regarded as ineffective.[92]\n 1More than 400,000 television service subscribers.\n"
    },
    {
        "title": "In flight",
        "url": "https://en.wikipedia.org/wiki/In_flight",
        "content": "In baseball, the rules state that a batted ball is considered in flight when it has not yet touched any object other than a fielder or his equipment. Such a ball can be caught by a fielder to put the batter out. \n Once a batted ball touches the ground, a fence or wall, a foul pole, a base, the pitcher's rubber, an umpire, or a baserunner, it is no longer in flight. A batted ball that passes entirely out of the playing field ceases to be in flight when that occurs; if it was between the foul poles at that moment, then it is a home run which entitles the batter (and any other runners on base) to score.\n A special rule exists in covered baseball facilities (retractable or fixed roofed), where a batted ball striking the roof, roof supporting structure, or objects suspended from the roof (e.g., speakers) while in fair territory is still considered to be in flight. Rules for batted balls striking any of those objects in foul territory differ between ballparks, with most considering such a ball to still be in flight, and some considering it to be a foul ball and dead from the time it strikes.\n If a batted ball (other than a foul tip, with less than 2 strikes) is caught in flight, the batter is out\u2014called a fly out\u2014and all runners must tag up, meaning that they are out if a fielder with possession of the ball touches their starting base (time-of-pitch base) before they do. A batted ball cannot be ruled foul or fair while in flight; a batted ball that is past first or third base will be called foul or fair based on where it ceases to be in flight, or where it is first touched by a fielder, whichever occurs first.  A fly out on a ball in foul territory is also called a foul out.[1] A foul tip, which by definition is always caught in flight, is a strike by special rule, and not an out, unless caught as a 3rd strike.\n If a batted ball passes out of the playing field in flight and is fair, it is an automatic home run, entitling the batter and all runners to score without liability to be put out. However, if the fence or other barrier is less than 250 feet from home plate, a ball hit past that fence in flight and fair shall be ruled an automatic double. In the United States, such short fences are very rare even in the lowest-level amateur ballfields.  Fields with short fences can be commonplace in some countries where baseball is less popular; often, soccer fields have to be used, resulting in a very short left or right field.\n The shortest fair fences in Major League Baseball are both in Boston's Fenway Park; the shortest fence that is nearly perpendicular to the foul line is the Green Monster.  The left foul pole, renamed \"Fisk's Pole\" in honor of Carlton Fisk's famous home run in the 1975 World Series, stands 310 feet away from home plate.  The right field foul pole, known as Pesky's Pole, is 302 feet down the right field line, although the wall there is nearly parallel to the foul line as it curves back to the distant right field wall at 380 feet.  From 1958 through 1961, the Los Angeles Dodgers played home games in Los Angeles Memorial Coliseum, a stadium built for track and field; without the ability to move any of the permanent stadium structure, the Dodgers configured the field to result in a 251-foot left field foul line distance.\n"
    },
    {
        "title": "Russia",
        "url": "https://en.wikipedia.org/wiki/Russian_Federation",
        "content": "\n \n Russia,[b] or the Russian Federation,[c] is a country spanning Eastern Europe and North Asia. It is the largest country in the world by area, extending across eleven time zones and sharing land borders with fourteen countries.[d] It is the world's ninth-most populous country and Europe's most populous country. Russia is a highly urbanised country, with 16 of its population centres having more than 1\u00a0million inhabitants. Its capital and largest city is Moscow. Saint Petersburg is Russia's second-largest city and its cultural capital.\n The East Slavs emerged as a recognised group in Europe between the 3rd and 8th centuries CE. The first East Slavic state, Kievan Rus', arose in the 9th century, and in 988, it adopted Orthodox Christianity from the Byzantine Empire. Kievan Rus' ultimately disintegrated; the Grand Duchy of Moscow led the unification of Russian lands, leading to the proclamation of the Tsardom of Russia in 1547. By the early 18th century, Russia had vastly expanded through conquest, annexation, and the efforts of Russian explorers, developing into the Russian Empire, which remains the third-largest empire in history. However, with the Russian Revolution in 1917, Russia's monarchic rule was abolished and eventually replaced by the Russian SFSR\u2014the world's first constitutionally socialist state. Following the Russian Civil War, the Russian SFSR established the Soviet Union with three other Soviet republics, within which it was the largest and principal constituent. At the expense of millions of lives, the Soviet Union underwent rapid industrialisation in the 1930s and later played a decisive role for the Allies in World War II by leading large-scale efforts on the Eastern Front. With the onset of the Cold War, it competed with the United States for ideological dominance and international influence. The Soviet era of the 20th century saw some of the most significant Russian technological achievements, including the first human-made satellite and the first human expedition into outer space.\n In 1991, the Russian SFSR emerged from the dissolution of the Soviet Union as the Russian Federation. A new constitution was adopted, which established a federal semi-presidential system. Since the turn of the century, Russia's political system has been dominated by Vladimir Putin, under whom the country has experienced democratic backsliding and become an authoritarian dictatorship. Russia has been militarily involved in a number of conflicts in former Soviet states and other countries, including its war with Georgia in 2008 and its war with Ukraine since 2014, which has involved the internationally unrecognised annexations of Ukrainian territory including Crimea in 2014 and four other regions in 2022 during an ongoing invasion.\n Russia is a permanent member of the United Nations Security Council; a member state of the G20, SCO, BRICS, APEC, OSCE, and WTO; and the leading member state of post-Soviet organisations such as CIS, CSTO, and EAEU/EEU. It possesses the largest stockpile of nuclear weapons and has the third-highest military expenditure. Russia is generally considered a great power and is a regional power. Internationally, Russia ranks very low in measurements of democracy, human rights and freedom of the press; the country also has high levels of perceived corruption. As of 2024, Russia has a high-income economy which ranks eleventh in the world by nominal GDP and fourth at purchasing power parity, relying on its vast mineral and energy resources; the world's second-largest for oil production and natural gas production. Russia is home to 32 UNESCO World Heritage Sites.\n According to the Oxford English Dictionary, the English name Russia first appeared in the 14th century, borrowed from Medieval Latin: Russia, used in the 11th century and frequently in 12th-century British sources, in turn derived from Russi, 'the Russians' and the suffix -ia.[22][23] In modern historiography, this state is usually denoted as Kievan Rus' after its capital city.[24] Another Medieval Latin name for Rus' was Ruthenia.[25]\n In Russian, the current name of the country, \u0420\u043e\u0441\u0441\u0438\u044f (Rossiya), comes from the Byzantine Greek name for Rus', \u03a1\u03c9\u03c3\u03af\u03b1 (Ros\u00eda).[26] A new form of the name Rus', \u0420\u043e\u0441\u0438\u044f (Rosiya), was borrowed from the Greek term and first attested in 1387.[27][failed verification] The name Rossiia appeared in Russian sources in the late 15th century, but until the end of the 17th century the country was more often referred to by its inhabitants as Rus', the Russian land (Russkaia zemlia), or the Muscovite state (Moskovskoe gosudarstvo), among other variations.[28][29][30] In 1721, Peter the Great changed the name of the state from Tsardom of Russia (Russian: \u0420\u0443\u0441\u0441\u043a\u043e\u0435 \u0446\u0430\u0440\u0441\u0442\u0432\u043e, romanized:\u00a0Russkoye tsarstvo) or Tsardom of Muscovy (Russian: \u041c\u043e\u0441\u043a\u043e\u0432\u0441\u043a\u043e\u0435 \u0446\u0430\u0440\u0441\u0442\u0432\u043e, romanized:\u00a0Moskovskoye tsarstvo)[31][32] to Russian Empire (Rossiiskaia imperiia).[28][30]\n There are several words in Russian which translate to \"Russians\" in English. The noun and adjective \u0440\u0443\u0441\u0441\u043a\u0438\u0439, russkiy refers to ethnic Russians. The adjective \u0440\u043e\u0441\u0441\u0438\u0439\u0441\u043a\u0438\u0439, rossiiskiy denotes Russian citizens regardless of ethnicity. The same applies to the more recently coined noun \u0440\u043e\u0441\u0441\u0438\u044f\u043d\u0438\u043d, rossiianyn, \"Russian\" in the sense of citizen of the Russian state.[29][33]\n According to the Primary Chronicle, the word Rus' is derived from the Rus' people, who were a Swedish tribe, and where the three original members of the Rurikid dynasty came from.[34] The Finnish word for Swedes, ruotsi, has the same origin.[35]\nLater archeological studies mostly confirmed this theory.[36][better\u00a0source\u00a0needed]\n The first human settlement on Russia dates back to the Oldowan period in the early Lower Paleolithic. About 2 million years ago, representatives of Homo erectus migrated to the Taman Peninsula in southern Russia.[37] Flint tools, some 1.5 million years old, have been discovered in the North Caucasus.[38] Radiocarbon dated specimens from Denisova Cave in the Altai Mountains estimate the oldest Denisovan specimen lived 195\u2013122,700 years ago.[39] Fossils of Denny, an archaic human hybrid that was half Neanderthal and half Denisovan, and lived some 90,000 years ago, was also found within the latter cave.[40] Russia was home to some of the last surviving Neanderthals, from about 45,000 years ago, found in Mezmaiskaya cave.[41]\n The first trace of an early modern human in Russia dates back to 45,000 years, in Western Siberia.[42] The discovery of high concentration cultural remains of anatomically modern humans, from at least 40,000 years ago, was found at Kostyonki\u2013Borshchyovo,[43] and at Sungir, dating back to 34,600 years ago\u2014both in western Russia.[44] Humans reached Arctic Russia at least 40,000 years ago, in Mamontovaya Kurya.[45] Ancient North Eurasian populations from Siberia genetically similar to Mal'ta\u2013Buret' culture and Afontova Gora were an important genetic contributor to Ancient Native Americans and Eastern Hunter-Gatherers.[46]\n The Kurgan hypothesis places the Volga-Dnieper region of southern Russia and Ukraine as the urheimat of the Proto-Indo-Europeans.[48] Early Indo-European migrations from the Pontic\u2013Caspian steppe of Ukraine and Russia spread Yamnaya ancestry and Indo-European languages across large parts of Eurasia.[49][50] Nomadic pastoralism developed in the Pontic\u2013Caspian steppe beginning in the Chalcolithic.[51] Remnants of these steppe civilisations were discovered in places such as Ipatovo,[51] Sintashta,[52] Arkaim,[53] and Pazyryk,[54] which bear the earliest known traces of horses in warfare.[52] The genetic makeup of speakers of the Uralic language family in northern Europe was shaped by migration from Siberia that began at least 3,500 years ago.[55]\n In the 3rd to 4th centuries CE, the Gothic kingdom of Oium existed in southern Russia, which was later overrun by Huns. Between the 3rd and 6th centuries CE, the Bosporan Kingdom, which was a Hellenistic polity that succeeded the Greek colonies,[56] was also overwhelmed by nomadic invasions led by warlike tribes such as the Huns and Eurasian Avars.[57] The Khazars, who were of Turkic origin, ruled the steppes between the Caucasus in the south, to the east past the Volga river basin, and west as far as Kyiv on the Dnieper river until the 10th century.[58] After them came the Pechenegs who created a large confederacy, which was subsequently taken over by the Cumans and the Kipchaks.[59]\n The ancestors of Russians are among the Slavic tribes that separated from the Proto-Indo-Europeans, who appeared in the northeastern part of Europe c.\u20091500\u202fyears ago.[60] The East Slavs gradually settled western Russia (approximately between modern Moscow and Saint-Petersburg) in two waves: one moving from Kiev towards present-day Suzdal and Murom and another from Polotsk towards Novgorod and Rostov.[61] Prior to Slavic migration, that territory was populated by Finno-Ugrian peoples. From the 7th century onwards, the incoming East Slavs slowly assimilated the native Finno-Ugrians.[62][63]\n The establishment of the first East Slavic states in the 9th century coincided with the arrival of Varangians, the Vikings who ventured along the waterways extending from the eastern Baltic to the Black and Caspian Seas. According to the Primary Chronicle, a Varangian from the Rus' people, named Rurik, was elected ruler of Novgorod in 862. In 882, his successor Oleg ventured south and conquered Kiev, which had been previously paying tribute to the Khazars.[62] Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar Khaganate,[64] and launched several military expeditions to Bulgaria, Byzantium and Persia.[65][66]\n In the 10th to 11th centuries, Kievan Rus' became one of the largest and most prosperous states in Europe. The reigns of Vladimir the Great (980\u20131015) and his son Yaroslav the Wise (1019\u20131054) constitute the Golden Age of Kiev, which saw the acceptance of Orthodox Christianity from Byzantium, and the creation of the first East Slavic written legal code, the Russkaya Pravda.[62] The age of feudalism and decentralisation had come, marked by constant in-fighting between members of the Rurik dynasty that ruled Kievan Rus' collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, the Novgorod Republic in the north, and Galicia-Volhynia in the south-west.[62] By the 12th century, Kiev lost its pre-eminence and Kievan Rus' had fragmented into different principalities.[67] Prince Andrey Bogolyubsky sacked Kiev in 1169 and made Vladimir his base,[67] leading to political power being shifted to the north-east.[62]\n Led by Prince Alexander Nevsky, Novgorodians repelled the invading Swedes in the Battle of the Neva in 1240,[68] as well as the Germanic crusaders in the Battle on the Ice in 1242.[69]\n Kievan Rus' finally fell to the Mongol invasion of 1237\u20131240, which resulted in the sacking of Kiev and other cities, as well as the death of a major part of the population.[62] The invaders, later known as Tatars, formed the state of the Golden Horde, which ruled over Russia for the next two centuries.[70] Only the Novgorod Republic escaped foreign occupation after it agreed to pay tribute to the Mongols.[62] Galicia-Volhynia would later be absorbed by Lithuania and Poland, while the Novgorod Republic continued to prosper in the north. In the northeast, the Byzantine-Slavic traditions of Kievan Rus' were adapted to form the Russian autocratic state.[62]\n The destruction of Kievan Rus' saw the eventual rise of the Grand Duchy of Moscow, initially a part of Vladimir-Suzdal.[71]:\u200a11\u201320\u200a While still under the domain of the Mongol-Tatars and with their connivance, Moscow began to assert its influence in the region in the early 14th century,[72] gradually becoming the leading force in the \"gathering of the Russian lands\".[73][74] When the seat of the Metropolitan of the Russian Orthodox Church moved to Moscow in 1325, its influence increased.[75] Moscow's last rival, the Novgorod Republic, prospered as the chief fur trade centre and the easternmost port of the Hanseatic League.[76]\n Led by Prince Dmitry Donskoy of Moscow, the united army of Russian principalities inflicted a milestone defeat on the Mongol-Tatars in the Battle of Kulikovo in 1380.[62] Moscow gradually absorbed its parent duchy and surrounding principalities, including formerly strong rivals such as Tver and Novgorod.[73]\n Ivan\u00a0III (\"the Great\") threw off the control of the Golden Horde and consolidated the whole of northern Rus' under Moscow's dominion, and was the first Russian ruler to take the title \"Grand Duke of all Rus'\". After the fall of Constantinople in 1453, Moscow claimed succession to the legacy of the Eastern Roman Empire. Ivan\u00a0III married Sophia Palaiologina, the niece of the last Byzantine emperor Constantine\u00a0XI, and made the Byzantine double-headed eagle his own, and eventually Russia's, coat-of-arms.[73] Vasili III united all of Russia by annexing the last few independent Russian states in the early 16th century.[77]\n In development of the Third Rome ideas, the grand duke Ivan\u00a0IV (\"the Terrible\") was officially crowned the first tsar of Russia in 1547. The tsar promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (the Zemsky Sobor), revamped the military, curbed the influence of the clergy, and reorganised local government.[73] During his long reign, Ivan nearly doubled the already large Russian territory by annexing the three Tatar khanates: Kazan and Astrakhan along the Volga,[78] and the Khanate of Sibir in southwestern Siberia. Ultimately, by the end of the 16th century, Russia expanded east of the Ural Mountains.[79] However, the Tsardom was weakened by the long and unsuccessful Livonian War against the coalition of the Kingdom of Poland and the Grand Duchy of Lithuania (later the united Polish\u2013Lithuanian Commonwealth), the Kingdom of Sweden, and Denmark\u2013Norway for access to the Baltic coast and sea trade.[80] In 1572, an invading army of Crimean Tatars were thoroughly defeated in the crucial Battle of Molodi.[81]\n The death of Ivan's sons marked the end of the ancient Rurik dynasty in 1598, and in combination with the disastrous famine of 1601\u20131603, led to a civil war, the rule of pretenders, and foreign intervention during the Time of Troubles in the early 17th century.[82] The Polish\u2013Lithuanian Commonwealth, taking advantage, occupied parts of Russia, extending into the capital Moscow.[83] In 1612, the Poles were forced to retreat by the Russian volunteer corps, led by merchant Kuzma Minin and prince Dmitry Pozharsky.[84] The Romanov dynasty acceded to the throne in 1613 by the decision of the Zemsky Sobor, and the country started its gradual recovery from the crisis.[85]\n Russia continued its territorial growth through the 17th century, which was the age of the Cossacks.[86] In 1654, the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian tsar, Alexis; whose acceptance of this offer led to another Russo-Polish War. Ultimately, Ukraine was split along the Dnieper, leaving the eastern part, (Left-bank Ukraine and Kiev) under Russian rule.[87] In the east, the rapid Russian exploration and colonisation of vast Siberia continued, hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian River Routes, and by the mid-17th century, there were Russian settlements in eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the coast of the Pacific Ocean.[86] In 1648, Semyon Dezhnyov became the first European to navigate through the Bering Strait.[88]\n Under Peter the Great, Russia was proclaimed an empire in 1721, and established itself as one of the European great powers. Ruling from 1682 to 1725, Peter defeated Sweden in the Great Northern War (1700\u20131721), securing Russia's access to the sea and sea trade. In 1703, on the Baltic Sea, Peter founded Saint Petersburg as Russia's new capital. Throughout his rule, sweeping reforms were made, which brought significant Western European cultural influences to Russia.[89] He was succeeded by Catherine I (1725\u20131727), followed by Peter II (1727\u20131730), and Anna. The reign of Peter\u00a0I's daughter Elizabeth in 1741\u20131762 saw Russia's participation in the Seven Years' War (1756\u20131763). During the conflict, Russian troops overran East Prussia, reaching Berlin.[90] However, upon Elizabeth's death, all these conquests were returned to the Kingdom of Prussia by pro-Prussian Peter\u00a0III of Russia.[91]\n Catherine\u00a0II (\"the Great\"), who ruled in 1762\u20131796, presided over the Russian Age of Enlightenment. She extended Russian political control over the Polish\u2013Lithuanian Commonwealth and annexed most of its territories into Russia, making it the most populous country in Europe.[92] In the south, after the successful Russo-Turkish Wars against the Ottoman Empire, Catherine advanced Russia's boundary to the Black Sea, by dissolving the Crimean Khanate, and annexing Crimea.[93] As a result of victories over Qajar Iran through the Russo-Persian Wars, by the first half of the 19th century, Russia also conquered the Caucasus.[94] Catherine's successor, her son Paul, was unstable and focused predominantly on domestic issues.[95] Following his short reign, Catherine's strategy was continued with Alexander\u00a0I's (1801\u20131825) wresting of Finland from the weakened Sweden in 1809,[96] and of Bessarabia from the Ottomans in 1812.[97] In North America, the Russians became the first Europeans to reach and colonise Alaska.[98] In 1803\u20131806, the first Russian circumnavigation was made.[99] In 1820, a Russian expedition discovered the continent of Antarctica.[100]\n During the Napoleonic Wars, Russia joined alliances with various European powers, and fought against France. The French invasion of Russia at the height of Napoleon's power in 1812 reached Moscow, but eventually failed as the obstinate resistance in combination with the bitterly cold Russian winter led to a disastrous defeat of invaders, in which the pan-European Grande Arm\u00e9e faced utter destruction. Led by Mikhail Kutuzov and Michael Andreas Barclay de Tolly, the Imperial Russian Army ousted Napoleon and drove throughout Europe in the War of the Sixth Coalition, ultimately entering Paris.[101] Alexander I controlled Russia's delegation at the Congress of Vienna, which defined the map of post-Napoleonic Europe.[102]\n The officers who pursued Napoleon into Western Europe brought ideas of liberalism back to Russia, and attempted to curtail the tsar's powers during the abortive Decembrist revolt of 1825.[103] At the end of the conservative reign of Nicholas I (1825\u20131855), a zenith period of Russia's power and influence in Europe, was disrupted by defeat in the Crimean War.[104]\n Nicholas's successor Alexander\u00a0II (1855\u20131881) enacted significant changes throughout the country, including the emancipation reform of 1861.[105] These reforms spurred industrialisation, and modernised the Imperial Russian Army, which liberated much of the Balkans from Ottoman rule in the aftermath of the 1877\u20131878 Russo-Turkish War.[106] During most of the 19th and early 20th century, Russia and Britain colluded over Afghanistan and its neighbouring territories in Central and South Asia; the rivalry between the two major European empires came to be known as the Great Game.[107]\n The late 19th century saw the rise of various socialist movements in Russia. Alexander\u00a0II was assassinated in 1881 by revolutionary terrorists.[108] The reign of his son Alexander\u00a0III (1881\u20131894) was less liberal but more peaceful.[109]\n Under last Russian emperor, Nicholas\u00a0II (1894\u20131917), the Revolution of 1905 was triggered by the humiliating failure of the Russo-Japanese War.[110] The uprising was put down, but the government was forced to concede major reforms (Russian Constitution of 1906), including granting freedoms of speech and assembly, the legalisation of political parties, and the creation of an elected legislative body, the State Duma.[111]\n In 1914, Russia entered World War\u00a0I in response to Austria-Hungary's declaration of war on Russia's ally Serbia,[112] and fought across multiple fronts while isolated from its Triple Entente allies.[113] In 1916, the Brusilov Offensive of the Imperial Russian Army almost completely destroyed the Austro-Hungarian Army.[114] However, the already-existing public distrust of the regime was deepened by the rising costs of war, high casualties, and rumors of corruption and treason. All this formed the climate for the Russian Revolution of 1917, carried out in two major acts.[115] In early 1917, Nicholas II was forced to abdicate; he and his family were imprisoned and later executed during the Russian Civil War.[116] The monarchy was replaced by a shaky coalition of political parties that declared itself the Provisional Government,[117] and proclaimed the Russian Republic. On 19 January [O.S. 6 January], 1918, the Russian Constituent Assembly declared Russia a democratic federal republic (thus ratifying the Provisional Government's decision). The next day the Constituent Assembly was dissolved by the All-Russian Central Executive Committee.[115]\n An alternative socialist establishment co-existed, the Petrograd Soviet, wielding power through the democratically elected councils of workers and peasants, called soviets. The rule of the new authorities only aggravated the crisis in the country instead of resolving it, and eventually, the October Revolution, led by Bolshevik leader Vladimir Lenin, overthrew the Provisional Government and gave full governing power to the soviets, leading to the creation of the world's first socialist state.[115] The Russian Civil War broke out between the anti-communist White movement and the Bolsheviks with its Red Army.[118] In the aftermath of signing the Treaty of Brest-Litovsk that concluded hostilities with the Central Powers of World War\u00a0I; Bolshevist Russia surrendered most of its western territories, which hosted 34% of its population, 54% of its industries, 32% of its agricultural land, and roughly 90% of its coal mines.[119]\n The Allied powers launched an unsuccessful military intervention in support of anti-communist forces.[120] In the meantime, both the Bolsheviks and White movement carried out campaigns of deportations and executions against each other, known respectively as the Red Terror and White Terror.[121] By the end of the violent civil war, Russia's economy and infrastructure were heavily damaged, and as many as 10 million perished during the war, mostly civilians.[122] Millions became White \u00e9migr\u00e9s,[123] and the Russian famine of 1921\u20131922 claimed up to five\u00a0million victims.[124]\n On 30 December 1922, Lenin and his aides formed the Soviet Union, by joining the Russian SFSR into a single state with the Byelorussian, Transcaucasian, and Ukrainian republics.[125] Eventually internal border changes and annexations during World War II created a union of 15 republics; the largest in size and population being the Russian SFSR, which dominated the union politically, culturally, and economically.[126]\n Following Lenin's death in 1924, a troika was designated to take charge. Eventually Joseph Stalin, the General Secretary of the Communist Party, managed to suppress all opposition factions and consolidate power in his hands to become the country's dictator by the 1930s.[127] Leon Trotsky, the main proponent of world revolution, was exiled from the Soviet Union in 1929,[128] and Stalin's idea of Socialism in One Country became the official line.[129] The continued internal struggle in the Bolshevik party culminated in the Great Purge.[130]\n Under Stalin's leadership, the government launched a command economy, industrialisation of the largely rural country, and collectivisation of its agriculture. During this period of rapid economic and social change, millions of people were sent to penal labour camps, including many political convicts for their suspected or real opposition to Stalin's rule;[131] and millions were deported and exiled to remote areas of the Soviet Union.[132] The transitional disorganisation of the country's agriculture, combined with the harsh state policies and a drought,[133] led to the Soviet famine of 1932\u20131933; which killed 5.7[134] to 8.7 million, 3.3 million of them in the Russian SFSR.[135] The Soviet Union, ultimately, made the costly transformation from a largely agrarian economy to a major industrial powerhouse within a short span of time.[136]\n The Soviet Union entered World War II on 17 September 1939 with its invasion of Poland,[137] in accordance with a secret protocol within the Molotov\u2013Ribbentrop Pact with Nazi Germany.[138] The Soviet Union later invaded Finland,[139] and occupied and annexed the Baltic states,[140] as well as parts of Romania.[141]:\u200a91\u201395\u200a On 22 June 1941, Germany invaded the Soviet Union,[142] opening the Eastern Front, the largest theater of World War\u00a0II.[143]:\u200a7\u200a\n Eventually, some 5 million Red Army troops were captured by the Nazis;[144]:\u200a272\u200a the latter deliberately starved to death or otherwise killed 3.3\u00a0million Soviet POWs, and a vast number of civilians, as the \"Hunger Plan\" sought to fulfil Generalplan Ost.[145]:\u200a175\u2013186\u200a Although the Wehrmacht had considerable early success, their attack was halted in the Battle of Moscow.[146] Subsequently, the Germans were dealt major defeats first at the Battle of Stalingrad in the winter of 1942\u20131943,[147] and then in the Battle of Kursk in the summer of 1943.[148] Another German failure was the Siege of Leningrad, in which the city was fully blockaded on land between 1941 and 1944 by German and Finnish forces, and suffered starvation and more than a million deaths, but never surrendered.[149] Soviet forces steamrolled through Eastern and Central Europe in 1944\u20131945 and captured Berlin in May 1945.[150] In August 1945, the Red Army invaded Manchuria and ousted the Japanese from Northeast Asia, contributing to the Allied victory over Japan.[151]\n The 1941\u20131945 period of World War\u00a0II is known in Russia as the Great Patriotic War.[152] The Soviet Union, along with the United States, the United Kingdom and China were considered the Big Four of Allied powers in World War II, and later became the Four Policemen, which was the foundation of the United Nations Security Council.[153]:\u200a27\u200a During the war, Soviet civilian and military death were about 26\u201327 million,[154] accounting for about half of all World War\u00a0II casualties.[155]:\u200a295\u200a The Soviet economy and infrastructure suffered massive devastation, which caused the Soviet famine of 1946\u20131947.[156] However, at the expense of a large sacrifice, the Soviet Union emerged as a global superpower.[157]\n After World War II, according to the Potsdam Conference, the Red Army occupied parts of Eastern and Central Europe, including East Germany and the eastern regions of Austria.[158] Dependent communist governments were installed in the Eastern Bloc satellite states.[159] After becoming the world's second nuclear power,[160] the Soviet Union established the Warsaw Pact alliance,[161] and entered into a struggle for global dominance, known as the Cold War, with the rivalling United States and NATO.[162]\n After Stalin's death in 1953 and a short period of collective rule, the new leader Nikita Khrushchev denounced Stalin and launched the policy of de-Stalinization, releasing many political prisoners from the Gulag labour camps.[163] The general easement of repressive policies became known later as the Khrushchev Thaw.[164] At the same time, Cold War tensions reached its peak when the two rivals clashed over the deployment of the United States Jupiter missiles in Turkey and Soviet missiles in Cuba.[165]\n In 1957, the Soviet Union launched the world's first artificial satellite, Sputnik\u00a01, thus starting the Space Age.[166] Russian cosmonaut Yuri Gagarin became the first human to orbit the Earth, aboard the Vostok\u00a01 crewed spacecraft on 12 April 1961.[167]\n Following the ousting of Khrushchev in 1964, another period of collective rule ensued, until Leonid Brezhnev became the leader. The era of the 1970s and the early 1980s was later designated as the Era of Stagnation. The 1965 Kosygin reform aimed for partial decentralisation of the Soviet economy.[168] In 1979, after a communist-led revolution in Afghanistan, Soviet forces invaded the country, ultimately starting the Soviet\u2013Afghan War.[169] In May 1988, the Soviets started to withdraw from Afghanistan, due to international opposition, persistent anti-Soviet guerrilla warfare, and a lack of support by Soviet citizens.[170]\n From 1985 onwards, the last Soviet leader Mikhail Gorbachev, who sought to enact liberal reforms in the Soviet system, introduced the policies of glasnost (openness) and perestroika (restructuring) in an attempt to end the period of economic stagnation and to democratise the government.[171] This, however, led to the rise of strong nationalist and separatist movements across the country.[172] Prior to 1991, the Soviet economy was the world's second-largest, but during its final years, it went into a crisis.[173]\n By 1991, economic and political turmoil began to boil over as the Baltic states chose to secede from the Soviet Union.[174] On 17 March, a referendum was held, in which the vast majority of participating citizens voted in favour of changing the Soviet Union into a renewed federation.[175] In June 1991, Boris Yeltsin became the first directly elected President in Russian history when he was elected President of the Russian SFSR.[176] In August 1991, a coup d'\u00e9tat attempt by members of Gorbachev's government, directed against Gorbachev and aimed at preserving the Soviet Union, instead led to the end of the Communist Party of the Soviet Union.[177] On 25 December 1991, following the dissolution of the Soviet Union, along with contemporary Russia, fourteen other post-Soviet states emerged.[178]\n The economic and political collapse of the Soviet Union led Russia into a deep and prolonged depression. During and after the disintegration of the Soviet Union, wide-ranging reforms including privatisation and market and trade liberalisation were undertaken, including radical changes along the lines of \"shock therapy\".[179] The privatisation largely shifted control of enterprises from state agencies to individuals with inside connections in the government, which led to the rise of Russian oligarchs.[180] Many of the newly rich moved billions in cash and assets outside of the country in an enormous capital flight.[181] The depression of the economy led to the collapse of social services\u2014the birth rate plummeted while the death rate skyrocketed,[182][183] and millions plunged into poverty;[184] while extreme corruption,[185] as well as criminal gangs and organised crime rose significantly.[186]\n In late 1993, tensions between Yeltsin and the Russian parliament culminated in a constitutional crisis which ended violently through military force. During the crisis, Yeltsin was backed by Western governments, and over 100 people were killed.[187]\n In December, a referendum was held and approved, which introduced a new constitution, giving the president enormous powers.[188] The 1990s were plagued by armed conflicts in the North Caucasus, both local ethnic skirmishes and separatist Islamist insurrections.[189] From the time Chechen separatists declared independence in the early 1990s, an intermittent guerrilla war was fought between the rebel groups and Russian forces.[190] Terrorist attacks against civilians were carried out by Chechen separatists, claiming the lives of thousands of Russian civilians.[e][191]\n After the dissolution of the Soviet Union, Russia assumed responsibility for settling the latter's external debts.[192] In 1992, most consumer price controls were eliminated, causing extreme inflation and significantly devaluing the rouble.[193] High budget deficits coupled with increasing capital flight and inability to pay back debts, caused the 1998 Russian financial crisis, which resulted in a further GDP decline.[194]\n On 31 December 1999, President Yeltsin unexpectedly resigned,[195] handing the post to the recently appointed prime minister and his chosen successor, Vladimir Putin.[196] Putin then won the 2000 presidential election,[197] and defeated the Chechen insurgency in the Second Chechen War.[198]\n Putin won a second presidential term in 2004.[199] High oil prices and a rise in foreign investment saw the Russian economy and living standards improve significantly.[200] Putin's rule increased stability, while transforming Russia into an authoritarian state.[201] In 2008, Putin took the post of prime minister, while Dmitry Medvedev was elected President for one term, to hold onto power despite legal term limits;[202] this period has been described as a \"tandemocracy\".[203]\n Following a diplomatic crisis with neighbouring Georgia, the Russo-Georgian War took place during 1\u201312 August 2008, resulting in Russia recognising two separatist states in the territories that it occupies in Georgia.[204] It was the first European war of the 21st century.[205]\n In early 2014, following a pro-Western revolution in neighbouring Ukraine, Russia annexed Crimea after a disputed referendum on the status of Crimea was staged under Russian occupation.[206][207] The annexation generated an insurgency in the Donbas region of Ukraine, supported by Russian military intervention as part of an undeclared war against Ukraine.[208] Russian mercenaries and military forces, with the support of local separatist militias, waged a war in eastern Ukraine against the new Ukrainian government after the Russian government fostered anti-government and pro-Russian protests in the region,[209] although most residents had opposed secession from Ukraine.[210][211]\n In a major escalation of the conflict, Russia launched a full-scale invasion of Ukraine on 24 February 2022.[212] The invasion marked the largest conventional war in Europe since World War\u00a0II,[213] and was met with international condemnation,[214] as well as expanded sanctions against Russia.[215]\n As a result, Russia was expelled from the Council of Europe in March,[216] and was suspended from the United Nations Human Rights Council in April.[217] In September, following successful Ukrainian counteroffensives,[218] Putin announced a \"partial mobilisation\", Russia's first mobilisation since Operation Barbarossa.[219] In the end of September, Putin proclaimed the annexation of four partially-occupied Ukrainian regions, the largest annexation in Europe since World War\u00a0II.[220] Putin and Russian-installed leaders signed treaties of accession, internationally unrecognised and widely denounced as illegal.[220] As a result of the invasion, hundreds of thousands of people are estimated to have been killed or injured,[221][222] while Russia has been accused of numerous war crimes.[223][224][225] The war in Ukraine has further exacerbated Russia's demographic crisis.[226]\n In June 2023, the Wagner Group, a private military contractor fighting for Russia in Ukraine, declared an open rebellion against the Russian Ministry of Defence, capturing Rostov-on-Don, before beginning a march on Moscow. However, after negotiations between Wagner and the Belarusian government, the rebellion was called off.[227][228] The leader of the rebellion, Yevgeny Prigozhin, was later killed in a plane crash.[229]\n Russia's vast landmass stretches over the easternmost part of Europe and the northernmost part of Asia.[230] It spans the northernmost edge of Eurasia; and has the world's fourth-longest coastline, of over 37,653\u00a0km (23,396\u00a0mi).[f][232] Russia lies between latitudes 41\u00b0 and 82\u00b0 N, and longitudes 19\u00b0 E and 169\u00b0 W, extending some 9,000\u00a0km (5,600\u00a0mi) east to west, and 2,500 to 4,000\u00a0km (1,600 to 2,500\u00a0mi) north to south.[233] Russia, by landmass, is larger than three continents,[g] and has the same surface area as Pluto.[234]\n Russia has nine major mountain ranges, and they are found along the southernmost regions, which share a significant portion of the Caucasus Mountains (containing Mount Elbrus, which at 5,642\u00a0m (18,510\u00a0ft) is the highest peak in Russia and Europe);[9] the Altai and Sayan Mountains in Siberia; and in the East Siberian Mountains and the Kamchatka Peninsula in the Russian Far East (containing Klyuchevskaya Sopka, which at 4,750\u00a0m (15,584\u00a0ft) is the highest active volcano in Eurasia).[235][236] The Ural Mountains, running north to south through the country's west, are rich in mineral resources, and form the traditional boundary between Europe and Asia.[237] The lowest point in Russia and Europe, is situated at the head of the Caspian Sea, where the Caspian Depression reaches some 29 metres (95.1\u00a0ft) below sea level.[238]\n Russia, as one of the world's only three countries bordering three oceans,[230] has links with a great number of seas.[h][239] Its major islands and archipelagos include Novaya Zemlya, Franz Josef Land, Severnaya Zemlya, the New Siberian Islands, Wrangel Island, the Kuril Islands (four of which are disputed with Japan), and Sakhalin.[240][241] The Diomede Islands, administered by Russia and the United States, are just 3.8\u00a0km (2.4\u00a0mi) apart;[242] and Kunashir Island of the Kuril Islands is merely 20\u00a0km (12.4\u00a0mi) from Hokkaido, Japan.[2]\n Russia, home of over 100,000 rivers,[230] has one of the world's largest surface water resources, with its lakes containing approximately one-quarter of the world's liquid fresh water.[236] Lake Baikal, the largest and most prominent among Russia's fresh water bodies, is the world's deepest, purest, oldest and most capacious fresh water lake, containing over one-fifth of the world's fresh surface water.[243] Ladoga and Onega in northwestern Russia are two of the largest lakes in Europe.[230] Russia is second only to Brazil by total renewable water resources.[244] The Volga in western Russia, widely regarded as Russia's national river, is the longest river in Europe; and forms the Volga Delta, the largest river delta in the continent.[245] The Siberian rivers of Ob, Yenisey, Lena, and Amur are among the world's longest rivers.[246]\n The size of Russia and the remoteness of many of its areas from the sea result in the dominance of the humid continental climate throughout most of the country, except for the tundra and the extreme southwest. Mountain ranges in the south and east obstruct the flow of warm air masses from the Indian and Pacific oceans, while the European Plain spanning its west and north opens it to influence from the Atlantic and Arctic oceans.[247] Most of northwest Russia and Siberia have a subarctic climate, with extremely severe winters in the inner regions of northeast Siberia (mostly Sakha, where the Northern Pole of Cold is located with the record low temperature of \u221271.2\u00a0\u00b0C or \u221296.2\u00a0\u00b0F),[240] and more moderate winters elsewhere. Russia's vast coastline along the Arctic Ocean and the Russian Arctic islands have a polar climate.[247]\n The coastal part of Krasnodar Krai on the Black Sea, most notably Sochi, and some coastal and interior strips of the North Caucasus possess a humid subtropical climate with mild and wet winters.[247] In many regions of East Siberia and the Russian Far East, winter is dry compared to summer; while other parts of the country experience more even precipitation across seasons. Winter precipitation in most parts of the country usually falls as snow. The westernmost parts of Kaliningrad Oblast and some parts in the south of Krasnodar Krai and the North Caucasus have an oceanic climate.[247] The region along the Lower Volga and Caspian Sea coast, as well as some southernmost slivers of Siberia, possess a semi-arid climate.[248]\n Throughout much of the territory, there are only two distinct seasons, winter and summer; as spring and autumn are usually brief.[247] The coldest month is January (February on the coastline); the warmest is usually July. Great ranges of temperature are typical. In winter, temperatures get colder both from south to north and from west to east. Summers can be quite hot, even in Siberia.[249] Climate change in Russia is causing more frequent wildfires,[250] and thawing the country's large expanse of permafrost.[251]\n Russia, owing to its gigantic size, has diverse ecosystems, including polar deserts, tundra, forest tundra, taiga, mixed and broadleaf forest, forest steppe, steppe, semi-desert, and subtropics.[252] About half of Russia's territory is forested,[9] and it has the world's largest area of forest,[253] which sequester some of the world's highest amounts of carbon dioxide.[253][254]\n Russian biodiversity includes 12,500 species of vascular plants, 2,200 species of bryophytes, about 3,000 species of lichens, 7,000\u20139,000 species of algae, and 20,000\u201325,000 species of fungi. Russian fauna is composed of 320 species of mammals, over 732 species of birds, 75 species of reptiles, about 30 species of amphibians, 343 species of freshwater fish (high endemism), approximately 1,500 species of saltwater fishes, 9 species of cyclostomata, and approximately 100\u2013150,000 invertebrates (high endemism).[252][255] Approximately 1,100 rare and endangered plant and animal species are included in the Russian Red Data Book.[252]\n Russia's entirely natural ecosystems are conserved in nearly 15,000 specially protected natural territories of various statuses, occupying more than 10% of the country's total area.[252] They include 45 biosphere reserves,[256] 64 national parks, and 101 nature reserves.[257] Although in decline, the country still has many ecosystems which are still considered intact forest; mainly in the northern taiga areas, and the subarctic tundra of Siberia.[258] Russia had a Forest Landscape Integrity Index mean score of 9.02 in 2019, ranking 10th out of 172 countries; and the first ranked major nation globally.[259]\n Russia, by constitution, is a symmetric federal republic with a semi-presidential system, wherein the president is the head of state,[260] and the prime minister is the head of government.[9] It is structured as a multi-party representative democracy, with the federal government composed of three branches:[261]\n The president is elected by popular vote for a six-year term and may be elected no more than twice.[265][i] Ministries of the government are composed of the premier and his deputies, ministers, and selected other individuals; all are appointed by the president on the recommendation of the prime minister (whereas the appointment of the latter requires the consent of the State Duma). United Russia is the dominant political party in Russia, and has been described as \"big tent\" and the \"party of power\".[267][268]\n Under the administrations of Vladimir Putin, Russia has experienced democratic backsliding,[269][270] and has been described as an authoritarian dictatorship.[11][12][271] Putin's policies are generally referred to as Putinism.[272] According to Kathryn Stoner a succession of terms has developed to encapsulate the essence of \u201cPutinism\u201d in conjunction with the transformations within Putin\u2019s administrationfrom \u201cmanaged democracy\u201d during the initial phase of his second presidential term (2004\u20132008), to \u201ccompetitive authoritarianism\u201d characterized by a \u201ckleptocratic\u201d political economy, ultimately culminating in \u201cpersonalistic, autocratic, conservative populism\u201d or merely \u201cdictatorship\u201d in the era following 2012.[273]\n Russia, by 1993 constitution, is a symmetric (with the possibility of an asymmetric configuration) federation. Unlike the Soviet asymmetric model of the RSFSR, where only republics were \"subjects of the federation\", the current constitution raised the status of other regions to the level of republics and made all regions equal with the title \"subject of the federation\". The regions of Russia have reserved areas of competence, but regions do not have sovereignty, do not have the status of a sovereign state, do not have the right to indicate any sovereignty in their constitutions and do not have the right to secede from the country. The laws of the regions cannot contradict federal laws.[274]\n The federal subjects[j] have equal representation\u2014two delegates each\u2014in the Federation Council, the upper house of the Federal Assembly.[275] They do, however, differ in the degree of autonomy they enjoy.[276] The federal districts of Russia were established by Putin in 2000 to facilitate central government control of the federal subjects.[277] Originally seven, currently there are eight federal districts, each headed by an envoy appointed by the president.[278]\n Russia had the world's fifth-largest diplomatic network in 2019. It maintains diplomatic relations with 190 United Nations member states, four partially-recognised states, and three United Nations observer states; along with 144 embassies.[285] Russia is one of the five permanent members of the United Nations Security Council. The country is generally considered a great power,[286][287][288] though its status as a modern great power has been questioned following the struggles it has faced while invading Ukraine starting in 2022.[289][290] Russia is also a former superpower as the leading constituent of the former Soviet Union.[157] Russia is a member of the G20, the OSCE, and the APEC. Russia also takes a leading role in organisations such as the CIS,[291] the EAEU,[292] the CSTO,[293] the SCO,[294] and BRICS.[295]\n Russia maintains close relations with neighbouring Belarus, which is a part of the Union State, a supranational confederation of the two states.[296] Serbia has been a historically close ally of Russia, as both countries share a strong mutual cultural, ethnic, and religious affinity.[297] India is the largest customer of Russian military equipment, and the two countries share a strong strategic and diplomatic relationship since the Soviet era.[298] Russia wields influence across the geopolitically important South Caucasus and Central Asia; and the two regions have been described as Russia's \"backyard\".[299][300]\n In the 21st century, Russia has pursued an aggressive foreign policy aimed at securing regional dominance and international influence, as well as increasing domestic support for the government. Military intervention in the post-Soviet states include a war with Georgia in 2008 and a war with Ukraine beginning in 2014. Russia has also sought to increase its influence in the Middle East, most significantly through military intervention in the Syrian civil war. Cyberwarfare and airspace violations, along with electoral interference, have been used to increase perceptions of Russian power.[301] Russia's relations with neighbouring Ukraine and the Western world\u2014especially the United States, the European Union, the United Nations and NATO\u2014have collapsed; especially since the annexation of Crimea in 2014 and the beginning of a full-scale invasion in 2022.[302][303] Relations between Russia and China have significantly strengthened bilaterally and economically; due to shared political interests.[304] Turkey and Russia share a complex strategic, energy, and defence relationship.[305] Russia maintains cordial relations with Iran, as it is a strategic and economic ally.[306] Russia has also increasingly pushed to expand its influence across the Arctic,[307] Asia-Pacific,[308] Africa,[309] the Middle East,[310] and Latin America.[311] According to the Economist Intelligence Unit, two-thirds of the world's population live in countries such as China or India that are neutral or leaning towards Russia.[312][313]\n The Russian Armed Forces are divided into the Ground Forces, the Navy, and the Aerospace Forces\u2014and there are also two independent arms of service: the Strategic Missile Troops and the Airborne Troops.[9] As of 2021[update], the military have around a million active-duty personnel, which is the world's fifth-largest, and about 2\u201320 million reserve personnel.[315][316] It is mandatory for all male citizens aged 18\u201327 to be drafted for a year of service in the Armed Forces.[9]\n Russia is among the five recognised nuclear-weapons states, with the world's largest stockpile of nuclear weapons; over half of the world's nuclear weapons are owned by Russia.[317] Russia possesses the second-largest fleet of ballistic missile submarines,[318] and is one of the only three countries operating strategic bombers.[319] Russia maintains the world's third-highest military expenditure, spending $109 billion in 2023, corresponding to around 5.9% of its GDP.[320] In 2021 it was the world's second-largest arms exporter, and had a large and entirely indigenous defence industry, producing most of its own military equipment.[321]\n Violations of human rights in Russia have been increasingly reported by leading democracy and human rights groups. In particular, Amnesty International and Human Rights Watch say that Russia is not democratic and allows few political rights and civil liberties to its citizens.[323][324]\n Since 2004, Freedom House has ranked Russia as \"not free\" in its Freedom in the World survey.[325] Since 2011, the Economist Intelligence Unit has ranked Russia as an \"authoritarian regime\" in its Democracy Index, ranking it 144th out of 167 countries in 2023.[326] In regards to media freedom, Russia was ranked 155th out of 180 countries in Reporters Without Borders' Press Freedom Index for 2022.[327] The Russian government has been widely criticised by political dissidents and human rights activists for unfair elections,[328] crackdowns on opposition political parties and protests,[329][330] persecution of non-governmental organisations and enforced suppression and killings of independent journalists,[331][332][333] and censorship of mass media and internet.[334]\n Muslims, especially Salafis, have faced persecution in Russia.[335][336] To quash the insurgency in the North Caucasus, Russian authorities have been accused of indiscriminate killings,[337] arrests, forced disappearances, and torture of civilians.[338][339] In Dagestan, some Salafis along with facing government harassment based on their appearance, have had their homes blown up in counterinsurgency operations.[340][341] Chechens and Ingush in Russian prisons reportedly take more abuse than other ethnic groups.[342] During the 2022 invasion of Ukraine, Russia has set up filtration camps where many Ukrainians are subjected to abuses and forcibly sent to Russia; the camps have been compared to those used in the Chechen Wars.[343][344] Political repression also increased following the start of the invasion, with laws adopted that establish punishments for \"discrediting\" the armed forces.[345]\n Russia has introduced several restrictions on LGBT rights, including a 2020 ban on same-sex marriage and the designation of LGBT+ organisations such as the Russian LGBT Network as \"foreign agents\".[346][347]\n Russia's political system has been variously described as a kleptocracy,[348] an oligarchy,[349] and a plutocracy.[350] It was the lowest rated European country in Transparency International's Corruption Perceptions Index for 2023, ranking 141st out of 180 countries.[351] Russia has a long history of corruption, which is seen as a significant problem.[352] It affects various sectors, including the economy,[353] business,[354] public administration,[355] law enforcement,[356] healthcare,[357][358] education,[359] and the military.[360]\n The primary and fundamental statement of laws in Russia is the Constitution of the Russian Federation. Statutes, like the Russian Civil Code and the Russian Criminal Code, are the predominant legal sources of Russian law.[361][362][363]\n Russia has the world's second-largest illegal arms trade market, after the United States, is ranked first in Europe and 32nd globally in the Global Organized Crime Index, and is among the countries with the highest number of people in prison.[364][365][366]\n Russia has a mixed market economy, following a turbulent transition from the Soviet planned model during the 1990s.[368] Much of the country's economic activity is centred on its abundant and varied natural resources, particularly oil and natural gas.[369] Russia is classified by the World Bank as a high-income country,[370] with the world's ninth-largest economy by nominal GDP and the sixth-largest by PPP; by some measures, its economy ranks fourth or fifth in the world by PPP.[371] Services account for roughly 54% of total GDP, followed by industry (33%), while the agricultural sector is the smallest, at less than 4% of total GDP.[372] Russia has a labour force of roughly 70 million, which is the world's eighth-largest,[373] and a low official unemployment rate of 4.1%.[374]\n Russia is the world's thirteenth-largest exporter and the 21st-largest importer.[375][376] It relies heavily on revenues from oil and gas-related taxes and export tariffs, which accounted for 45% of Russia's federal budget revenues in January 2022,[377] and up to 60% of its exports in 2019.[378] Russia has one of the lowest levels of external debt among major economies,[379] and had the fifth-largest foreign exchange reserves, valued at over $601 billion,[380] although half of that is frozen abroad, and a significant amount is believed to have been spent on the Ukrainian war. Inequality of household income and wealth is among the highest among developed countries,[381] and there are considerable regional disparities in economic development.[382][383]\n After over a decade of post-Soviet rapid economic growth, backed by high oil prices and a surge in foreign exchange reserves and investment,[200] Russia's economy was damaged by a wave of international sanctions imposed in 2014 following the Russo-Ukrainian War and annexation of Crimea.[384] In the aftermath of the Russian invasion of Ukraine in 2022, the country has faced revamped sanctions and corporate boycotts,[385] becoming the most sanctioned country in the world,[386] in a move described as an \"all-out economic and financial war\" to isolate the Russian economy from the Western financial system.[215] Due to the resulting negative impact, the Russian government has stopped publishing a raft of economic data since April 2022.[387] Although Russia has maintained relative economic stability and growth\u2014driven primarily by high military spending, household consumption, and capital investment\u2014economists suggest the sanctions will have a long-term effect on the Russian economy.[388][389][390]\n Railway transport in Russia is mostly controlled by the state-run Russian Railways. The total length of common-used railway tracks is the world's third-longest, exceeding 87,000\u00a0km (54,100\u00a0mi).[392] As of 2019[update], Russia has the world's fifth-largest road network, with over 1.5 million\u00a0km of roads,[393] although its road density is among the world's lowest, due in part to its vast land area.[394] Russia's inland waterways are the world's longest, totaling 102,000\u00a0km (63,380\u00a0mi).[395] Russia has over 900 airports,[396] ranking seventh in the world, of which the busiest is Sheremetyevo International Airport in Moscow. Russia's largest port is the Port of Novorossiysk in Krasnodar Krai along the Black Sea.[397]\n Russia was widely described as an energy superpower.[398] It has the world's largest proven gas reserves,[399] the second-largest coal reserves,[400] the eighth-largest oil reserves,[401] and the largest oil shale reserves in Europe.[402] Russia is also the world's leading natural gas exporter,[403] the second-largest natural gas producer,[404] and the second-largest oil producer and exporter.[405][406] Russia's oil and gas production led to deep economic relationships with the European Union, China, and former Soviet and Eastern Bloc states.[407][408] For example, over the last decade, Russia's share of the total gas demand for the EU (including the United Kingdom) increased from 25% in 2009 to 32% in the weeks before the Russian invasion of Ukraine in February 2022.[408]\n In the mid-2000s, the share of the oil and gas sector in GDP was around 20%, and in 2013 it was 20\u201321% of GDP.[409] The share of oil and gas in Russia's exports (about 50%) and federal budget revenues (about 50%) is large, and the dynamics of Russia's GDP are highly dependent on oil and gas prices,[410] but the share in GDP is much less than 50%. According to the first such comprehensive assessment published by the Russian statistics agency Rosstat in 2021, the maximum total share of the oil and gas sector in Russia's GDP\u2014including extraction, refining, transport, sale of oil and gas, all goods and services used, and all supporting activities\u2014amounts to 19.2% in 2019 and 15.2% in 2020; this is comparable to the share of GDP in Norway and Kazakhstan, and much lower than the share of GDP in Saudi Arabia and the United Arab Emirates.[411][412][413][414][415]\n Russia is the world's fourth-largest electricity producer.[416] Natural gas is by far the largest source of energy, comprising over half of all primary energy and 42% of electricity consumption.[417][418] Russia was the first country to develop civilian nuclear power, building the world's first nuclear power plant in 1954;[419] it remains a pioneer in nuclear energy technology and is considered a world leader in fast neutron reactors.[420] Russia is the world's fourth-largest nuclear energy producer, which accounts for one-fourth of total electricity generation.[418][421] Russian energy policy aims to expand the role of nuclear energy and develop new reactor technology.[420]\n Russia ratified the Paris Agreement in 2019.[422] The country's greenhouse gas emissions are the world's fourth-largest.[423] Coal still accounts for nearly one-fifth of electricity generation (17.64%).[418] Russia is the fifth-largest hydroelectric producer as of 2022,[424] with hydro power also contributing to almost a fifth of total electricity generation (17.54%).[418] The use and development of other renewable energy resources remains negligible, as Russia is among the few countries without strong governmental or public support for expanding these energy resources.[421]\n Russia's agriculture sector contributes about 5% of the country's total GDP, although the sector employs about one-eighth of the total labour force.[425] It has the world's third-largest cultivated area, at 1,265,267 square kilometres (488,522\u00a0sq\u00a0mi). However, due to the harshness of its environment, only about 13.1% of its land is agricultural,[9] with an additional 7.4% being arable.[426] The country's agricultural land is considered part of the \"breadbasket\" of Europe.[427] More than one-third of the sown area is devoted to fodder crops, and the remaining farmland is used industrial crops, vegetables, and fruits.[425] The main product of Russian farming has always been grain, which occupies well over half the cropland.[425] Russia is the world's largest exporter of wheat,[428][429] the largest producer of barley and buckwheat, among the largest exporters of maize and sunflower oil, and the leading producer of fertiliser.[430]\n Various analysts of climate change adaptation foresee large opportunities for Russian agriculture during the rest of the 21st century as arability increases in Siberia, which would lead to both internal and external migration to the region.[431] Owing to its large coastline along three oceans and twelve marginal seas, Russia maintains the world's sixth-largest fishing industry; capturing nearly 5 million tons of fish in 2018.[432] It is home to the world's finest caviar, the beluga; and produces about one-third of all canned fish, and some one-fourth of the world's total fresh and frozen fish.[425]\n Russia spent about 1% of its GDP on research and development in 2019, with the world's tenth-highest budget.[433] It also ranked tenth worldwide in the number of scientific publications in 2020, with roughly 1.3 million papers.[434] Since 1904, Nobel Prize were awarded to 26 Soviets and Russians in physics, chemistry, medicine, economy, literature and peace.[435] Russia ranked 60th in the Global Innovation Index in 2024, down from 45th in 2021.[436][437]\n Since the times of Nikolay Lobachevsky, who pioneered the non-Euclidean geometry, and Pafnuty Chebyshev, a prominent tutor; Russian mathematicians became among the world's most influential.[438] Dmitry Mendeleev invented the Periodic table, the main framework of modern chemistry.[439] Nine Soviet and Russian mathematicians have been awarded with the Fields Medal. Grigori Perelman was offered the first ever Clay Millennium Prize Problems Award for his final proof of the Poincar\u00e9 conjecture in 2002, as well as the Fields Medal in 2006.[440]\n Alexander Popov was among the inventors of radio,[441] while Nikolai Basov and Alexander Prokhorov were co-inventors of laser and maser.[442] Oleg Losev made crucial contributions in the field of semiconductor junctions, and discovered light-emitting diodes.[443] Vladimir Vernadsky is considered one of the founders of geochemistry, biogeochemistry, and radiogeology.[444] \u00c9lie Metchnikoff is known for his groundbreaking research in immunology.[445] Ivan Pavlov is known chiefly for his work in classical conditioning.[446] Lev Landau made fundamental contributions to many areas of theoretical physics.[447]\n Nikolai Vavilov was best known for having identified the centres of origin of cultivated plants.[448] Trofim Lysenko was known mainly for Lysenkoism.[449] Many famous Russian scientists and inventors were \u00e9migr\u00e9s. Igor Sikorsky was an aviation pioneer.[450] Vladimir Zworykin was the inventor of the iconoscope and kinescope television systems.[451] Theodosius Dobzhansky was the central figure in the field of evolutionary biology for his work in shaping the modern synthesis.[452] George Gamow was one of the foremost advocates of the Big Bang theory.[453]\n Roscosmos is Russia's national space agency. The country's achievements in the field of space technology and space exploration can be traced back to Konstantin Tsiolkovsky, the father of theoretical astronautics, whose works had inspired leading Soviet rocket engineers, such as Sergey Korolyov, Valentin Glushko, and many others who contributed to the success of the Soviet space programme in the early stages of the Space Race and beyond.[454]:\u200a6\u20137,\u200a333\u200a\n In 1957, the first Earth-orbiting artificial satellite, Sputnik\u00a01, was launched. In 1961, the first human trip into space was successfully made by Yuri Gagarin. Many other Soviet and Russian space exploration records ensued. In 1963, Valentina Tereshkova became the first and youngest woman in space, having flown a solo mission on Vostok 6.[455] In 1965, Alexei Leonov became the first human to conduct a spacewalk, exiting the space capsule during Voskhod 2.[456]\n In 1957, Laika, a Soviet space dog, became the first animal to orbit the Earth, aboard Sputnik 2.[457] In 1966, Luna\u00a09 became the first spacecraft to achieve a survivable landing on a celestial body, the Moon.[458] In 1968, Zond 5 brought the first Earthlings (two tortoises and other life forms) to circumnavigate the Moon.[459] In 1970, Venera\u00a07 became the first spacecraft to land on another planet, Venus.[460] In 1971, Mars\u00a03 became the first spacecraft to land on Mars.[461]:\u200a34\u201360\u200a During the same period, Lunokhod 1 became the first space exploration rover,[462] while Salyut\u00a01 became the world's first space station.[463]\n Russia had 172 active satellites in space in April 2022, the world's third-highest.[464] Between the final flight of the Space Shuttle programme in 2011 and the 2020 SpaceX's first crewed mission, Soyuz rockets were the only launch vehicles capable of transporting astronauts to the ISS.[465] Luna\u00a025 launched in August 2023, was the first of the Luna-Glob Moon exploration programme.[466]\n According to the World Tourism Organisation, Russia was the sixteenth-most visited country in the world, and the tenth-most visited country in Europe, in 2018, with over 24.6 million visits.[467] According to Federal Agency for Tourism, the number of inbound trips of foreign citizens to Russia amounted to 24.4 million in 2019.[468] Russia's international tourism receipts in 2018 totaled $11.6 billion.[467] In 2019, travel and tourism accounted for about 4.8% of country's total GDP.[469] In the wake of the COVID-19 pandemic, tourism declined precipitously in 2020, to just over 6.3 million foreign visitors.[470]\n Major tourist routes in Russia include a journey around the Golden Ring of Russia, a theme route of ancient Russian cities; cruises on large rivers such as the Volga; hikes on mountain ranges such as the Caucasus Mountains,[471] and journeys on the famous Trans-Siberian Railway.[472] Russia's most visited and popular landmarks include Red Square, the Peterhof Palace, the Kazan Kremlin, the Trinity Lavra of St. Sergius and Lake Baikal.[473]\n Moscow, the nation's cosmopolitan capital and historic core, is a bustling modern megacity; it retains classical and Soviet-era architecture while boasting high art, world class ballet, and modern skyscrapers.[474] Saint Petersburg, the imperial capital, is famous for its classical architecture, cathedrals, museums and theatres, white nights, crisscrossing rivers and numerous canals.[475] Russia is famed worldwide for its rich museums, such as the State Russian, the State Hermitage, and the Tretyakov Gallery; and for theatres such as the Bolshoi and the Mariinsky. The Moscow Kremlin and the Saint Basil's Cathedral are among the cultural landmarks of Russia.[476]\n Russia had a population of 144.7 million in 2021 (excluding Crimea and Sevastopol),[17] growing from 142.8 million in 2010.[477] It is the most populous country in Europe and ninth most populous in the world.[478] With a population density of 8 inhabitants\u00a0per square kilometre (21 inhabitants/sq\u00a0mi), Russia is one of the world's most sparsely populated countries,[9] with the vast majority of its people concentrated within its western part.[479] The country is highly urbanised, with two-thirds of the population living in towns and cities,\n Russia's population peaked at over 148 million in 1993, having subsequently declined due to its death rate exceeding its birth rate, which some analysts have called a demographic crisis.[480] In 2009, it recorded annual population growth for the first time in fifteen years, and subsequently experienced annual population growth due to declining death rates, increased birth rates, and increased immigration.[481] However, these population gains have been reversed since 2020, as excessive deaths from the COVID-19 pandemic resulted in the largest peacetime decline in its history.[482] Following the Russian invasion of Ukraine in 2022, the demographic crisis has deepened,[483] owing to reportedly high military fatalities and renewed emigration caused by Western mass-sanctions and boycotts.[484]\n In 2022, the total fertility rate across Russia was estimated to be 1.42 children born per woman,[485] which is below the replacement rate of 2.1 and among the lowest in the world.[486] Subsequently, the nation has one of the world's oldest populations, with a median age of 40.3 years.[9]\n Russia is a multinational state with many subnational entities associated with different minorities.[487] There are over 193 ethnic groups nationwide. In the 2010 census, roughly 81% of the population were ethnic Russians, and the remaining 19% of the population were ethnic minorities;[488] while over four-fifths of Russia's population was of European descent\u2014of whom the vast majority were Slavs,[489] with a substantial minority of Finno-Ugric and Germanic peoples.[490][491] According to the United Nations, Russia's immigrant population is the world's third-largest, numbering over 11.6 million;[492] most of which are from post-Soviet states, mainly from Central Asia.[493]\n\n Russian is the official and the predominantly spoken language in Russia.[3] It is the most spoken native language in Europe, the most geographically widespread language of Eurasia, as well as the world's most widely spoken Slavic language.[496] Russian is one of two official languages aboard the International Space Station,[497] as well as one of the six official languages of the United Nations.[496]\n Russia is a multilingual nation; approximately 100\u2013150 minority languages are spoken across the country.[498][499] According to the Russian Census of 2010, 137.5\u00a0million across the country spoke Russian, 4.3\u00a0million spoke Tatar, and 1.1\u00a0million spoke Ukrainian.[500] The constitution gives the country's individual republics the right to establish their own state languages in addition to Russian, as well as guarantee its citizens the right to preserve their native language and to create conditions for its study and development.[501] However, various experts have claimed Russia's linguistic diversity is rapidly declining due to many languages becoming endangered.[502][503]\n Russia is constitutionally a secular state that officially enshrines freedom of religion.[504] The largest religion is Eastern Orthodox Christianity, chiefly represented by the Russian Orthodox Church,[505] which is legally recognised for its \"special role\" in the country's \"history and the formation and development of its spirituality and culture.\"[504] Christianity, Islam, Judaism, and Buddhism are recognised by Russian law as the \"traditional\" religions of the country constituting its \"historical heritage\".[506][507]\n Islam is the second-largest religion in Russia and is traditional among the majority of peoples in the North Caucasus and some Turkic peoples in the Volga-Ural region.[505] Large populations of Buddhists are found in Kalmykia, Buryatia, Zabaykalsky Krai, and they are the vast majority of the population in Tuva.[505] Many Russians practise other religions, including Rodnovery (Slavic Neopaganism),[508] Assianism (Scythian Neopaganism),[509] other ethnic Paganisms, and inter-Pagan movements such as Ringing Cedars' Anastasianism,[510] various movements of Hinduism,[511] Siberian shamanism[512] and Tengrism, various Neo-Theosophical movements such as Roerichism, and other faiths.[513][514] Some religious minorities have faced oppression and some have been banned in the country;[515] notably, in 2017 the Jehovah's Witnesses were outlawed in Russia, facing persecution ever since, after having been declared an \"extremist\" and \"nontraditional\" faith.[516]\n In 2012, the research organisation Sreda, in cooperation with the Ministry of Justice, published the Arena Atlas, an adjunct to the 2010 census, enumerating in detail the religious populations and nationalities of Russia, based on a large-sample country-wide survey. The results showed that 47.3% of Russians declared themselves Christians\u2014including 41% Russian Orthodox, 1.5% simply Orthodox or members of non-Russian Orthodox churches, 4.1% unaffiliated Christians, and less than 1% Old Believers, Catholics or Protestants\u201425% were believers without affiliation to any specific religion, 13% were atheists, 6.5% were Muslims,[k] 1.2% were followers of \"traditional religions honouring gods and ancestors\" (Rodnovery, other Paganisms, Siberian shamanism and Tengrism), 0.5% were Buddhists, 0.1% were religious Jews and 0.1% were Hindus.[505]\n In 2024, the Public Opinion Foundation (FOM) found that 61.8% of Russians identify as Orthodox Christians, 2.6% as other Christians, 9.5% as Muslims, 21.2% as not religious, 1.4% follow other religions and 3.5% are unsure about their belief.[6][7] According to the survey, Orthodoxy is more widespread among women, people aged 60 and older, and people living in the Central and Southern Federal Districts, while Islam is the dominant religion in the North Caucasian Federal District.\n Russia has an adult literacy rate of 100%,[518] and has compulsory education for a duration of 11 years, exclusively for children aged 7 to 17\u201318.[519] It grants free education to its citizens by constitution.[520] The Ministry of Education of Russia is responsible for primary and secondary education, as well as vocational education; while the Ministry of Education and Science of Russia is responsible for science and higher education.[519] Regional authorities regulate education within their jurisdictions within the prevailing framework of federal laws. Russia is among the world's most educated countries, and has the sixth-highest proportion of tertiary-level graduates in terms of percentage of population, at 62.1%.[521] It spent roughly 4.7% of its GDP on education in 2018.[522]\n Russia's pre-school education system is highly developed and optional,[523] some four-fifths of children aged 3 to 6 attend day nurseries or kindergartens. Primary school is compulsory for eleven years, starting from age 6 to 7, and leads to a basic general education certificate.[519] An additional two or three years of schooling are required for the secondary-level certificate, and some seven-eighths of Russians continue their education past this level.[524]\n Admission to an institute of higher education is selective and highly competitive:[520] first-degree courses usually take five years.[524] The oldest and largest universities in Russia are Moscow State University and Saint Petersburg State University.[525] There are ten highly prestigious federal universities across the country. Russia was the world's fifth-leading destination for international students in 2019, hosting roughly 300 thousand.[526]\n Russia, by constitution, guarantees free, universal health care for all Russian citizens, through a compulsory state health insurance programme.[528] The Ministry of Health of the Russian Federation oversees the Russian public healthcare system, and the sector employs more than two million people. Federal regions also have their own departments of health that oversee local administration. A separate private health insurance plan is needed to access private healthcare in Russia.[529]\n Russia spent 5.65% of its GDP on healthcare in 2019.[530] Its healthcare expenditure is notably lower than other developed nations.[531] Russia has one of the world's most female-biased sex ratios, with 0.859 males to every female,[9] due to its high male mortality rate.[532] In 2021, the overall life expectancy in Russia at birth was 70.06 years (65.51 years for males and 74.51 years for females),[533] and it had a very low infant mortality rate (5 per 1,000 live births).[534]\n The principal cause of death in Russia are cardiovascular diseases.[535] Obesity is a prevalent health issue in Russia; most adults are overweight or obese.[536] However, Russia's historically high alcohol consumption rate is the biggest health issue in the country,[537] as it remains one of the world's highest, despite a stark decrease in the last decade.[538] Smoking is another health issue in the country.[539] The country's high suicide rate, although on the decline,[540] remains a significant social issue.[541]\n Russian writers and philosophers have played an important role in the development of European literature and thought.[542][543] The Russians have also greatly influenced classical music,[544] ballet,[545] sport,[546] painting,[547] and cinema.[548] The nation has also made pioneering contributions to science and technology and space exploration.[549][550]\n Russia is home to 32 UNESCO World Heritage Sites, 21 out of which are cultural; while 31 more sites lie on the tentative list.[551] The large global Russian diaspora has also played a major role in spreading Russian culture throughout the world. Russia's national symbol, the double-headed eagle, dates back to the Tsardom period, and is featured in its coat of arms and heraldry.[73] The Russian Bear and Mother Russia are often used as national personifications of the country.[552][553] Matryoshka dolls are considered a cultural icon of Russia.[554]\n Russia has eight\u2014public, patriotic, and religious\u2014official holidays.[555] The year starts with New Year's Day on 1 January, soon followed by Russian Orthodox Christmas on 7 January; the two are the country's most popular holidays.[556] Defender of the Fatherland Day, dedicated to men, is celebrated on 23 February.[557] International Women's Day on 8 March, gained momentum in Russia during the Soviet era. The annual celebration of women has become so popular, especially among Russian men, that Moscow's flower vendors often see profits of \"15 times\" more than other holidays.[558] Spring and Labour Day, originally a Soviet era holiday dedicated to workers, is celebrated on 1 May.[559]\n Victory Day, which honours Soviet victory over Nazi Germany and the End of World War II in Europe, is celebrated on 9 May as an annual large parade in Moscow's Red Square;[560] and marks the famous Immortal Regiment civil event.[561] Other patriotic holidays include Russia Day on 12 June, celebrated to commemorate Russia's declaration of sovereignty from the collapsing Soviet Union;[562] and Unity Day on 4 November, commemorating the 1612 uprising which marked the end of the Polish occupation of Moscow.[563]\n There are many popular non-public holidays. Old New Year is celebrated on 14 January.[564] Maslenitsa is an ancient and popular East Slavic folk holiday.[565] Cosmonautics Day on 12 April, in tribute to the first human trip into space.[566] Two major Christian holidays are Easter and Trinity Sunday.[567]\n Early Russian painting is represented in icons and vibrant frescos. In the early 15th century, the master icon painter Andrei Rublev created some of Russia's most treasured religious art.[568] The Russian Academy of Arts, which was established in 1757, to train Russian artists, brought Western techniques of secular painting to Russia.[89] In the 18th century, academicians Ivan Argunov, Dmitry Levitzky, Vladimir Borovikovsky became influential.[569] The early 19th century saw many prominent paintings by Karl Briullov and Alexander Ivanov, both of whom were known for Romantic historical canvases.[570][571] Ivan Aivazovsky, another Romantic painter, is considered one of the greatest masters of marine art.[572]\n In the 1860s, a group of critical realists (Peredvizhniki), led by Ivan Kramskoy, Ilya Repin and Vasiliy Perov broke with the academy, and portrayed the many-sided aspects of social life in paintings.[573] The turn of the 20th century saw the rise of symbolism; represented by Mikhail Vrubel and Nicholas Roerich.[574][575] The Russian avant-garde flourished from approximately 1890 to 1930; and globally influential artists from this era were El Lissitzky,[576] Kazimir Malevich, Natalia Goncharova, Wassily Kandinsky, and Marc Chagall.[577]\n The history of Russian architecture begins with early woodcraft buildings of ancient Slavs, and the church architecture of Kievan Rus'.[578] Following the Christianization of Kievan Rus', for several centuries it was influenced predominantly by Byzantine architecture.[579] Aristotle Fioravanti and other Italian architects brought Renaissance trends into Russia.[580] The 16th century saw the development of the unique tent-like churches; and the onion dome design, which is a distinctive feature of Russian architecture.[581] In the 17th century, the \"fiery style\" of ornamentation flourished in Moscow and Yaroslavl, gradually paving the way for the Naryshkin baroque of the 1680s.[582]\n After the reforms of Peter the Great, Russia's architecture became influenced by Western European styles. The 18th-century taste for Rococo architecture led to the works of Bartolomeo Rastrelli and his followers. The most influential Russian architects of the eighteenth century; Vasily Bazhenov, Matvey Kazakov, and Ivan Starov, created lasting monuments in Moscow and Saint Petersburg and established a base for the more Russian forms that followed.[568] During the reign of Catherine the Great, Saint Petersburg was transformed into an outdoor museum of Neoclassical architecture.[583] Under Alexander I, Empire style became the de facto architectural style.[584] The second half of the 19th century was dominated by the Neo-Byzantine and Russian Revival style.[585] In the early 20th century, Russian neoclassical revival became a trend.[586] Prevalent styles of the late 20th century were Art Nouveau,[587] Constructivism,[588] and Socialist Classicism.[589]\n Until the 18th century, music in Russia consisted mainly of church music and folk songs and dances.[590] In the 19th century, it was defined by the tension between classical composer Mikhail Glinka along with other members of The Mighty Handful, who were later succeeded by the Belyayev circle,[591] and the Russian Musical Society led by composers Anton and Nikolay Rubinstein.[592] The later tradition of Pyotr Ilyich Tchaikovsky, one of the greatest composers of the Romantic era, was continued into the 20th century by Sergei Rachmaninoff. World-renowned composers of the 20th century include Alexander Scriabin, Alexander Glazunov,[590] Igor Stravinsky, Sergei Prokofiev and Dmitri Shostakovich, and later Edison Denisov, Sofia Gubaidulina,[593] Georgy Sviridov,[594] and Alfred Schnittke.[593]\n During the Soviet era, popular music also produced a number of renowned figures, such as the two balladeers\u2014Vladimir Vysotsky and Bulat Okudzhava,[593] and performers such as Alla Pugacheva.[595] Jazz, even with sanctions from Soviet authorities, flourished and evolved into one of the country's most popular musical forms.[593] By the 1980s, rock music became popular across Russia, and produced bands such as Aria, Aquarium,[596] DDT,[597] and Kino;[598] the latter's leader Viktor Tsoi, was in particular, a gigantic figure.[599] Pop music has continued to flourish in Russia since the 1960s, with globally famous acts such as t.A.T.u.[600]\n Russian literature is considered to be among the world's most influential and developed.[542] It can be traced to the Middle Ages, when epics and chronicles in Old East Slavic were composed.[603] By the Age of Enlightenment, literature had grown in importance, with works from Mikhail Lomonosov, Denis Fonvizin, Gavrila Derzhavin, and Nikolay Karamzin.[604] From the early 1830s, during the Golden Age of Russian Poetry, literature underwent an astounding golden age in poetry, prose and drama.[605] Romanticism permitted a flowering of poetic talent: Vasily Zhukovsky and later his prot\u00e9g\u00e9 Alexander Pushkin came to the fore.[606] Following Pushkin's footsteps, a new generation of poets were born, including Mikhail Lermontov, Nikolay Nekrasov, Aleksey Konstantinovich Tolstoy, Fyodor Tyutchev and Afanasy Fet.[604]\n The first great Russian novelist was Nikolai Gogol.[607] Then came Ivan Turgenev, who mastered both short stories and novels.[608] Fyodor Dostoevsky and Leo Tolstoy soon became internationally renowned. Mikhail Saltykov-Shchedrin wrote prose satire,[609] while Nikolai Leskov is best remembered for his shorter fiction.[610] In the second half of the century Anton Chekhov excelled in short stories and became a leading dramatist.[611] Other important 19th-century developments included the fabulist Ivan Krylov,[612] non-fiction writers such as the critic Vissarion Belinsky,[613] and playwrights such as Aleksandr Griboyedov and Aleksandr Ostrovsky.[614][615] The beginning of the 20th century ranks as the Silver Age of Russian Poetry. This era had poets such as Alexander Blok, Anna Akhmatova, Boris Pasternak, and Konstantin Balmont.[616] It also produced some first-rate novelists and short-story writers, such as Aleksandr Kuprin, Nobel Prize winner Ivan Bunin, Leonid Andreyev, Yevgeny Zamyatin, Dmitry Merezhkovsky and Andrei Bely.[604]\n After the Russian Revolution of 1917, Russian literature split into Soviet and white \u00e9migr\u00e9 parts. In the 1930s, Socialist realism became the predominant trend in Russia. Its leading figure was Maxim Gorky, who laid the foundations of this style.[617] Mikhail Bulgakov was one of the leading writers of the Soviet era.[618] Nikolay Ostrovsky's novel How the Steel Was Tempered has been among the most successful works of Russian literature. Influential \u00e9migr\u00e9 writers include Vladimir Nabokov,[619] and Isaac Asimov; who was considered one of the \"Big Three\" science fiction writers.[620] Some writers dared to oppose Soviet ideology, such as Nobel Prize-winning novelist Aleksandr Solzhenitsyn, who wrote about life in the Gulag camps.[621]\n Russian philosophy has been greatly influential. Alexander Herzen is known as one of the fathers of agrarian populism.[622] Mikhail Bakunin is referred to as the father of anarchism.[623] Peter Kropotkin was the most important theorist of anarcho-communism.[624] Mikhail Bakhtin's writings have significantly inspired scholars.[625] Helena Blavatsky gained international following as the leading theoretician of Theosophy, and co-founded the Theosophical Society.[626] Vladimir Lenin, a major revolutionary, developed a variant of communism known as Leninism.[627] Leon Trotsky, on the other hand, founded Trotskyism.[628] Alexander Zinoviev was a prominent philosopher in the second half of the 20th century.[629] Aleksandr Dugin, known for his fascist views, has been regarded as the \"guru of geopolitics\".[630]\n Russian cuisine has been formed by climate, cultural and religious traditions, and the vast geography of the nation; and it shares similarities with the cuisines of its neighbouring countries. Crops of rye, wheat, barley, and millet provide the ingredients for various breads, pancakes and cereals, as well as for many drinks. Bread, of many varieties,[631] is very popular across Russia.[632] Flavourful soups and stews include shchi, borsch, ukha, solyanka, and okroshka. Smetana (a heavy sour cream) and mayonnaise are often added to soups and salads.[633][634] Pirozhki,[635] blini,[636] and syrniki are native types of pancakes.[637] Beef Stroganoff,[638]:\u200a266\u200a Chicken Kiev,[638]:\u200a320\u200a pelmeni,[639] and shashlyk are popular meat dishes.[640] Other meat dishes include stuffed cabbage rolls (golubtsy) usually filled with meat.[641] Salads include Olivier salad,[642] vinegret,[643] and dressed herring.[644]\n Russia's national non-alcoholic drink is kvass,[645] and the national alcoholic drink is vodka; its production in Russia (and elsewhere) dates back to the 14th century.[646] The country has the world's highest vodka consumption,[647] while beer is the most popular alcoholic beverage.[648] Wine has become increasingly popular in Russia in the 21st century.[649] Tea has been popular in Russia for centuries.[650]\n There are 400 news agencies in Russia, among which the largest internationally operating are TASS, RIA Novosti, Sputnik, and Interfax.[652] Television is the most popular medium in Russia.[653] Among the 3,000 licensed radio stations nationwide, notable ones include Radio Rossii, Vesti FM, Echo of Moscow, Radio Mayak, and Russkoye Radio. Of the 16,000 registered newspapers, Argumenty i Fakty, Komsomolskaya Pravda, Rossiyskaya Gazeta, Izvestia, and Moskovskij Komsomolets are popular. State-run Channel One and Russia-1 are the leading news channels, while RT is the flagship of Russia's international media operations.[653] Russia has the largest video gaming market in Europe, with over 65 million players nationwide.[654]\n Russian and later Soviet cinema was a hotbed of invention, resulting in world-renowned films such as The Battleship Potemkin, which was named the greatest film of all time at the Brussels World's Fair in 1958.[655][656] Soviet-era filmmakers, most notably Sergei Eisenstein and Andrei Tarkovsky, would go on to become among of the world's most innovative and influential directors.[657][658] Eisenstein was a student of Lev Kuleshov, who developed the groundbreaking Soviet montage theory of film editing at the world's first film school, the All-Union Institute of Cinematography.[659] Dziga Vertov's \"Kino-Eye\" theory had a large effect on the development of documentary filmmaking and cinema realism.[660] Many Soviet socialist realism films were artistically successful, including Chapaev, The Cranes Are Flying, and Ballad of a Soldier.[548]\n The 1960s and 1970s saw a greater variety of artistic styles in Soviet cinema.[548] The comedies of Eldar Ryazanov and Leonid Gaidai of that time were immensely popular, with many of the catchphrases still in use today.[661][662] In 1961\u201368 Sergey Bondarchuk directed an Oscar-winning film adaptation of Leo Tolstoy's epic War and Peace, which was the most expensive film made in the Soviet Union.[548] In 1969, Vladimir Motyl's White Sun of the Desert was released, a very popular film in a genre of ostern; the film is traditionally watched by cosmonauts before any trip into space.[663] After the dissolution of the Soviet Union, the Russian cinema industry suffered large losses\u2014however, since the late 2000s, it has seen growth once again, and continues to expand.[664]\n Football is the most popular sport in Russia.[666] The Soviet Union national football team became the first European champions by winning Euro 1960,[667] and reached the finals of Euro 1988.[668] Russian clubs CSKA Moscow and Zenit Saint Petersburg won the UEFA Cup in 2005 and 2008.[669][670] The Russian national football team reached the semi-finals of Euro 2008.[671] Russia was the host nation for the 2017 FIFA Confederations Cup,[672] and the 2018 FIFA World Cup.[673] However, Russian teams are currently suspended from FIFA and UEFA competitions.[674]\n Ice hockey is very popular in Russia, and the Soviet national ice hockey team dominated the sport internationally throughout its existence.[546] Bandy is Russia's national sport, and it has historically been the highest-achieving country in the sport.[675] The Russian national basketball team won the EuroBasket 2007,[676] and the Russian basketball club PBC CSKA Moscow is among the most successful European basketball teams.[677] The annual Formula One Russian Grand Prix was held at the Sochi Autodrom in the Sochi Olympic Park, until its termination following the Russian invasion of Ukraine in 2022.[678][679]\n Historically, Russian athletes have been one of the most successful contenders in the Olympic Games.[546] Russia is the leading nation in rhythmic gymnastics; and Russian synchronised swimming is considered to be the world's best.[680] Figure skating is another popular sport in Russia, especially pair skating and ice dancing.[681] Russia has produced numerous prominent tennis players.[682] Chess is also a widely popular pastime in the nation, with many of the world's top chess players being Russian for decades.[683] The 1980 Summer Olympic Games were held in Moscow,[684] and the 2014 Winter Olympics and the 2014 Winter Paralympics were hosted in Sochi.[685][686] However, Russia has also had 43 Olympic medals stripped from its athletes due to doping violations, which is the most of any country, and nearly a third of the global total.[687]\n Government\n General information\n Other\n 66\u00b0N 94\u00b0E\ufeff / \ufeff66\u00b0N 94\u00b0E\ufeff / 66; 94\n"
    },
    {
        "title": "Pinch hitter",
        "url": "https://en.wikipedia.org/wiki/Pinch_hitter",
        "content": "In baseball, a pinch hitter (PH) is a substitute batter. Batters can be substituted at any time while the ball is dead (not in active play); the manager may use any player who has not yet entered the game as a substitute. Unlike basketball, American football or ice hockey, and in a similar way to association football, baseball does not have a free substitution rule (at the professional level) and thus the replaced player is not allowed back into that game. The pinch hitter assumes the spot in the batting order of the player whom he replaces. Pinch hitters are commonly used to replace a weak hitter (often the pitcher) or to gain a platoon advantage.\n The player chosen to be a pinch hitter is often a backup infielder or outfielder whose defensive skills are limited. In Major League Baseball (MLB), catchers are less likely to be called upon to pinch-hit, because most teams have only two catchers. Pitchers are rarely used as pinch hitters, because they tend to be worse hitters than other players on the team. However, some pitchers have been used as pinch hitters; this tactic had almost vanished by the 1980s, but later saw a comeback in situations when benches have diminished due to injuries, offering few other options beyond a team's 12 or 13 pitchers.\n MLB, the Pacific League of Nippon Professional Baseball (NPB), the KBO League (in South Korea), the Liga Mexicana de B\u00e9isbol (in Mexico), and various other leagues use the designated hitter rule, such that pitchers seldom bat. This eliminates one possible situation in which a pinch hitter may be more desirable.\n For statistical and scorekeeping purposes, the pinch hitter is denoted by \"PH\".[1]\n Pinch hitters are often used to replace a starting player because of injury or when the pinch hitter is thought to have a better chance of reaching base or helping other runners to score.\n When the designated hitter rule is not in effect (e.g., in the Central League in NPB, in the National League before 2022 and American League before 1973 in MLB and leagues such as the Atlantic League which use the double hook rule), pinch hitters are often substituted for the pitcher in the middle or late innings of a game. This is because pitchers are often poor hitters and may become less effective after six to seven innings of pitching. Thus, as the manager often plans to replace the pitcher in the next inning, the player being replaced cannot re-enter the game, the major downside of using a pinch hitter.\n This use of a pinch hitter is often part of a double switch, in which a relief pitcher replaces a defensive player who will not bat soon, and at the same time a defensive player replaces the pitcher who is scheduled to bat soon. If a player acts as a pinch hitter and his team bats around in the inning, he may come to the plate a second time. The second (and subsequent) times he bats in the inning are not considered pinch-hitting appearances.\n The pinch hitter need not (but may) assume the same position as the player for whom he pinch-hits as long as some other player assumes that position. For example, on August 16, 2009, the Washington Nationals' Ryan Zimmerman pinch-hit for second baseman Alberto Gonz\u00e1lez and then remained in the game at third base, with previous third baseman Ronnie Belliard switching positions to play second base after the change.[2] Alternatively, the manager may designate another player to replace the pinch hitter; this scenario is common when a team pinch-hits for a pitcher without executing a double switch, such that the new pitcher then replaces the pinch hitter and assumes the previous pitcher's place in the batting order.\n If a pinch hitter hits for the DH, the new pinch hitter stays in the game as a DH, and may not be used in the field. If the new DH does take the field, then the team forfeits the DH for the remainder of the game (thus, causing the pitcher to enter the batting order).\n This is a list of players with the most pinch hits in Major League Baseball history. Names which appear in bold are active players. Includes games through July 22, 2011.\n"
    },
    {
        "title": "Prisoner",
        "url": "https://en.wikipedia.org/wiki/Prisoner",
        "content": "A prisoner (also known as an inmate or detainee) is a person who is deprived of liberty against their will. This can be by confinement or captivity in a prison, or physical restraint. The term usually applies to one serving a sentence in prison.[1]\n \"Prisoner\" is a legal term for a person who is imprisoned.[3]\n In section 1 of the Prison Security Act 1992, the word \"prisoner\" means any person for the time being in a prison as a result of any requirement imposed by a court or otherwise that he be detained in legal custody.[4]\n \"Prisoner\" was a legal term for a person prosecuted for felony. It was not applicable to a person prosecuted for misdemeanour.[5] The abolition of the distinction between felony and misdemeanour by section 1 of the Criminal Law Act 1967 has rendered this distinction obsolete.\n Glanville Williams described as \"invidious\" the practice of using the term \"prisoner\" in reference to a person who had not been convicted.[6]\n The earliest evidence of the existence of the prisoner dates back to 8,000 BC from prehistoric graves in Lower Egypt.[citation needed] This evidence suggests that people from Libya enslaved a San-like tribe.[7][failed verification][8]\n Some of the most extreme adverse effects suffered by prisoners appear to be caused by solitary confinement for long durations. When held in \"Special Housing Units\" (SHU), prisoners are subject to sensory deprivation and lack of social contact that can have a severe negative impact on their mental health.\n A psychopathological condition identified as \"SHU syndrome\" has been observed among such prisoners. Symptoms are characterized as problems with concentration and memory, distortions of perception, and hallucinations. Most convicts suffering from SHU syndrome exhibit extreme generalized anxiety and panic disorder, with some suffering amnesia.[9]\n The State-Trait Anxiety Inventory (STAI) was developed to understand the mechanisms behind anxiety. State anxiety describes anxiety that takes place in a stressful situation while trait anxiety is the tendency of feeling anxious in many situations because of a set of beliefs that an individual has that threatens their well-being.[10]\n SHU syndrome is a term that was created by Psychiatrist Stuart Grassian to describe the six basic mechanisms that happen in a cognitive matter in prisoners that are in solitary confinements or supermax level cell prison. The six basic mechanisms that occur together are:\n Stuart Grassian proposed that the symptoms are unique and are not found in any other situation.[11]\n Long durations may lead to depression and changes in brain physiology. In the absence of a social context that is needed to validate perceptions of their environment, prisoners become highly malleable, abnormally sensitive, and exhibit increased vulnerability to the influence of those controlling their environment. Social connection and the support provided by social interaction are prerequisites to long-term social adjustment as a prisoner.\n Prisoners exhibit the paradoxical effect of social withdrawal after long periods of solitary confinement. A shift takes place from a craving for greater social contact to a fear of it. They may grow lethargic and apathetic, and no longer be able to control their own conduct when released from solitary confinement. They can come to depend upon the prison structure to control and limit their conduct.\n Long-term stays in solitary confinement can cause prisoners to develop clinical depression, and long-term impulse control disorder. Those with pre-existing mental illnesses are at a higher risk for developing psychiatric symptoms.[12] Some common behaviours are self-mutilation, suicidal tendencies, and psychosis.[9]\n The psychological syndrome known as Stockholm syndrome describes a paradoxical phenomenon where, over time, hostages develop positive feelings towards their captors.[13] The victim's ego develops a series of defense mechanisms to achieve survival and cope with stress in a traumatic situation.[14]\n The founding of ethnographic prison sociology as a discipline, from which most of the meaningful knowledge of prison life and culture stems, is commonly credited to the publication of two key texts:[15] Donald Clemmer's The Prison Community,[16] which was first published in 1940 and republished in 1958; and Gresham Sykes classic study The Society of Captives,[17] which was also published in 1958. Clemmer's text, based on his study of 2,400 convicts over three years at the Menard Correctional Center where he worked as a clinical sociologist,[18] propagated the notion of the existence of a distinct inmate culture and society with values and norms antithetical to both the prison authority and the wider society.\n In this world, for Clemmer, these values, formalized as the \"inmate code\", provided behavioural precepts that unified prisoners and fostered antagonism to prison officers and the prison institution as a whole. The process whereby inmates acquired this set of values and behavioural guidelines as they adapted to prison life he termed \"prisonization\", which he defined as the \"taking on, in a greater or lesser degree, the folkways, mores, customs and general culture of the penitentiary\".[19] However, while Clemmer argued that all prisoners experienced some degree of \"prisonization\" this was not a uniform process and factors such as the extent to which a prisoner involved himself in primary group relations in the prison and the degree to which he identified with the external society all had a considerable impact.[20]\n \"Prisonization\" as the inculcation of a convict culture was defined by identification with primary groups in prison, the use of prison slang and argot,[21] the adoption of specified rituals and a hostility to prison authority in contrast to inmate solidarity and was asserted by Clemmer to create individuals who were acculturated into a criminal and deviant way of life that stymied all attempts to reform their behaviour.[22]\n Opposed to these theories, several European sociologists[23] have shown that inmates were often fragmented and the links they have with society are often stronger than those forged in prison, particularly through the action of work on time perception[24]\n The convict code was theorized as a set of tacit behavioural norms which exercised a pervasive impact on the conduct of prisoners. Competency in following the routines demanded by the code partly determined the inmate's identity as a convict.[25] As a set of values and behavioural guidelines, the convict code referred to the behaviour of inmates in antagonising staff members and to the mutual solidarity between inmates as well as the tendency to the non-disclosure to prison authorities of prisoner activities and to resistance to rehabilitation programmer.[26] Thus, it was seen as providing an expression and form of communal resistance and allowed for the psychological survival of the individual under extremely repressive and regimented systems of carceral control.[27]\n Sykes outlined some of the most salient points of this code as it applied in the post-war period in the United States:\n Both federal and state laws govern the rights of prisoners. Prisoners in the United States do not have full rights under the Constitution, however, they are protected by the Eighth Amendment which prohibits cruel and unusual punishment.[29] However, the mass incarcerations in the United States prisons raise concerns about the 8th Amendment being overridden by these conditions.[30]\n Growing research associates education with a number of positive outcomes for prisoners, the institution, and society. Although at the time of the ban's enactment there was limited knowledge about the relationship between education and recidivism, there is growing merit to idea that education in prison is a preventative to re-incarceration. Several studies help illustrate the point. For example, one study in 1997 that focused on 3,200 prisoners in Maryland, Minnesota, and Ohio, showed that prison education reduced the likelihood of re-incarceration by 29 percent. In 2000, the Texas Department of Education conducted a longitudinal study of 883 men and women who earned college degrees while incarcerated, finding recidivism rates between 27.2 percent (completion of an AA degree) and 7.8 percent (completion of a BA degree), compared to a system-wide recidivism rate between 40 and 43 percent.10 One report, sponsored by the Correctional Education Association, focused on recidivism in three states, concluding that education prevented crime. More recently, a 2013 Department of Justice funded study from the RAND Corporation found that incarcerated individuals who participated in correctional education were 43% less likely to return to prison within 3 years than prisoners who did not participate in such programmes. The research implies that education has the potential to impact recidivism rates positively by lowering them.[31]\n Other types of prisoners can include those under police custody, house arrest, those in psychiatric institutions, internment camps, and peoples restricted to a specific area.\n"
    },
    {
        "title": "Paragon Cable",
        "url": "https://en.wikipedia.org/wiki/Paragon_Cable",
        "content": "Paragon Cable was a cable system based in Minneapolis, Minnesota and was owned by Houston Industries.\n Formed in the 1980s, Paragon Cable was the largest cable provider in Minnesota with 177,100 subscribers in the Twin Cities and South Central Minnesota. In the latter years, it has expanded to serve other states, such as California, Oregon, Florida, Texas and in the Borough of Manhattan in New York City above west 79 Street and East 86 Streets (absorbing the former TelePrompter/Group W Cable franchise area).\n In 1995, Houston Industries, Time Warner, and KBLCOM, Inc. inked an agreement, in which Time Warner Cable would acquire all assets of Paragon Cable and the entire stock of KBLCOM.[1] The deal was finalized in 1999 and all of the Paragon assets were transferred into Time Warner's portfolio, however some headends (like one in Irving, Texas) kept Paragon Cable's verbal ident until late 2000. As for the assets in Florida, they were spun off in 2002 to a new company, Bright House Networks. Some former Paragon systems, such as in Oregon and Texas, would later be sold or swapped to AT&T Broadband, who in turn sold it to Comcast in 2003. Coincidentally, the North Texas cluster formerly served by Comcast was swapped back to Time Warner Cable in August 2006; the Minnesota cluster was transferred to Comcast as part of this transaction.\n In 2016, Time Warner Cable and Bright House Networks were acquired by Charter Communications, and was absorbed into the Spectrum brand.\n"
    },
    {
        "title": "Education",
        "url": "https://en.wikipedia.org/wiki/Education",
        "content": "\n Education is the transmission of knowledge, skills, and character traits and manifests in various forms. Formal education occurs within a structured institutional framework, such as public schools, following a curriculum. Non-formal education also follows a structured approach but occurs outside the formal schooling system, while informal education entails unstructured learning through daily experiences. Formal and non-formal education are categorized into levels, including early childhood education, primary education, secondary education, and tertiary education. Other classifications focus on teaching methods, such as teacher-centered and student-centered education, and on subjects, such as science education, language education, and physical education. Additionally, the term \"education\" can denote the mental states and qualities of educated individuals and the academic field studying educational phenomena.\n The precise definition of education is disputed, and there are disagreements about the aims of education and the extent to which education differs from indoctrination by fostering critical thinking. These disagreements impact how to identify, measure, and enhance various forms of education. Essentially, education socializes children into society by instilling cultural values and norms, equipping them with the skills necessary to become productive members of society. In doing so, it stimulates economic growth and raises awareness of local and global problems. Organized institutions play a significant role in education. For instance, governments establish education policies to determine the timing of school classes, the curriculum, and attendance requirements. International organizations, such as UNESCO, have been influential in promoting primary education for all children.\n Many factors influence the success of education. Psychological factors include motivation, intelligence, and personality. Social factors, such as socioeconomic status, ethnicity, and gender, are often associated with discrimination. Other factors encompass access to educational technology, teacher quality, and parental involvement.\n The primary academic field examining education is known as education studies. It delves into the nature of education, its objectives, impacts, and methods for enhancement. Education studies encompasses various subfields, including philosophy, psychology, sociology, and economics of education. Additionally, it explores topics such as comparative education, pedagogy, and the history of education.\n In prehistory, education primarily occurred informally through oral communication and imitation. With the emergence of ancient civilizations, the invention of writing led to an expansion of knowledge, prompting a transition from informal to formal education. Initially, formal education was largely accessible to elites and religious groups. The advent of the printing press in the 15th century facilitated widespread access to books, thus increasing general literacy. In the 18th and 19th centuries, public education gained significance, paving the way for the global movement to provide primary education to all, free of charge, and compulsory up to a certain age. Presently, over 90% of primary-school-age children worldwide attend primary school.\n The term \"education\" originates from the Latin words educare, meaning \"to bring up,\" and educere, meaning \"to bring forth.\"[1] The definition of education has been explored by theorists from various fields.[2] Many agree that education is a purposeful activity aimed at achieving goals like the transmission of knowledge, skills, and character traits.[3] However, extensive debate surrounds its precise nature beyond these general features. One approach views education as a process occurring during events such as schooling, teaching, and learning.[4] Another perspective perceives education not as a process but as the mental states and dispositions of educated individuals resulting from this process.[5] Furthermore, the term may also refer to the academic field that studies the methods, processes, and social institutions involved in teaching and learning.[6] Having a clear understanding of the term is crucial when attempting to identify educational phenomena, measure educational success, and improve educational practices.[7]\n Some theorists provide precise definitions by identifying specific features exclusive to all forms of education. Education theorist R. S. Peters, for instance, outlines three essential features of education, including imparting knowledge and understanding to the student, ensuring the process is beneficial, and conducting it in a morally appropriate manner.[8] While such precise definitions often characterize the most typical forms of education effectively, they face criticism because less common types of education may occasionally fall outside their parameters.[9] Dealing with counterexamples not covered by precise definitions can be challenging, which is why some theorists prefer offering less exact definitions based on family resemblance instead. This approach suggests that all forms of education are similar to each other but need not share a set of essential features common to all.[10] Some education theorists, such as Keira Sewell and Stephen Newman, argue that the term \"education\" is context-dependent.[a][11]\n Evaluative or thick conceptions[b] of education assert that it is inherent in the nature of education to lead to some form of improvement. They contrast with thin conceptions, which offer a value-neutral explanation.[13] Some theorists provide a descriptive conception of education by observing how the term is commonly used in ordinary language. Prescriptive conceptions, on the other hand, define what constitutes good education or how education should be practiced.[14] Many thick and prescriptive conceptions view education as an endeavor that strives to achieve specific objectives,[15] which may encompass acquiring knowledge, learning to think rationally, and cultivating character traits such as kindness and honesty.[16]\n Various scholars emphasize the importance of critical thinking in distinguishing education from indoctrination.[17] They argue that indoctrination focuses solely on instilling beliefs in students, regardless of their rationality;[18] whereas education also encourages the rational ability to critically examine and question those beliefs.[19] However, it is not universally accepted that these two phenomena can be clearly distinguished, as some forms of indoctrination may be necessary in the early stages of education when the child's mind is not yet fully developed. This is particularly relevant in cases where young children must learn certain things without comprehending the underlying reasons, such as specific safety rules and hygiene practices.[20]\n Education can be characterized from both the teacher's and the student's perspectives. Teacher-centered definitions emphasize the perspective and role of the teacher in transmitting knowledge and skills in a morally appropriate manner.[21] On the other hand, student-centered definitions analyze education based on the student's involvement in the learning process, suggesting that this process transforms and enriches their subsequent experiences.[22] It's also possible to consider definitions that incorporate both perspectives. In this approach, education is seen as a process of shared experience, involving the discovery of a common world and the collaborative solving of problems.[23]\n There are several classifications of education. One classification depends on the institutional framework, distinguishing between formal, non-formal, and informal education. Another classification involves different levels of education based on factors such as the student's age and the complexity of the content. Further categories focus on the topic, teaching method, medium used, and funding.[24]\n The most common division is between formal, non-formal, and informal education.[25][c] Formal education occurs within a structured institutional framework, typically with a chronological and hierarchical order. The modern schooling system organizes classes based on the student's age and progress, ranging from primary school to university. Formal education is usually overseen and regulated by the government and often mandated up to a certain age.[27]\n Non-formal and informal education occur outside the formal schooling system, with non-formal education serving as a middle ground. Like formal education, non-formal education is organized, systematic, and pursued with a clear purpose, as seen in activities such as tutoring, fitness classes, and participation in the scouting movement.[28] Informal education, on the other hand, occurs in an unsystematic manner through daily experiences and exposure to the environment. Unlike formal and non-formal education, there is typically no designated authority figure responsible for teaching.[29] Informal education unfolds in various settings and situations throughout one's life, often spontaneously, such as children learning their first language from their parents or individuals mastering cooking skills by preparing a dish together.[30]\n Some theorists differentiate between the three types based on the learning environment: formal education occurs within schools, non-formal education takes place in settings not regularly frequented, such as museums, and informal education unfolds in the context of everyday routines.[31] Additionally, there are disparities in the source of motivation. Formal education tends to be propelled by extrinsic motivation, driven by external rewards. Conversely, in non-formal and informal education, intrinsic motivation, stemming from the enjoyment of the learning process, typically prevails.[32] While the differentiation among the three types is generally clear, certain forms of education may not neatly fit into a single category.[33]\n In primitive cultures, education predominantly occurred informally, with little distinction between educational activities and other daily endeavors. Instead, the entire environment served as a classroom, and adults commonly assumed the role of educators. However, informal education often proves insufficient for imparting large quantities of knowledge. To address this limitation, formal educational settings and trained instructors are typically necessary. This necessity contributed to the increasing significance of formal education throughout history. Over time, formal education led to a shift towards more abstract learning experiences and topics, distancing itself from daily life. There was a greater emphasis on understanding general principles and concepts rather than simply observing and imitating specific behaviors.[34]\n Types of education are often categorized into different levels or stages. One influential framework is the International Standard Classification of Education, maintained by the United Nations Educational, Scientific and Cultural Organization (UNESCO). This classification encompasses both formal and non-formal education and distinguishes levels based on factors such as the student's age, the duration of learning, and the complexity of the content covered. Additional criteria include entry requirements, teacher qualifications, and the intended outcome of successful completion. The levels are grouped into early childhood education (level 0), primary education (level 1), secondary education (levels 2\u20133), post-secondary non-tertiary education (level 4), and tertiary education (levels 5\u20138).[35]\n Early childhood education, also referred to as preschool education or nursery education, encompasses the period from birth until the commencement of primary school. It is designed to facilitate holistic child development, addressing physical, mental, and social aspects. Early childhood education is pivotal in fostering socialization and personality development, while also imparting fundamental skills in communication, learning, and problem-solving. Its overarching goal is to prepare children for the transition to primary education.[36] While preschool education is typically optional, in certain countries such as Brazil, it is mandatory starting from the age of four.[37]\n Primary (or elementary) education usually begins between the ages of five and seven and spans four to seven years. It has no additional entry requirements and aims to impart fundamental skills in reading, writing, and mathematics. Additionally, it provides essential knowledge in subjects such as history, geography, the sciences, music, and art. Another objective is to facilitate personal development.[38] Presently, primary education is compulsory in nearly all nations, with over 90% of primary-school-age children worldwide attending such schools.[39]\n Secondary education succeeds primary education and typically spans the ages of 12 to 18 years. It is normally divided into lower secondary education (such as middle school or junior high school) and upper secondary education (like high school, senior high school, or college, depending on the country). Lower secondary education usually requires the completion of primary school as its entry prerequisite. It aims to expand and deepen learning outcomes, with a greater focus on subject-specific curricula, and teachers often specialize in one or a few specific subjects. One of its goals is to acquaint students with fundamental theoretical concepts across various subjects, laying a strong foundation for lifelong learning. In certain instances, it may also incorporate rudimentary forms of vocational training.[40] Lower secondary education is compulsory in numerous countries across Central and East Asia, Europe, and the Americas. In some nations, it represents the final phase of compulsory education. However, mandatory lower secondary education is less common in Arab states, sub-Saharan Africa, and South and West Asia.[41]\n Upper secondary education typically commences around the age of 15, aiming to equip students with the necessary skills and knowledge for employment or tertiary education. Completion of lower secondary education is normally a prerequisite. The curriculum encompasses a broader range of subjects, often affording students the opportunity to select from various options. Attainment of a formal qualification, such as a high school diploma, is frequently linked to successful completion of upper secondary education.[42] Education beyond the secondary level may fall under the category of post-secondary non-tertiary education, which is akin to secondary education in complexity but places greater emphasis on vocational training to ready students for the workforce.[43]\n In some countries, tertiary education is synonymous with higher education, while in others, tertiary education encompasses a broader spectrum.[44] Tertiary education builds upon the foundation laid in secondary education but delves deeper into specific fields or subjects. Its culmination results in an academic degree. Tertiary education comprises four levels: short-cycle tertiary, bachelor's, master's, and doctoral education. These levels often form a hierarchical structure, with the attainment of earlier levels serving as a prerequisite for higher ones.[45] Short-cycle tertiary education concentrates on practical aspects, providing advanced vocational and professional training tailored to specialized professions.[46] Bachelor's level education, also known as undergraduate education, is typically longer than short-cycle tertiary education. It is commonly offered by universities and culminates in an intermediary academic credential known as a bachelor's degree.[47] Master's level education is more specialized than undergraduate education and often involves independent research, normally in the form of a master's thesis.[48] Doctoral level education leads to an advanced research qualification, usually a doctor's degree, such as a Doctor of Philosophy (PhD). It usually involves the submission of a substantial academic work, such as a dissertation. More advanced levels include post-doctoral studies and habilitation.[49]\n Successful completion of formal education typically leads to certification, a prerequisite for advancing to higher levels of education and entering certain professions. Undetected cheating during exams, such as utilizing a cheat sheet, poses a threat to this system by potentially certifying unqualified students.[50]\n In most countries, primary and secondary education is provided free of charge. However, there are significant global disparities in the cost of tertiary education. Some countries, such as Sweden, Finland, Poland, and Mexico, offer tertiary education for free or at a low cost. Conversely, in nations like the United States and Singapore, tertiary education often comes with high tuition fees, leading students to rely on substantial loans to finance their studies.[51] High education costs can pose a significant barrier for students in developing countries, as their families may struggle to cover school fees, purchase uniforms, and buy textbooks.[52]\n The academic literature explores various types of education, including traditional and alternative approaches. Traditional education encompasses long-standing and conventional schooling methods, characterized by teacher-centered instruction within a structured school environment. Regulations govern various aspects, such as the curriculum and class schedules.[53]\n Alternative education serves as an umbrella term for schooling methods that diverge from the conventional traditional approach. These variances might encompass differences in the learning environment, curriculum content, or the dynamics of the teacher-student relationship. Characteristics of alternative schooling include voluntary enrollment, relatively modest class and school sizes, and customized instruction, fostering a more inclusive and emotionally supportive environment. This category encompasses various forms, such as charter schools and specialized programs catering to challenging or exceptionally talented students, alongside homeschooling and unschooling. Alternative education incorporates diverse educational philosophies, including Montessori schools, Waldorf education, Round Square schools, Escuela Nueva schools, free schools, and democratic schools.[54] Alternative education encompasses indigenous education, which emphasizes the preservation and transmission of knowledge and skills rooted in indigenous heritage. This approach often employs traditional methods such as oral narration and storytelling.[55] Other forms of alternative schooling include gurukul schools in India,[56] madrasa schools in the Middle East,[57] and yeshivas in Jewish tradition.[58]\n Some distinctions revolve around the recipients of education. Categories based on the age of the learner are childhood education, adolescent education, adult education, and elderly education.[59] Categories based on the biological sex of students include single-sex education and mixed-sex education.[60] Special education is tailored to meet the unique needs of students with disabilities, addressing various impairments on intellectual, social, communicative, and physical levels. Its goal is to overcome the challenges posed by these impairments, providing affected students with access to an appropriate educational structure. In the broadest sense, special education also encompasses education for intellectually gifted children, who require adjusted curricula to reach their fullest potential.[61]\n Classifications based on the teaching method include teacher-centered education, where the teacher plays a central role in imparting information to students, and student-centered education, where students take on a more active and responsible role in shaping classroom activities.[62] In conscious education, learning and teaching occur with a clear purpose in mind. Unconscious education unfolds spontaneously without conscious planning or guidance.[63] This may occur, in part, through the influence of teachers' and adults' personalities, which can indirectly impact the development of students' personalities.[64] Evidence-based education employs scientific studies to determine the most effective educational methods. Its aim is to optimize the effectiveness of educational practices and policies by ensuring they are grounded in the best available empirical evidence. This encompasses evidence-based teaching, evidence-based learning, and school effectiveness research.[65]\n Autodidacticism, or self-education, occurs independently of teachers and institutions. Primarily observed in adult education, it offers the freedom to choose what and when to study, making it a potentially more fulfilling learning experience. However, the lack of structure and guidance may lead to aimless learning, while the absence of external feedback could result in autodidacts developing misconceptions and inaccurately assessing their learning progress.[66] Autodidacticism is closely associated with lifelong education, which entails continuous learning throughout one's life.[67]\n Categories of education based on the subject encompass science education, language education, art education, religious education, physical education, and sex education.[68] Special mediums such as radio or websites are utilized in distance education, including e-learning (use of computers), m-learning (use of mobile devices), and online education. Often, these take the form of open education, wherein courses and materials are accessible with minimal barriers, contrasting with traditional classroom or onsite education. However, not all forms of online education are open; for instance, some universities offer full online degree programs that are not part of open education initiatives.[69]\n State education, also known as public education,[d] is funded and controlled by the government and available to the general public. It typically does not require tuition fees and is therefore a form of free education. In contrast, private education is funded and managed by private institutions. Private schools often have a more selective admission process and offer paid education by charging tuition fees.[71] A more detailed classification focuses on the social institutions responsible for education, such as family, school, civil society, state, and church.[72]\n Compulsory education refers to education that individuals are legally mandated to receive, primarily affecting children who must attend school up to a certain age. This stands in contrast to voluntary education, which individuals pursue based on personal choice rather than legal obligation.[73]\n Education serves various roles in society, spanning social, economic, and personal domains. Socially, education establishes and maintains a stable society by imparting fundamental skills necessary for interacting with the environment and fulfilling individual needs and aspirations. In contemporary society, these skills encompass speaking, reading, writing, arithmetic, and proficiency in information and communications technology. Additionally, education facilitates socialization by instilling awareness of dominant social and cultural norms, shaping appropriate behavior across diverse contexts. It fosters social cohesion, stability, and peace, fostering productive engagement in daily activities. While socialization occurs throughout life, early childhood education holds particular significance. Moreover, education plays a pivotal role in democracies by enhancing civic participation through voting and organizing, while also promoting equal opportunities for all.[74]\n On an economic level, individuals become productive members of society through education, acquiring the technical and analytical skills necessary for their professions, as well as for producing goods and providing services to others. In early societies, there was minimal specialization, with children typically learning a broad range of skills essential for community functioning. However, modern societies are increasingly complex, with many professions requiring specialized training alongside general education. Consequently, only a relatively small number of individuals master certain professions. Additionally, skills and tendencies acquired for societal functioning may sometimes conflict, with their value dependent on context. For instance, fostering curiosity and questioning established teachings promotes critical thinking and innovation, while at times, obedience to authority is necessary to maintain social stability.[75]\n By facilitating individuals' integration into society, education fosters economic growth and diminishes poverty. It enables workers to enhance their skills, thereby improving the quality of goods and services produced, which ultimately fosters prosperity and enhances competitiveness.[77] Public education is widely regarded as a long-term investment that benefits society as a whole, with primary education showing particularly high rates of return.[78] Additionally, besides bolstering economic prosperity, education contributes to technological and scientific advancements, reduces unemployment, and promotes social equity.[79] Moreover, increased education is associated with lower birth rates, partly due to heightened awareness of family planning, expanded opportunities for women, and delayed marriage.[80]\n Education plays a pivotal role in equipping a country to adapt to changes and effectively confront new challenges. It raises awareness and contributes to addressing contemporary global issues, including climate change, sustainability, and the widening disparities between the rich and the poor.[81] By instilling in students an understanding of how their lives and actions impact others, education can inspire individuals to strive towards realizing a more sustainable and equitable world.[82] Thus, education not only serves to maintain societal norms but also acts as a catalyst for social development.[83] This extends to evolving economic circumstances, where technological advancements, notably increased automation, impose new demands on the workforce that education can help meet.[84] As circumstances evolve, skills and knowledge taught may become outdated, necessitating curriculum adjustments to include subjects like digital literacy, and promote proficiency in handling new technologies.[85] Moreover, education can embrace innovative forms such as massive open online courses to prepare individuals for emerging challenges and opportunities.[86]\n On a more individual level, education fosters personal development, encompassing learning new skills, honing talents, nurturing creativity, enhancing self-knowledge, and refining problem-solving and decision-making abilities.[87] Moreover, education contributes positively to health and well-being. Educated individuals are often better informed about health issues and adjust their behavior accordingly, benefit from stronger social support networks and coping strategies, and enjoy higher incomes, granting them access to superior healthcare services.[88] The social significance of education is underscored by the annual International Day of Education on January 24, established by the United Nations, which designated 1970 as the International Education Year.[89]\n Organized institutions play a pivotal role in multiple facets of education. Entities such as schools, universities, teacher training institutions, and ministries of education comprise the education sector. They interact not only with one another but also with various stakeholders, including parents, local communities, religious groups, non-governmental organizations, healthcare professionals, law enforcement agencies, media platforms, and political leaders. Numerous individuals are directly engaged in the education sector, such as students, teachers, school principals, as well as school nurses and curriculum developers.[90]\n Various aspects of formal education are regulated by the policies of governmental institutions. These policies determine at what age children need to attend school and at what times classes are held, as well as issues pertaining to the school environment, such as infrastructure. Regulations also cover the exact qualifications and requirements that teachers need to fulfill. An important aspect of education policy concerns the curriculum used for teaching at schools, colleges, and universities. A curriculum is a plan of instruction or a program of learning that guides students to achieve their educational goals. The topics are usually selected based on their importance and depend on the type of school. The goals of public school curricula are usually to offer a comprehensive and well-rounded education, while vocational training focuses more on specific practical skills within a field. The curricula also cover various aspects besides the topic to be discussed, such as the teaching method, the objectives to be reached, and the standards for assessing progress. By determining the curricula, governmental institutions have a strong impact on what knowledge and skills are transmitted to the students.[91] Examples of governmental institutions include the Ministry of Education in India,[92] the Department of Basic Education in South Africa,[93] and the Secretariat of Public Education in Mexico.[94]\n International organizations also play a pivotal role in education. For example, UNESCO is an intergovernmental organization that promotes education through various means. One of its activities is advocating for education policies, such as the treaty Convention on the Rights of the Child, which declares education as a fundamental human right for all children and young people. The Education for All initiative aimed to provide basic education to all children, adolescents, and adults by 2015, later succeeded by the Sustainable Development Goals initiative, particularly goal 4.[95] Related policies include the Convention against Discrimination in Education and the Futures of Education initiative.[96]\n Some influential organizations are non-governmental rather than intergovernmental. For instance, the International Association of Universities promotes collaboration and knowledge exchange among colleges and universities worldwide, while the International Baccalaureate offers international diploma programs.[97] Institutions like the Erasmus Programme facilitate student exchanges between countries,[98] while initiatives such as the Fulbright Program provide similar services for teachers.[99]\n Educational success, also referred to as student and academic achievement, pertains to the extent to which educational objectives are met, such as the acquisition of knowledge and skills by students. For practical purposes, it is often primarily measured in terms of official exam scores, but numerous additional indicators exist, including attendance rates, graduation rates, dropout rates, student attitudes, and post-school indicators such as later income and incarceration rates.[100] Several factors influence educational achievement, such as psychological factors related to the individual student, and sociological factors associated with the student's social environment. Additional factors encompass access to educational technology, teacher quality, and parental involvement. Many of these factors overlap and mutually influence each other.[101]\n On a psychological level, relevant factors include motivation, intelligence, and personality.[102] Motivation is the internal force propelling people to engage in learning.[103] Motivated students are more likely to interact with the content to be learned by participating in classroom activities like discussions, resulting in a deeper understanding of the subject. Motivation can also help students overcome difficulties and setbacks. An important distinction lies between intrinsic and extrinsic motivation. Intrinsically motivated students are driven by an interest in the subject and the learning experience itself. Extrinsically motivated students seek external rewards such as good grades and recognition from peers. Intrinsic motivation tends to be more beneficial, leading to increased creativity, engagement, and long-term commitment.[104] Educational psychologists aim to discover methods to increase motivation, such as encouraging healthy competition among students while maintaining a balance of positive and negative feedback through praise and constructive criticism.[105]\n Intelligence significantly influences individuals' responses to education. It is a cognitive trait associated with the capacity to learn from experience, comprehend, and apply knowledge and skills to solve problems. Individuals with higher scores in intelligence metrics typically perform better academically and pursue higher levels of education.[106] Intelligence is often closely associated with the concept of IQ, a standardized numerical measure assessing intelligence based on mathematical-logical and verbal abilities. However, it has been argued that intelligence encompasses various types beyond IQ. Psychologist Howard Gardner posited distinct forms of intelligence in domains such as mathematics, logic, spatial cognition, language, and music. Additional types of intelligence influence interpersonal and intrapersonal interactions. These intelligences are largely autonomous, meaning that an individual may excel in one type while performing less well in another.[107]\n According to proponents of learning style theory, the preferred method of acquiring knowledge and skills is another factor. They hold that students with an auditory learning style find it easy to comprehend spoken lectures and discussions, whereas visual learners benefit from information presented visually, such as in diagrams and videos. To facilitate efficient learning, it may be advantageous to incorporate a wide variety of learning modalities.[108] Learning styles have been criticized for ambiguous empirical evidence of student benefits and unreliability of student learning style assessment by teachers.[109]\n The learner's personality may also influence educational achievement. For instance, characteristics such as conscientiousness and openness to experience, identified in the Big Five personality traits, are associated with academic success.[110] Other mental factors include self-efficacy, self-esteem, and metacognitive abilities.[111]\n Sociological factors center not on the psychological attributes of learners but on their environment and societal position. These factors encompass socioeconomic status, ethnicity, cultural background, and gender, drawing significant interest from researchers due to their association with inequality and discrimination. Consequently, they play a pivotal role in policy-making endeavors aimed at mitigating their impact.[112]\n Socioeconomic status is influenced by factors beyond just income, including financial security, social status, social class, and various attributes related to quality of life. Low socioeconomic status impacts educational success in several ways. It correlates with slower cognitive development in language and memory, as well as higher dropout rates. Families with limited financial means may struggle to meet their children's basic nutritional needs, hindering their development. Additionally, they may lack resources to invest in educational materials such as stimulating toys, books, and computers. Financial constraints may also prevent attendance at prestigious schools, leading to enrollment in institutions located in economically disadvantaged areas. Such schools often face challenges such as teacher shortages and inadequate educational materials and facilities like libraries, resulting in lower teaching standards. Moreover, parents may be unable to afford private lessons for children falling behind academically. In some cases, students from economically disadvantaged backgrounds are compelled to drop out of school to contribute to family income. Limited access to information about higher education and challenges in securing and repaying student loans further exacerbate the situation. Low socioeconomic status is also associated with poorer physical and mental health, contributing to a cycle of social inequality that persists across generations.[113]\n Ethnic background correlates with cultural distinctions and language barriers, which can pose challenges for students in adapting to the school environment and comprehending classes. Moreover, explicit and implicit biases and discrimination against ethnic minorities further compound these difficulties. Such biases can impact students' self-esteem, motivation, and access to educational opportunities. For instance, teachers may harbor stereotypical perceptions, albeit not overtly racist, leading to differential grading of comparable performances based on a child's ethnicity.[114]\n Historically, gender has played a pivotal role in education as societal norms dictated distinct roles for men and women. Education traditionally favored men, who were tasked with providing for the family, while women were expected to manage households and care for children, often limiting their access to education. Although these disparities have improved in many modern societies, gender differences persist in education. This includes biases and stereotypes related to gender roles in various academic domains, notably in fields such as science, technology, engineering, and mathematics (STEM), which are often portrayed as male-dominated. Such perceptions can deter female students from pursuing these subjects.[115] In various instances, discrimination based on gender and social factors occurs openly as part of official educational policies, such as the severe restrictions imposed on female education by the Taliban in Afghanistan,[116] and the school segregation of migrants and locals in urban China under the hukou system.[117]\n One facet of several social factors is characterized by the expectations linked to stereotypes. These expectations operate externally, influenced by how others respond to individuals belonging to specific groups, and internally, shaped by how individuals internalize and conform to them. In this regard, these expectations can manifest as self-fulfilling prophecies by affecting the educational outcomes they predict. Such outcomes may be influenced by both positive and negative stereotypes.[118]\n Technology plays a crucial role in educational success. While educational technology is often linked with modern digital devices such as computers, its scope extends far beyond that. It encompasses a diverse array of resources and tools for learning, including traditional aids like books and worksheets, in addition to digital devices.[119]\n Educational technology can enhance learning in various ways. In the form of media, it often serves as the primary source of information in the classroom, allowing teachers to allocate their time and energy to other tasks such as lesson planning, student guidance, and performance assessment.[120] By presenting information using graphics, audio, and video instead of mere text, educational technology can also enhance comprehension. Interactive elements, such as educational games, further engage learners in the learning process. Moreover, technology facilitates the accessibility of educational materials to a wide audience, particularly through online resources, while also promoting collaboration among students and communication with teachers.[121] The integration of artificial intelligence in education holds promise for providing new learning experiences to students and supporting teachers in their work. However, it also introduces new risks related to data privacy, misinformation, and manipulation.[122] Various organizations advocate for student access to educational technologies, including initiatives such as the One Laptop per Child initiative, the African Library Project, and Pratham.[123]\n School infrastructure also plays a crucial role in educational success. It encompasses physical aspects such as the school's location, size, and available facilities and equipment. A healthy and safe environment, well-maintained classrooms, appropriate classroom furniture, as well as access to a library and a canteen, all contribute to fostering educational success.[124] Additionally, the quality of teachers significantly impacts student achievement. Skilled teachers possess the ability to motivate and inspire students, and tailor instructions to individual abilities and needs. Their skills depend on their own education, training, and teaching experience.[125] A meta-analysis by Engin Karada\u011f et al. concludes that, compared to other influences, factors related to the school and the teacher have the greatest impact on educational success.[126]\n Parent involvement also enhances achievement and can increase children's motivation and commitment when they know their parents are invested in their educational endeavors. This often results in heightened self-esteem, improved attendance rates, and more positive behavior at school. Parent involvement covers communication with teachers and other school staff to raise awareness of current issues and explore potential resolutions.[127] Other relevant factors, occasionally addressed in academic literature, encompass historical, political, demographic, religious, and legal aspects.[128]\n The primary field exploring education is known as education studies, also termed education sciences. It seeks to understand how knowledge is transmitted and acquired by examining various methods and forms of education. This discipline delves into the goals, impacts, and significance of education, along with the cultural, societal, governmental, and historical contexts that influence it.[130] Education theorists draw insights from various disciplines, including philosophy, psychology, sociology, economics, history, politics, and international relations. Consequently, some argue that education studies lacks the clear methodological and subject delineations found in disciplines like physics or history.[131] Education studies focuses on academic analysis and critical reflection and differs in this respect from teacher training programs, which show participants how to become effective teachers. Furthermore, it encompasses not only formal education but also explores all forms and facets of educational processes.[132]\n Various research methods are utilized to investigate educational phenomena, broadly categorized into quantitative, qualitative, and mixed-methods approaches. Quantitative research mirrors the methodologies of the natural sciences, employing precise numerical measurements to collect data from numerous observations and utilizing statistical tools for analysis. Its goal is to attain an objective and impartial understanding. Conversely, qualitative research typically involves a smaller sample size and seeks to gain a nuanced insight into subjective and personal factors, such as individuals' experiences within the educational process. Mixed-methods research aims to integrate data gathered from both approaches to achieve a balanced and comprehensive understanding. Data collection methods vary and may include direct observation, test scores, interviews, and questionnaires.[133] Research projects may investigate fundamental factors influencing all forms of education or focus on specific applications, seek solutions to particular problems, or evaluate the effectiveness of educational initiatives and policies.[134]\n Education studies encompasses various subfields such as pedagogy, educational research, comparative education, and the philosophy, psychology, sociology, economics, and history of education.[135] The philosophy of education is the branch of applied philosophy that examines many of the fundamental assumptions underlying the theory and practice of education. It explores education both as a process and a discipline while seeking to provide precise definitions of its nature and distinctions from other phenomena. Additionally, it delves into the purpose of education, its various types, and the conceptualization of teachers, students, and their relationship.[136] Furthermore, it encompasses educational ethics, which examines the moral implications of education, such as the ethical principles guiding it and how teachers should apply them to specific situations. The philosophy of education boasts a long history and was a subject of discourse in ancient Greek philosophy.[137]\n The term \"pedagogy\" is sometimes used interchangeably with education studies, but in a more specific sense, it refers to the subfield focused on teaching methods.[138] It investigates how educational objectives, such as knowledge transmission or the development of skills and character traits, can be achieved.[139] Pedagogy is concerned with the methods and techniques employed in teaching within conventional educational settings. While some definitions confine it to this context, in a broader sense, it encompasses all forms of education, including teaching methods beyond traditional school environments.[140] In this broader context, it explores how teachers can facilitate learning experiences for students to enhance their understanding of the subject matter and how learning itself occurs.[141]\n The psychology of education delves into the mental processes underlying learning, focusing on how individuals acquire new knowledge and skills and experience personal development. It investigates the various factors influencing educational outcomes, how these factors vary among individuals, and the extent to which nature or nurture contribute to these outcomes. Key psychological theories shaping education encompass behaviorism, cognitivism, and constructivism.[142] Related disciplines include educational neuroscience and the neurology of education, which explore the neuropsychological processes and changes associated with learning.[143]\n The field of sociology of education delves into how education shapes socialization, examining how social factors and ideologies influence access to education and individual success within it. It explores the impact of education on different societal groups and its role in shaping personal identity. Specifically, the sociology of education focuses on understanding the root causes of inequalities, offering insights relevant to education policy aimed at identifying and addressing factors contributing to inequality.[144] Two prominent perspectives within this field are consensus theory and conflict theory. Consensus theorists posit that education benefits society by preparing individuals for their societal roles, while conflict theorists view education as a tool employed by the ruling class to perpetuate inequalities.[145]\n The field of economics of education investigates the production, distribution, and consumption of education. It seeks to optimize resource allocation to enhance education, such as assessing the impact of increased teacher salaries on teacher quality. Additionally, it explores the effects of smaller class sizes and investments in new educational technologies. By providing insights into resource allocation, the economics of education aids policymakers in making decisions that maximize societal benefits. Furthermore, it examines the long-term economic implications of education, including its role in fostering a highly skilled workforce and enhancing national competitiveness. A related area of interest involves analyzing the economic advantages and disadvantages of different educational systems.[146]\n Comparative education is the discipline that examines and contrasts education systems. Comparisons can occur from a general perspective or focus on specific factors like social, political, or economic aspects. Often applied to different countries, comparative education assesses the similarities and differences of their educational institutions and practices, evaluating the consequences of distinct approaches. It can be used to glean insights from other countries on effective education policies and how one's own system may be improved.[147] This practice, known as policy borrowing, presents challenges as policy success can hinge on the social and cultural context of students and teachers. A related and contentious topic concerns whether the educational systems of developed countries are superior and should be exported to less developed ones.[148] Other key topics include the internationalization of education and the role of education in transitioning from authoritarian regimes to democracies.[149]\n The history of education delves into the evolution of educational practices, systems, and institutions. It explores various key processes, their potential causes and effects, and their interrelations.[150]\n A central topic in education studies revolves around how people should be educated and what goals should guide this process. Various aims have been proposed, including the acquisition of knowledge and skills, personal development, and the cultivation of character traits. Commonly suggested attributes encompass qualities like curiosity, creativity, rationality, and critical thinking, along with tendencies to think, feel, and act morally. Scholars diverge on whether to prioritize liberal values such as freedom, autonomy, and open-mindedness, or qualities like obedience to authority, ideological purity, piety, and religious faith.[153]\n Some education theorists concentrate on a single overarching purpose of education, viewing more specific aims as means to this end.[154] At a personal level, this purpose is often equated with assisting the student in leading a good life.[155] Societally, education aims to cultivate individuals into productive members of society.[156] There is debate regarding whether the primary aim of education is to benefit the educated individual or society as a whole.[157]\n Educational ideologies encompass systems of fundamental philosophical assumptions and principles utilized to interpret, understand, and assess existing educational practices and policies. They address various aspects beyond the aims of education, including the subjects taught, the structure of learning activities, the role of teachers, methods for assessing educational progress, and the design of institutional frameworks and policies. These ideologies are diverse and often interrelated. Teacher-centered ideologies prioritize the role of teachers in imparting knowledge to students, while student-centered ideologies afford students a more active role in the learning process. Process-based ideologies focus on the methods of teaching and learning, contrasting with product-based ideologies, which consider education in terms of the desired outcomes. Conservative ideologies uphold traditional practices, whereas Progressive ideologies advocate for innovation and creativity. Additional categories are humanism, romanticism, essentialism, encyclopaedism, pragmatism, as well as authoritarian and democratic ideologies.[158]\n Learning theories attempt to elucidate the mechanisms underlying learning. Influential theories include behaviorism, cognitivism, and constructivism. Behaviorism posits that learning entails a modification in behavior in response to environmental stimuli. This occurs through the presentation of a stimulus, the association of this stimulus with the desired response, and the reinforcement of this stimulus-response connection. Cognitivism views learning as a transformation in cognitive structures and emphasizes the mental processes involved in encoding, retrieving, and processing information. Constructivism asserts that learning is grounded in the individual's personal experiences and places greater emphasis on social interactions and their interpretation by the learner. These theories carry significant implications for instructional practices. For instance, behaviorists often emphasize repetitive drills, cognitivists may advocate for mnemonic techniques, and constructivists typically employ collaborative learning strategies.[159]\n Various theories suggest that learning is more effective when it is based on personal experience. Additionally, aiming for a deeper understanding by connecting new information to pre-existing knowledge is considered more beneficial than simply memorizing a list of unrelated facts.[160] An influential developmental theory of learning is proposed by psychologist Jean Piaget, who outlines four stages of learning through which children progress on their way to adulthood: the sensorimotor, pre-operational, concrete operational, and formal operational stages. These stages correspond to different levels of abstraction, with early stages focusing more on simple sensory and motor activities, while later stages involve more complex internal representations and information processing, such as logical reasoning.[161]\n The teaching method pertains to how the content is delivered by the teacher, such as whether group work is employed rather than focusing on individual learning. There is a wide array of teaching methods available, and the most effective one in a given scenario depends on factors like the subject matter and the learner's age and level of competence.[162] This is reflected in modern school systems, which organize students into different classes based on age, competence, specialization, and native language to ensure an effective learning process. Different subjects often employ distinct approaches; for example, language education frequently emphasizes verbal learning, while mathematical education focuses on abstract and symbolic thinking alongside deductive reasoning.[163] One crucial aspect of teaching methodologies is ensuring that learners remain motivated, either through intrinsic factors like interest and curiosity or through external rewards.[164]\n The teaching method also includes the utilization of instructional media, such as books, worksheets, and audio-visual recordings, as well as implementing some form of test or evaluation to gauge learning progress. Educational assessment is the process of documenting the student's knowledge and skills, which can happen formally or informally and may take place before, during, or after the learning activity. Another significant pedagogical element in many modern educational approaches is that each lesson is part of a broader educational framework governed by a syllabus, which often spans several months or years.[165] According to Herbartianism, teaching is broken down into phases. The initial phase involves preparing the student's mind for new information. Subsequently, new ideas are introduced to the learner and then linked to concepts already familiar to them. In later phases, understanding transitions to a more general level beyond specific instances, and the ideas are then applied in practical contexts.[166]\n The history of education delves into the processes, methods, and institutions entwined with teaching and learning, aiming to elucidate their interplay and influence on educational practices over time.[167]\n Education during prehistory primarily facilitated enculturation, emphasizing practical knowledge and skills essential for daily life, such as food production, clothing, shelter, and safety. Formal schools and specialized instructors were absent, with adults in the community assuming teaching roles, and learning transpiring informally through daily activities, including observation and imitation of elders. In oral societies, storytelling served as a pivotal means of transmitting cultural and religious beliefs across generations.[168][e] With the advent of agriculture during the Neolithic Revolution around 9000 BCE, a gradual educational shift toward specialization ensued, driven by the formation of larger communities and the demand for increasingly intricate artisanal and technical skills.[170]\n Commencing in the 4th millennium BCE and spanning subsequent eras, a pivotal transformation in educational methodologies unfolded with the advent of writing in regions such as Mesopotamia, ancient Egypt, the Indus Valley, and ancient China.[171][f] This breakthrough profoundly influenced the trajectory of education. Writing facilitated the storage, preservation, and dissemination of information, ushering in subsequent advancements such as the creation of educational aids like textbooks and the establishment of institutions such as schools.[173]\n Another significant aspect of ancient education was the establishment of formal education. This became necessary as civilizations evolved and the volume of knowledge expanded, surpassing what informal education could effectively transmit across generations. Teachers assumed specialized roles to impart knowledge, leading to a more abstract educational approach less tied to daily life. Formal education remained relatively rare in ancient societies, primarily accessible to the intellectual elite.[174] It covered fields like reading and writing, record keeping, leadership, civic and political life, religion, and technical skills associated with specific professions.[175] Formal education introduced a new teaching paradigm that emphasized discipline and drills over the informal methods prevalent earlier.[176] Two notable achievements of ancient education include the founding of Plato's Academy in Ancient Greece, often regarded as the earliest institution of higher learning,[177] and the establishment of the Great Library of Alexandria in Ancient Egypt, renowned as one of the ancient world's premier libraries.[178]\n Many facets of education during the medieval period were profoundly influenced by religious traditions. In Europe, the Catholic Church wielded considerable authority over formal education.[179] In the Arab world, the rapid spread of Islam led to various educational advancements during the Islamic Golden Age, integrating classical and religious knowledge and establishing madrasa schools.[180] In Jewish communities, yeshivas emerged as institutions dedicated to the study of religious texts and Jewish law.[181] In China, an expansive state educational and examination system, shaped by Confucian teachings, was instituted.[182] As new complex societies emerged in regions like Africa, the Americas, Northern Europe, and Japan, some adopted existing educational practices, while others developed new traditions.[183]\n Additionally, this era witnessed the establishment of various institutes of higher education and research. Prominent among these were the University of Bologna (the world's oldest university in continuous operation), the University of Paris, and Oxford University in Europe.[184] Other influential centers included the Al-Qarawiyyin University in Morocco,[185] Al-Azhar University in Egypt,[186] and the House of Wisdom in Iraq.[187] Another significant development was the formation of guilds, associations of skilled craftsmen and merchants who regulated their trades and provided vocational education. Prospective members underwent various stages of training on their journey to mastery.[188]\n Starting in the early modern period, education in Europe during the Renaissance slowly began to shift from a religious approach towards one that was more secular. This development was tied to an increased appreciation of the importance of education and a broadened range of topics, including a revived interest in ancient literary texts and educational programs.[189] The turn toward secularization was accelerated during the Age of Enlightenment starting in the 17th century, which emphasized the role of reason and the empirical sciences.[190] European colonization affected education in the Americas through Christian missionary initiatives.[191] In China, the state educational system was further expanded and focused more on the teachings of neo-Confucianism.[192] In the Islamic world, the outreach of formal education increased and remained under the influence of religion.[193] A key development in the early modern period was the invention and popularization of the printing press in the middle of the 15th century, which had a profound impact on general education. It significantly reduced the cost of producing books, which were hand-written before, and thereby augmented the dissemination of written documents, including new forms like newspapers and pamphlets. The increased availability of written media had a major influence on the general literacy of the population.[194]\n These alterations paved the way for the advancement of public education during the 18th and 19th centuries. This era witnessed the establishment of publicly funded schools with the goal of providing education for all, in contrast to previous periods when formal education was primarily delivered by private schools, religious institutions, and individual tutors.[195] An exception to this trend was the Aztec civilization, where formal education was compulsory for youth across social classes as early as the 14th century.[196] Closely related changes were to make education compulsory and free of charge for all children up to a certain age.[197]\n The promotion of public education and universal access to education gained momentum in the 20th and 21st centuries, endorsed by intergovernmental organizations such as the UN. Key initiatives included the Universal Declaration of Human Rights, the Convention on the Rights of the Child, the Education for All initiative, the Millennium Development Goals, and the Sustainable Development Goals.[198] These endeavors led to a consistent increase in all forms of education, particularly impacting primary education. In 1970, 28% of all primary-school-age children worldwide were not enrolled in school; by 2015, this figure had decreased to 9%.[199]\n The establishment of public education was accompanied by the introduction of standardized curricula for public schools as well as standardized tests to assess the progress of students. Contemporary examples are the Test of English as a Foreign Language, which is a globally used test to assess language proficiency in non-native English speakers, and the Programme for International Student Assessment, which evaluates education systems across the world based on the performance of 15-year-old students in reading, mathematics, and science. Similar shifts impacted teachers, with the establishment of institutions and norms to  regulate and oversee teacher training, including certification mandates for teaching in public schools.[200]\n Emerging educational technologies have significantly influenced modern education. The widespread availability of computers and the internet has notably expanded access to educational resources and facilitated new forms of learning, such as online education. This became particularly pertinent during the COVID-19 pandemic when schools worldwide closed for prolonged periods, prompting many to adopt remote learning methods through video conferencing or pre-recorded video lessons to sustain instruction.[201] Additionally, contemporary education is impacted by the increasing globalization and internationalization of educational practices.[202]\n"
    },
    {
        "title": "Harriet Williams Russell Strong",
        "url": "https://en.wikipedia.org/wiki/Harriet_Williams_Russell_Strong",
        "content": "Harriet Russell Strong (n\u00e9e Harriet Williams Russell; July 23, 1844 \u2013 September 6, 1926) was an  American social activist, inventor, businesswoman, conservationist, and leading figure of the early woman's movement.  She has been inducted into the National Women's Hall of Fame and the National Inventors Hall of Fame.  Her pioneering innovations in water storage and flood control enabled the construction of the Hoover Dam and the All-American Canal.[1]\n Harriet Williams Russell was born in Buffalo, New York, fourth daughter of Henry Pierrepont and Mary Guest (Musier) Russell.  She was educated by private teachers and at Young Ladies Seminary at Benicia, California.[citation needed]\n In 1861, the family moved again to Carson City, Nevada, where Harriet met her future husband, Charles Lyman Strong. She was married in Virginia City, Nevada at the age of nineteen, and at thirty-nine was left a widow with four daughters when her husband committed suicide after a series of business failures. Her husband's property, consisting of mines and other lands in Southern California, was involved in litigation lasting eight years. She then devoted her attention to the management and development of this estate, which was known as Ranchito del Fuerte in San Gabriel Valley, California. It was largely planted with walnut and orange trees, as well as pampas grass, and yielded profitable returns. In 1897 she drilled a number of artesian wells, and to utilize the water thus obtained purchased 1,000 acres (4.0\u00a0km2) of land five miles (8\u00a0km) away, installed a pumping plant, and incorporated the property under the name of the Paso de Bartolo Water Company, of which she was president, and her two daughters, respectively, treasurer and secretary, and issued bonds amounting to $110,000 to carry on the enterprise, selling the property four years later at a handsome profit.[2]\n Harriet Strong made a study about the shortage of water, including the control of floodwaters and water storage. She advocated source conservation as a flood remedy, proposing a succession of dams in the Grand Canyon of the Colorado River to conserve the water for irrigation purposes and the generation of electricity. On Dec. 6, 1887, she was granted a patent for a dam and reservoir construction. Her invention consists of a series of dams, one behind the other, to be constructed in a valley, canyon or watercourse in such a way that when the water has filled the lower dam it will extend up to a certain height upon the lower face of the second dam, and thus act as a brace and support for the dam above it. She obtained another patent, Nov. 6, 1894, on a new method for impounding debris and storing water. She was awarded two medals for these inventions by the World's Columbian Exposition in Chicago, Illinois, in 1893. In 1918 she appeared before the congressional committee on water power and urged the government to store the floodwaters of the Colorado River by constructing a series of dams by her method in the Grand Canyon, (which in its full capacity is 150 miles (240\u00a0km) long), and thus control floods and increase irrigation water, making available thousands of acres of land and unlimited power for generating electricity.[3]\n Strong had considerable talent as a musical composer; she published a number of songs and a book of musical sketches, and for many years was vice president of the Los Angeles Symphony Orchestra Association. She was the founder of Ebell of Los Angeles, serving as its president for three consecutive terms. She was also a member of the Friday Morning and Ruskin Art clubs of Los Angeles and became the first female member of the Los Angeles Chamber of Commerce,[4] and of the executive board of the Inland Waterways Association of San Francisco. She was a delegate to the annual convention of the United States Chamber of Commerce, in Chicago, in 1918, representing both the Whittier and Los Angeles chambers of commerce, being the first woman delegate to attend those conventions. She was a member of the board of directors of the Whittier Chamber of Commerce, and chairman of its flood control committee, and member of its Law and Legislative committee, also a member of the Los Angeles Chamber of Commerce Legislative committee. She was responsible for preserving the adobe of Pio Pico, who had been a family friend when she first moved to Southern California. \n Harriet Strong was a dedicated student of Christian Science and was a founding member of First Church of Christ, Scientist, Whittier, California.  Her daughter Nellie became a Christian Science practitioner.[5]\n She had four daughters: Mary Lyman, wife of Dean Mason, of Los Angeles; Harriet Russell;  Nelle de Luce Strong; and Georgina Pierrepont, wife of Hon. Frederick C. Hicks, of New York (who died in Washington, D.C., on January 1, 1918).[citation needed]\n Strong died as a result of an automobile accident in 1926.[6]\n Text from 1921 Biographical Sketch, in the Public Domain\n"
    },
    {
        "title": "Mary Berkeley Minor Blackford",
        "url": "https://en.wikipedia.org/wiki/Mary_Berkeley_Minor_Blackford",
        "content": "Mary Berkeley Minor Blackford (December 2, 1802 \u2013 September 15, 1896) was an American anti-slavery activist, founder of the Female Auxiliary of the American Colonization Society in Fredericksburg.\n Mary Berkeley Minor was born in Fredericksburg, Virginia, the daughter of John Minor III and Lucy Landon Carter Minor. Her father was an American general in the War of 1812, and a close friend of James Monroe.[1] Her extended family included her cousins John Barbee Minor, John Minor Maury, and Matthew Fontaine Maury.[2]\n Blackford was active in the Episcopal church and the temperance movement, and in 1829 founded the Fredericksburg and Falmouth Female Auxiliary of the American Colonization Society.[3] Colonization was a movement to resettle free black people from the United States to Liberia. The proponents of colonization had a  range of anti-slavery, pragmatic, and racist motivations.[4]\n Blackford was herself opposed to slavery,[5] and kept a journal, Notes Illustrative of the Wrongs of Slavery to record her personal experiences that shaped her beliefs on the topic.[6] \"From childhood I have bewailed the unnumbered ills of slavery,\" she wrote, while considering abolition itself impracticable.[7] She taught slaves to read Bibles, against Virginia law, and was threatened with legal consequences.[8] But her husband was not anti-slavery, and the Blackfords had slaves who worked in their household.[3]\n In 1834, she renamed the auxiliary as the Ladies' Society of Fredericksburg and Falmouth, for the Promotion of Female Education in Africa. Their emphasis moved to raising funds to support missionary teachers in Liberia. She raised funds, wrote pamphlets, and maintained an active correspondence with other anti-slavery activists, including her younger brother, a missionary in Liberia from 1937 until his death in 1843.[9] Her direct work lessened when the Blackford family moved to Lynchburg in 1846, but she remained involved with the American Colonization Society and opposed secession before the American Civil War.[3]\n Mary Berkeley Minor married an attorney and newspaper editor, William Matthews Blackford (1801\u20131864) in 1825. They had eight children in fifteen years,[10] and perhaps as a result, she experienced chronic severe back pain most of her adult life. Five of her sons served in the Confederate States Army during the American Civil War,[10] which grieved her: \u201cTo see my sons arrayed against one part of their country,\u201d she wrote in 1861, \u201cis a sorrow that makes me feel the grave is the only place for me.\u201d In 1892, her six living children gathered in Alexandria, Virginia, together for the first time since the war, to celebrate her ninetieth birthday with her.[11]\n She died in Alexandria in 1896, aged 93 years, survived by six of her children.[3] The Blackford Family Papers, and a separate collection of her son Launcelot Minor Blackford's diaries, are held in the Southern Historical Collection at the University of North Carolina.[2][12] Blackford's grandson, also named Launcelot Minor Blackford, wrote a biography of her, Mine Eyes Have Seen The Glory (Harvard University Press 1954).[13][14] Her biographer's papers are archived at Emory University.[15]\n"
    },
    {
        "title": "Women in Singapore",
        "url": "https://en.wikipedia.org/wiki/Women_in_Singapore",
        "content": "\n Women in Singapore, particularly those who have joined Singapore's workforce, are faced with balancing their traditional and modern-day roles in Singaporean society and economy. According to the book The Three Paradoxes: Working Women in Singapore written by Jean Lee S.K., Kathleen Campbell, and Audrey Chia, there are \"three paradoxes\" confronting and challenging the career women of Singapore. Firstly, Singapore's society expects women to become creative and prolific corporate workers who are also expected to play the role of traditional women in the household, particularly as wife and mother. Secondly, Singaporean women are confronted by the \"conflict between work and family\" resulting from their becoming members of the working population. Thirdly, Singapore's female managers are still fewer in number despite their rising educational level and attainments when compared to male managers.[3]\n Until 2007, marital rape was not legally recognised. In 2007, marital rape was recognised under certain circumstances that signalled marriage breakdown. A committee called for the repeal of any kind of marital rape immunity on 9 September 2018.[4] Marital rape has since been completely criminalised under the Criminal Law Reform Act passed on 6 May 2019.[5] The laws came into force on 1 January 2020.[6]\n On 20 September 2020, a virtual dialogue session involving more than 100 participants from youth and women organisations was held. Law and Home Affairs Minister K. Shanmugam announced an initiative that will start in October which will include a series of engagements between the public and private sectors, as well as non-governmental organisations. The aim is to identify and tackle issues concerning women in Singapore. These will culminate in a White Paper to be issued by the Government in the first half of 2021, which will consolidate feedback and recommendations during the sessions, to be called \u201cConversations on Women Development\u201d. The review was later extended to the second half of 2021 due to high demand.[7][8]\n After almost a year of engagements, on 18 September 2021, Prime Minister Lee Hsien Loong announced that the White Paper will be presented to Parliament in early 2022 with three broad areas to be looked into, being ensuring equal workplace opportunities with legislating anti-discrimination rules and better childcare arrangements, better support for caregivers including a possible enhancement to the Home Caregiving Grant and strengthening protection for women both physically and online. In addition, a garden at Dhoby Ghaut Green will be dedicated to the women of Singapore as part of a proposal accepted from the Singapore Council of Women's Organisations to name public spaces to reflect their contributions.[9][10][11]\n At present, there is a low presence of female participants in the political arena of Singapore. Females constitute 42% of Singapore's workforce, however, a large portion of this number occupy low-level and low-salary positions. According to the 2011 article Women's Rights Situation in Singapore, these discrepancies can be mainly attributed not to gender discrimination or gender inequality but instead to the women's lower educational qualifications and fewer job experiences than men, the women's focus and dedication to their role in family life, and the paternalistic character and Confucian temperament of Singaporean society.[12]\n In relation to entrepreneurship, in 1997 Bloomberg Businessweek stated that businesswomen in Singapore can be grouped into two main categories: the entrepreneur woman who was already able to establish and raise a family, and the businesswoman who sought a substitute to the conventional \"career path\". An example of a successful Singaporean businesswoman was Catherine Lam, who established the company known as Fabristeel, a manufacturer of steel carts. Before launching Fabristeel in 1979, Lam worked as an accountant for 10 years. Women in Singapore who ventured into running businesses were motivated by \"better education, the labor shortage\", the encouragement to achieve entrepreneurial success, and the resulting \"flexible lifestyle\" while doing business-related roles.[13][14]\n Another example is Lim Soo Hoon, who was Singapore's Woman of the Year in 1997. Lim was the first female Permanent Secretary of Singapore who worked for the Public Service Division of the office of the Prime Minister of Singapore. Lim held positions at Singapore's Ministry of Trade and Industry, then later into jobs in Singapore's Ministry of Transport, and then in the Ministry of Manpower, and Ministry of Community Development, Youth and Sports.[15]\n With regard to sexuality, BBC News reported in 2001 that Singaporean women have a more open attitude about sexual intimacy in Asia. The study reflected that 18% of the Singaporean women interviewed are \"most likely to initiate\" sexual activity with their personal and intimate partners.[16] This is usually met with mixed opinion, as in the case of the example in 2009 when Dr Eng Kai Er walked through Holland Village naked with Swedish exchange student Jan Phillip and was fined S$2,000 with a warning issued by the Agency for Science, Technology and Research which sponsored her undergraduate studies.[17]\n During the 2000s, 2-3 out of every 10 unfaithful couple members were women. Former decades, like 1980s and 1990s, adulterous women were rare. During the 2010s decade, the statistics changed, being women half the times.[18]\n During the 2010s, there was a trend among 50s and 60s years olds women getting divorced. Most of them claimed they grew tired of their husband's infidelities.[19]\n"
    },
    {
        "title": "Sustainability",
        "url": "https://en.wikipedia.org/wiki/Sustainability",
        "content": "\n Sustainability is a social goal for people to co-exist on Earth over a long period of time. Definitions of this term are disputed and have varied with literature, context, and time.[2][1] Sustainability usually has three dimensions (or pillars): environmental, economic, and social.[1] Many definitions emphasize the environmental dimension.[3][4] This can include addressing key environmental problems, including climate change and biodiversity loss. The idea of sustainability can guide decisions at the global, national, organizational, and individual levels.[5] A related concept is that of sustainable development, and the terms are often used to mean the same thing.[6] UNESCO distinguishes the two like this: \"Sustainability is often thought of as a long-term goal (i.e. a more sustainable world), while sustainable development refers to the many processes and pathways to achieve it.\"[7]\n Details around the economic dimension of sustainability are controversial.[1] Scholars have discussed this under the concept of weak and strong sustainability. For example, there will always be tension between the ideas of \"welfare and prosperity for all\" and environmental conservation,[8][1] so trade-offs are necessary. It would be desirable to find ways that separate economic growth from harming the environment.[9] This means using fewer resources per unit of output even while growing the economy.[10] This decoupling reduces the environmental impact of economic growth, such as pollution. Doing this is difficult.[11][12] Some experts say there is no evidence that such a decoupling is happening at the required scale.[13]\n It is challenging to measure sustainability as the concept is complex, contextual, and dynamic.[14] Indicators have been developed to cover the environment, society, or the economy but there is no fixed definition of sustainability indicators.[15] The metrics are evolving and include indicators, benchmarks and audits. They include sustainability standards and certification systems like Fairtrade and Organic. They also involve indices and accounting systems such as corporate sustainability reporting and Triple Bottom Line accounting. \n It is necessary to address many barriers to sustainability to achieve a sustainability transition or sustainability transformation.[5]:\u200a34\u200a[16] Some barriers arise from nature and its complexity while others are extrinsic to the concept of sustainability. For example, they can result from the dominant institutional frameworks in countries.\n Global issues of sustainability are difficult to tackle as they need global solutions. Existing global organizations such as the UN and WTO are seen as inefficient in enforcing current global regulations. One reason for this is the lack of suitable sanctioning mechanisms.[5]:\u200a135\u2013145\u200a Governments are not the only sources of action for sustainability. For example, business groups have tried to integrate ecological concerns with economic activity, seeking sustainable business.[17][18] Religious leaders have stressed the need for caring for nature and environmental stability. Individuals can also live more sustainably.[5]\n Some people have criticized the idea of sustainability. One point of criticism is that the concept is vague and only a buzzword.[19][1] Another is that sustainability might be an impossible goal.[20] Some experts have pointed out that \"no country is delivering what its citizens need without transgressing the biophysical planetary boundaries\".[21]:\u200a11\u200a\n Sustainability is regarded as a \"normative concept\".[5][22][23][2] This means it is based on what people value or find desirable: \"The quest for sustainability involves connecting what is known through scientific study to applications in pursuit of what people want for the future.\"[23]\n The 1983 UN Commission on Environment and Development (Brundtland Commission) had a big influence on the use of the term sustainability today. The commission's 1987 Brundtland Report provided a definition of sustainable development. The report, Our Common Future, defines it as development that \"meets the needs of the present without compromising the ability of future generations to meet their own needs\".[24][25] The report helped bring sustainability into the mainstream of policy discussions. It also popularized the concept of sustainable development.[1]\n Some other key concepts to illustrate the meaning of sustainability include:[23]\n In everyday usage, sustainability often focuses on the environmental dimension.[citation needed]\n Scholars say that a single specific definition of sustainability may never be possible. But the concept is still useful.[2][23] There have been attempts to define it, for example:\n Some definitions focus on the environmental dimension. The Oxford Dictionary of English defines sustainability as: \"the property of being environmentally sustainable; the degree to which a process or enterprise is able to be maintained or continued while avoiding the long-term depletion of natural resources\".[27]\n The term sustainability is derived from the Latin word sustinere. \"To sustain\" can mean to maintain, support, uphold, or endure.[28][29] So sustainability is the ability to continue over a long period of time.\n In the past, sustainability referred to environmental sustainability. It meant using natural resources so that people in the future could continue to rely on them in the long term.[30][31] The concept of sustainability, or Nachhaltigkeit in German, goes back to Hans Carl von Carlowitz (1645\u20131714), and applied to forestry. The term for this now would be sustainable forest management.[32] He used this term to mean the long-term responsible use of a natural resource. In his 1713 work Silvicultura oeconomica,[33] he wrote that \"the highest art/science/industriousness [...] will consist in such a conservation and replanting of timber that there can be a continuous, ongoing and sustainable use\".[34] The shift in use of \"sustainability\" from preservation of forests (for future wood production) to broader preservation of environmental resources (to sustain the world for future generations) traces to a 1972 book by Ernst Basler, based on a series of lectures at M.I.T.[35]\n The idea itself goes back a very long time: Communities have always worried about the capacity of their environment to sustain them in the long term. Many ancient cultures, traditional societies, and indigenous peoples have restricted the use of natural resources.[36]\n The terms sustainability and sustainable development are closely related. In fact, they are often used to mean the same thing.[6] Both terms are linked with the \"three dimensions of sustainability\" concept.[1] One distinction is that sustainability is a general concept, while sustainable development can be a policy or organizing principle. Scholars say sustainability is a broader concept because sustainable development focuses mainly on human well-being.[23]\n Sustainable development has two linked goals. It aims to meet human development goals. It also aims to enable natural systems to provide the natural resources and ecosystem services needed for economies and society. The concept of sustainable development has come to focus on economic development, social development and environmental protection for future generations.[citation needed]\n Scholars usually distinguish three different areas of sustainability. These are the environmental, the social, and the economic. Several terms are in use for this concept. Authors may speak of three pillars, dimensions, components, aspects,[37] perspectives, factors, or goals. All mean the same thing in this context.[1] The three dimensions paradigm has few theoretical foundations.[1]\n The popular three intersecting circles, or Venn diagram, representing sustainability first appeared in a 1987 article by the economist Edward Barbier.[1][38]\n Scholars rarely question the distinction itself. The idea of sustainability with three dimensions is a dominant interpretation in the literature.[1]\n In the Brundtland Report, the environment and development are inseparable and go together in the search for sustainability. It described sustainable development as a global concept linking environmental and social issues. It added sustainable development is important for both developing countries and industrialized countries:\n The 'environment' is where we all live; and 'development' is what we all do in attempting to improve our lot within that abode. The two are inseparable. [...] We came to see that a new development path was required, one that sustained human progress not just in a few pieces for a few years, but for the entire planet into the distant future. Thus 'sustainable development' becomes a goal not just for the 'developing' nations, but for industrial ones as well. The Rio Declaration from 1992 is seen as \"the foundational instrument in the move towards sustainability\".[39]:\u200a29\u200a It includes specific references to ecosystem integrity.[39]:\u200a31\u200a The plan associated with carrying out the Rio Declaration also discusses sustainability in this way. The plan, Agenda 21, talks about economic, social, and environmental dimensions:[40]:\u200a8.6\u200a\n Countries could develop systems for monitoring and evaluation of progress towards achieving sustainable development by adopting indicators that measure changes across economic, social and environmental dimensions. Agenda 2030 from 2015 also viewed sustainability in this way. It sees the 17 Sustainable Development Goals (SDGs) with their 169 targets as balancing \"the three dimensions of sustainable development, the economic, social and environmental\".[41]\n Scholars have discussed how to rank the three dimensions of sustainability. Many publications state that the environmental dimension is the most important.[3][4] (Planetary integrity or ecological integrity are other terms for the environmental dimension.) \n Protecting ecological integrity is the core of sustainability according to many experts.[4] If this is the case then its environmental dimension sets limits to economic and social development.[4]\n The diagram with three nested ellipses is one way of showing the three dimensions of sustainability together with a hierarchy: It gives the environmental dimension a special status. In this diagram, the environment includes society, and society includes economic conditions. Thus it stresses a hierarchy. \n Another model shows the three dimensions in a similar way: In this SDG wedding cake model, the economy is a smaller subset of the societal system. And the societal system in turn is a smaller subset of the biosphere system.[43]\n In 2022 an assessment examined the political impacts of the Sustainable Development Goals. The assessment found that the \"integrity of the earth's life-support systems\" was essential for sustainability.[3]:\u200a140\u200a The authors said that \"the SDGs fail to recognize that planetary, people and prosperity concerns are all part of one earth system, and that the protection of planetary integrity should not be a means to an end, but an end in itself\".[3]:\u200a147\u200a The aspect of environmental protection is not an explicit priority for the SDGs. This causes problems as it could encourage countries to give the environment less weight in their developmental plans.[3]:\u200a144\u200a The authors state that \"sustainability on a planetary scale is only achievable under an overarching Planetary Integrity Goal that recognizes the biophysical limits of the planet\".[3]:\u200a161\u200a\n Other frameworks bypass the compartmentalization of sustainability into separate dimensions completely.[1]\n The environmental dimension is central to the overall concept of sustainability. People became more and more aware of environmental pollution in the 1960s and 1970s. This led to discussions on sustainability and sustainable development. This process began in the 1970s with concern for environmental issues. These included natural ecosystems or natural resources and the human environment. It later extended to all systems that support life on Earth, including human society.[44]:\u200a31\u200a Reducing these negative impacts on the environment would improve environmental sustainability.[44][45]\n Environmental pollution is not a new phenomenon. But it has been only a local or regional concern for most of human history. Awareness of global environmental issues increased in the 20th century.[44]:\u200a5\u200a[46] The harmful effects and global spread of pesticides like DDT came under scrutiny in the 1960s.[47] In the 1970s it emerged that chlorofluorocarbons (CFCs) were depleting the ozone layer. This led to the de facto ban of CFCs with the Montreal Protocol in 1987.[5]:\u200a146\u200a\n In the early 20th century, Arrhenius discussed the effect of greenhouse gases on the climate (see also: history of climate change science).[48] Climate change due to human activity became an academic and political topic several decades later. This led to the establishment of the IPCC in 1988 and the UNFCCC in 1992.\n In 1972, the UN Conference on the Human Environment took place. It was the first UN conference on environmental issues. It stated it was important to protect and improve the human environment.[49]:\u200a3\u200aIt emphasized the need to protect wildlife and natural habitats:[49]:\u200a4\u200a\n The natural resources of the earth, including the air, water, land, flora and fauna and [...] natural ecosystems must be safeguarded for the benefit of present and future generations through careful planning or management, as appropriate. In 2000, the UN launched eight Millennium Development Goals. The aim was for the global community to achieve them by 2015. Goal 7 was to \"ensure environmental sustainability\". But this goal did not mention the concepts of social or economic sustainability.[1]\n Specific problems often dominate public discussion of the environmental dimension of sustainability: In the 21st century these problems have included climate change, biodiversity and pollution. Other global problems are loss of ecosystem services, land degradation, environmental impacts of animal agriculture and air and water pollution, including marine plastic pollution and ocean acidification.[50][51] Many people worry about human impacts on the environment. These include impacts on the atmosphere, land, and water resources.[44]:\u200a21\u200a\n Human activities now have an impact on Earth's geology and ecosystems. This led Paul Crutzen to call the current geological epoch the Anthropocene.[52]\n The economic dimension of sustainability is controversial.[1] This is because the term development within sustainable development can be interpreted in different ways. Some may take it to mean only economic development and growth. This can promote an economic system that is bad for the environment.[53][54][55] Others focus more on the trade-offs between environmental conservation and achieving welfare goals for basic needs (food, water, health, and shelter).[8]\n Economic development can indeed reduce hunger or energy poverty. This is especially the case in the least developed countries. That is why Sustainable Development Goal 8 calls for economic growth to drive social progress and well-being. Its first target is for: \"at least 7 per cent GDP growth per annum in the least developed countries\".[56] However, the challenge is to expand economic activities while reducing their environmental impact.[10]:\u200a8\u200a In other words, humanity will have to find ways how societal progress (potentially by economic development) can be reached without excess strain on the environment. \n The Brundtland report says poverty causes environmental problems. Poverty also results from them. So addressing environmental problems requires understanding the factors behind world poverty and inequality.[24]:\u200aSection I.1.8\u200a The report demands a new development path for sustained human progress. It highlights that this is a goal for both developing and industrialized nations.[24]:\u200aSection I.1.10\u200a\n UNEP and UNDP launched the Poverty-Environment Initiative in 2005 which has three goals. These are reducing extreme poverty, greenhouse gas emissions, and net natural asset loss. This guide to structural reform will enable countries to achieve the SDGs.[57][58]:\u200a11\u200a It should also show how to address the trade-offs between ecological footprint and economic development.[5]:\u200a82\u200a\n The social dimension of sustainability is not well defined.[59][60][61] One definition states that a society is sustainable in social terms if people do not face structural obstacles in key areas. These key areas are health, influence, competence, impartiality and meaning-making.[62]\n Some scholars place social issues at the very center of discussions.[63] They suggest that all the domains of sustainability are social. These include ecological, economic, political, and cultural sustainability. These domains all depend on the relationship between the social and the natural. The ecological domain is defined as human embeddedness in the environment. From this perspective, social sustainability encompasses all human activities.[64] It goes beyond the intersection of economics, the environment, and the social.[65]\n There are many broad strategies for more sustainable social systems. They include improved education and the political empowerment of women. This is especially the case in developing countries. They include greater regard for social justice. This involves equity between rich and poor both within and between countries. And it includes intergenerational equity.[66] Providing more social safety nets to vulnerable populations would contribute to social sustainability.[67]:\u200a11\u200a\n A society with a high degree of social sustainability would lead to livable communities with a good quality of life (being fair, diverse, connected and democratic).[68]\n Indigenous communities might have a focus on particular aspects of sustainability, for example spiritual aspects, community-based governance and an emphasis on place and locality.[69]\n Some experts have proposed further dimensions. These could cover institutional, cultural, political, and technical dimensions.[1]\n Some scholars have argued for a fourth dimension. They say the traditional three dimensions do not reflect the complexity of contemporary society.[70] For example, Agenda 21 for culture and the United Cities and Local Governments argue that sustainable development should include a solid cultural policy. They also advocate for a cultural dimension in all public policies. Another example was the Circles of Sustainability approach, which included cultural sustainability.[71]\n People often debate the relationship between the environmental and economic dimensions of sustainability.[72] In academia, this is discussed under the term weak and strong sustainability. In that model, the weak sustainability concept states that capital made by humans could replace most of the natural capital.[73][72] Natural capital is a way of describing environmental resources. People may refer to it as nature. An example for this is the use of environmental technologies to reduce pollution.[74]\n The opposite concept in that model is strong sustainability. This assumes that nature provides functions that technology cannot replace.[75] Thus, strong sustainability acknowledges the need to preserve ecological integrity.[5]:\u200a19\u200a The loss of those functions makes it impossible to recover or repair many resources and ecosystem services. Biodiversity, along with pollination and fertile soils, are examples. Others are clean air, clean water, and regulation of climate systems.\n Weak sustainability has come under criticism. It may be popular with governments and business but does not ensure the preservation of the earth's ecological integrity.[76] This is why the environmental dimension is so important.[4]\n The World Economic Forum illustrated this in 2020. It found that $44 trillion of economic value generation depends on nature. This value, more than half of the world's GDP, is thus vulnerable to nature loss.[77]:\u200a8\u200a Three large economic sectors are highly dependent on nature: construction, agriculture, and food and beverages. Nature loss results from many factors. They include land use change, sea use change and climate change. Other examples are natural resource use, pollution, and invasive alien species.[77]:\u200a11\u200a\n Trade-offs between different dimensions of sustainability are a common topic for debate. Balancing the environmental, social, and economic dimensions of sustainability is difficult. This is because there is often disagreement about the relative importance of each. To resolve this, there is a need to integrate, balance, and reconcile the dimensions.[1] For example, humans can choose to make ecological integrity a priority or to compromise it.[4]\n Some even argue the Sustainable Development Goals are unrealistic. Their aim of universal human well-being conflicts with the physical limits of Earth and its ecosystems.[21]:\u200a41\u200a\n There are several methods to measure or describe human impacts on Earth. They include the ecological footprint, ecological debt, carrying capacity, and sustainable yield. The idea of planetary boundaries is that there are limits to the carrying capacity of the Earth. It is important not to cross these thresholds to prevent irreversible harm to the Earth.[85][86] These planetary boundaries involve several environmental issues. These include climate change and biodiversity loss. They also include types of pollution. These are biogeochemical (nitrogen and phosphorus), ocean acidification, land use, freshwater, ozone depletion, atmospheric aerosols, and chemical pollution.[85][87] (Since 2015 some experts refer to biodiversity loss as change in biosphere integrity. They refer to chemical pollution as introduction of novel entities.)\n The IPAT formula measures the environmental impact of humans. It emerged in the 1970s. It states this impact is proportional to human population, affluence and technology.[88] This implies various ways to increase environmental sustainability. One would be human population control. Another would be to reduce consumption and affluence[89] such as energy consumption. Another would be to develop innovative or green technologies such as renewable energy. In other words, there are two broad aims. The first would be to have fewer consumers. The second would be to have less environmental footprint per consumer.\n The Millennium Ecosystem Assessment from 2005 measured 24 ecosystem services. It concluded that only four have improved over the last 50 years. It found 15 are in serious decline and five are in a precarious condition.[90]:\u200a6\u201319\u200a\n Experts in environmental economics have calculated the cost of using public natural resources. One project calculated the damage to ecosystems and biodiversity loss. This was the Economics of Ecosystems and Biodiversity project from 2007 to 2011.[91]\n An entity that creates environmental and social costs often does not pay for them. The market price also does not reflect those costs. In the end, government policy is usually required to resolve this problem.[92]\n Decision-making can take future costs and benefits into account. The tool for this is the social discount rate. The bigger the concern for future generations, the lower the social discount rate should be.[93] Another approach is to put an economic value on ecosystem services. This allows us to assess environmental damage against perceived short-term welfare benefits. One calculation is that, \"for every dollar spent on ecosystem restoration, between three and 75 dollars of economic benefits from ecosystem goods and services can be expected\".[94]\n In recent years, economist Kate Raworth has developed the concept of doughnut economics. This aims to integrate social and environmental sustainability into economic thinking. The social dimension acts as a minimum standard to which a society should aspire. The carrying capacity of the planet acts an outer limit.[95]\n There are many reasons why sustainability is so difficult to achieve. These reasons have the name sustainability barriers.[5][16] Before addressing these barriers it is important to analyze and understand them.[5]:\u200a34\u200a Some barriers arise from nature and its complexity (\"everything is related\").[23] Others arise from the human condition. One example is the value-action gap. This reflects the fact that people often do not act according to their convictions. Experts describe these barriers as intrinsic to the concept of sustainability.[96]:\u200a81\u200a\n Other barriers are extrinsic to the concept of sustainability. This means it is possible to overcome them. One way would be to put a price tag on the consumption of public goods.[96]:\u200a84\u200a Some extrinsic barriers relate to the nature of dominant institutional frameworks. Examples would be where market mechanisms fail for public goods. Existing societies, economies, and cultures encourage increased consumption. There is a structural imperative for growth in competitive market economies. This inhibits necessary societal change.[89]\n Furthermore, there are several barriers related to the difficulties of implementing sustainability policies. There are trade-offs between the goals of environmental policies and economic development. Environmental goals include nature conservation. Development may focus on poverty reduction.[16][5]:\u200a65\u200a There are also trade-offs between short-term profit and long-term viability.[96]:\u200a65\u200a Political pressures generally favor the short term over the long term. So they form a barrier to actions oriented toward improving sustainability.[96]:\u200a86\u200a\n Barriers to sustainability may also reflect current trends. These could include consumerism and short-termism.[96]:\u200a86\u200a\n While no consensus definition exists, sustainability transformation (or transition) can be understood as \u201ca fundamental, system-wide reorganization across technological, economic and social factors, including paradigms, goals and values\u201d.[97] Sustainability transformation is a process which is complex, multi-dimensional and politically contested. It needs to occur at scales ranging from households and communities to states and regional and global governance institutions. However, societal transformations are politically contested because different stakeholders may disagree over both the ends that transformation should achieve and the means of achieving those ends. Another reason is that transformations may involve or require disrupting existing configurations of power and resources.[97]\n There are long-standing debates in research and policy about whether democratic practices are capable of fostering timely, large-scale transformations towards sustainability. While a few scholars argue that large-scale transformation to sustainability will require the rollback of democratic safeguards or the imposition of technocratic or authoritarian rule, a majority of researchers on the democracy-environment nexus argue that democratization and sustainability transformation are mutually supportive.[97]\n A sustainability transition requires major change in societies. They must change their fundamental values and organizing principles.[44]:\u200a15\u200a These new values would emphasize \"the quality of life and material sufficiency, human solidarity and global equity, and affinity with nature and environmental sustainability\".[44]:\u200a15\u200a A transition may only work if far-reaching lifestyle changes accompany technological advances.[89]\n Scientists have pointed out that: \"Sustainability transitions come about in diverse ways, and all require civil-society pressure and evidence-based advocacy, political leadership, and a solid understanding of policy instruments, markets, and other drivers.\"[51]\n There are four possible overlapping processes of transformation. They each have different political dynamics. Technology, markets, government, or citizens can lead these processes.[22]\n The European Environment Agency defines a sustainability transition as \"a fundamental and wide-ranging transformation of a socio-technical system towards a more sustainable configuration that helps alleviate persistent problems such as climate change, pollution, biodiversity loss or resource scarcities.\"[98]:\u200a152\u200a The concept of sustainability transitions is similar to the concept of energy transitions.[99]\n One expert argues a sustainability transition must be \"supported by a new kind of culture, a new kind of collaboration, [and] a new kind of leadership\".[100] It requires a large investment in \"new and greener capital goods, while simultaneously shifting capital away from unsustainable systems\".[21]:\u200a107\u200a\n In 2024 an interdisciplinary group of experts including Chip Fletcher, William J. Ripple, Phoebe Barnard, Kamanamaikalani Beamer, Christopher Field, David Karl, David King, Michael E. Mann and Naomi Oreskes advocated for a paradigm shift toward genuine sustainability and resource regeneration. They said that \"such a transformation is imperative to reverse the tide of biodiversity loss due to overconsumption and to reinstate the security of food and water supplies, which are foundational for the survival of global populations.\"[101]\n It is possible to divide action principles to make societies more sustainable into four types. These are nature-related, personal, society-related and systems-related principles.[5]:\u200a206\u200a\n There are many approaches that people can take to transition to environmental sustainability. These include maintaining ecosystem services, protecting and co-creating common resources, reducing food waste, and promoting dietary shifts towards plant-based foods.[102]  Another is reducing population growth by cutting fertility rates. Others are promoting new green technologies, and adopting renewable energy sources while phasing out subsidies to fossil fuels.[51]\n In 2017 scientists published an update to the 1992 World Scientists' Warning to Humanity. It showed how to move towards environmental sustainability. It proposed steps in three areas:[51]\n In 2015, the United Nations agreed the Sustainable Development Goals (SDGs). Their official name is Agenda 2030 for the Sustainable Development Goals. The UN described this programme as a very ambitious and transformational vision. It said the SDGs were of unprecedented scope and significance.[41]:\u200a3/35\u200a\n The UN said: \"We are determined to take the bold and transformative steps which are urgently needed to shift the world on to a sustainable and resilient path.\"[41]\n The 17 goals and targets lay out transformative steps. For example, the SDGs aim to protect the future of planet Earth. The UN pledged to \"protect the planet from degradation, including through sustainable consumption and production, sustainably managing its natural resources and taking urgent action on climate change, so that it can support the needs of the present and future generations\".[41]\n Eco-economic decoupling is an idea to resolve tradeoffs between economic growth and environmental conservation. The idea is to \"decouple environmental bads from economic goods as a path towards sustainability\".[11] This would mean \"using less resources per unit of economic output and reducing the environmental impact of any resources that are used or economic activities that are undertaken\".[10]:\u200a8\u200a The intensity of pollutants emitted makes it possible to measure pressure on the environment. This in turn makes it possible to measure decoupling. This involves following changes in the emission intensity associated with economic output.[10] Examples of absolute long-term decoupling are rare. But some industrialized countries have decoupled GDP growth from production- and consumption-based CO2 emissions.[103] Yet, even in this example, decoupling alone is not enough. It is necessary to accompany it with \"sufficiency-oriented strategies and strict enforcement of absolute reduction targets\".[103]:\u200a1\u200a\n One study in 2020 found no evidence of necessary decoupling. This was a meta-analysis of 180 scientific studies. It found that there is \"no evidence of the kind of decoupling needed for ecological sustainability\" and that \"in the absence of robust evidence, the goal of decoupling rests partly on faith\".[11] Some experts have questioned the possibilities for decoupling and thus the feasibility of green growth.[12] Some have argued that decoupling on its own will not be enough to reduce environmental pressures. They say it would need to include the issue of economic growth.[12] There are several reasons why adequate decoupling is currently not taking place. These are rising energy expenditure, rebound effects, problem shifting, the underestimated impact of services, the limited potential of recycling, insufficient and inappropriate technological change, and cost-shifting.[12]\n The decoupling of economic growth from environmental deterioration is difficult. This is because the entity that causes environmental and social costs does not generally pay for them. So the market price does not express such costs.[92] For example, the cost of packaging into the price of a product. may factor in the cost of packaging. But it may omit the cost of disposing of that packaging. Economics describes such factors as externalities, in this case a negative externality.[104] Usually, it is up to government action or local governance to deal with externalities.[105]\n There are various ways to incorporate environmental and social costs and benefits into economic activities. Examples include: taxing the activity (the polluter pays); subsidizing activities with positive effects (rewarding stewardship); and outlawing particular levels of damaging practices (legal limits on pollution).[92]\n A textbook on natural resources and environmental economics stated in 2011: \"Nobody who has seriously studied the issues believes that the economy's relationship to the natural environment can be left entirely to market forces.\"[106]:\u200a15\u200a This means natural resources will be over-exploited and destroyed in the long run without government action.\n Elinor Ostrom (winner of the 2009Nobel economics prize) expanded on this. She stated that local governance (or self-governance) can be a third option besides the market or the national government.[107] She studied how people in small, local communities manage shared natural resources.[108] She showed that communities using natural resources can establish rules their for use and maintenance. These are resources such as pastures, fishing waters, and forests. This leads to both economic and ecological sustainability.[107] Successful self-governance needs groups with frequent communication among participants. In this case, groups can manage the usage of common goods without overexploitation.[5]:\u200a117\u200a Based on Ostrom's work, some have argued that: \"Common-pool resources today are overcultivated because the different agents do not know each other and cannot directly communicate with one another.\"[5]:\u200a117\u200a\n Questions of global concern are difficult to tackle. That is because global issues need global solutions. But existing global organizations (UN, WTO, and others) do not have sufficient means.[5]:\u200a135\u200a For example, they lack sanctioning mechanisms to enforce existing global regulations.[5]:\u200a136\u200a Some institutions do not enjoy universal acceptance. An example is the International Criminal Court. Their agendas are not aligned (for example UNEP, UNDP, and WTO) And some accuse them of nepotism and mismanagement.[5]:\u200a135\u2013145\u200a\u200a\n Multilateral international agreements, treaties, and intergovernmental organizations (IGOs) face further challenges. These result in barriers to sustainability. Often these arrangements rely on voluntary commitments. An example is Nationally Determined Contributions for climate action. There can be a lack of enforcement of existing national or international regulation. And there can be gaps in regulation for international actors such as multi-national enterprises. Critics of some global organizations say they lack legitimacy and democracy. Institutions facing such criticism include the WTO, IMF, World Bank, UNFCCC, G7, G8 and OECD.[5]:\u200a135\u200a\n Sustainable business practices integrate ecological concerns with social and economic ones.[17][18] One accounting framework for this approach uses the phrase \"people, planet, and profit\". The name of this approach is the triple bottom line. The circular economy is a related concept. Its goal is to decouple environmental pressure from economic growth.[109][110]\n Growing attention towards sustainability has led to the formation of many organizations. These include the Sustainability Consortium of the Society for Organizational Learning,[111] the Sustainable Business Institute,[112] and the World Business Council for Sustainable Development.[113] Supply chain sustainability looks at the environmental and human impacts of products in the supply chain. It considers how they move from raw materials sourcing to production, storage, and delivery, and every transportation link on the way.[114]\n \nReligious leaders have stressed the importance of caring for nature and environmental sustainability. In 2015 over 150 leaders from various faiths issued a joint statement to the UN Climate Summit in Paris 2015.[115] They reiterated a statement made in the Interfaith Summit in New York in 2014: As representatives from different faith and religious traditions, we stand together to express deep concern for the consequences of climate change on the earth and its people, all entrusted, as our faiths reveal, to our common care. Climate change is indeed a threat to life, a precious gift we have received and that we need to care for.[116] Individuals can also live in a more sustainable way. They can change their lifestyles, practise ethical consumerism, and embrace frugality.[5]:\u200a236\u200a  These sustainable living approaches can also make cities more sustainable. They do this by altering the built environment.[117] Such approaches include sustainable transport, sustainable architecture, and zero emission housing. Research can identify the main issues to focus on. These include flying, meat and dairy products, car driving, and household sufficiency. Research can show how to create cultures of sufficiency, care, solidarity, and simplicity.[89]\n Some young people are using activism, litigation, and on-the-ground efforts to advance sustainability. This is particularly the case in the area of climate action.[67]:\u200a60\u200a\n Scholars have criticized the concepts of sustainability and sustainable development from different angles. One was Dennis Meadows, one of the authors of the first report to the Club of Rome, called \"The Limits to Growth\". He argued many people deceive themselves by using the Brundtland definition of sustainability.[53] This is because the needs of the present generation are actually not met today. Instead, economic activities to meet present needs will shrink the options of future generations.[118][5]:\u200a27\u200a Another criticism is that the paradigm of sustainability is no longer suitable as a guide for transformation. This is because societies are \"socially and ecologically self-destructive consumer societies\".[119]\n Some scholars have even proclaimed the end of the concept of sustainability. This is because humans now have a significant impact on Earth's climate system and ecosystems.[20] It might become impossible to pursue sustainability because of these complex, radical, and dynamic issues.[20] Others have called sustainability a utopian ideal: \"We need to keep sustainability as an ideal; an ideal which we might never reach, which might be utopian, but still a necessary one.\"[5]:\u200a5\u200a\n The term is often hijacked and thus can lose its meaning. People use it for all sorts of things, such as saving the planet to recycling your rubbish.[27] A specific definition may never be possible. This is because sustainability is a concept that provides a normative structure. That describes what human society regards as good or desirable.[2]\n But some argue that while sustainability is vague and contested it is not meaningless.[2] Although lacking in a singular definition, this concept is still useful. Scholars have argued that its fuzziness can actually be liberating. This is because it means that \"the basic goal of sustainability (maintaining or improving desirable conditions [...]) can be pursued with more flexibility\".[23]\n Sustainability has a reputation as a buzzword.[1] People may use the terms sustainability and sustainable development in ways that are different to how they are usually understood. This can result in confusion and mistrust. So a clear explanation of how the terms are being used in a particular situation is important.[23]\n Greenwashing is a practice of deceptive marketing. It is when a company or organization provides misleading information about the sustainability of a product, policy, or other activity.[67]:\u200a26\u200a[120] Investors are wary of this issue as it exposes them to risk.[121] The reliability of eco-labels is also doubtful in some cases.[122] Ecolabelling is a voluntary method of environmental performance certification and labelling for food and consumer products. The most credible eco-labels are those developed with close participation from all relevant stakeholders.[123]\n"
    },
    {
        "title": "Biodiversity loss",
        "url": "https://en.wikipedia.org/wiki/Biodiversity_loss",
        "content": "\n \n Biodiversity loss happens when plant or animal species disappear completely from Earth (extinction) or when there is a decrease or disappearance of species in a specific area. Biodiversity loss means that there is a reduction in biological diversity in a given area. The decrease can be temporary or permanent. It is temporary if the damage that led to the loss is reversible in time, for example through ecological restoration. If this is not possible, then the decrease is permanent. The cause of most of the biodiversity loss is, generally speaking, human activities that push the planetary boundaries too far.[1][2][3] These activities include habitat destruction[4] (for example deforestation) and land use intensification (for example monoculture farming).[5][6] Further problem areas are air and water pollution (including nutrient pollution), over-exploitation, invasive species[7] and climate change.[4]\n Many scientists, along with the Global Assessment Report on Biodiversity and Ecosystem Services, say that the main reason for biodiversity loss is a growing human population because this leads to human overpopulation and excessive consumption.[8][9][10][11][12] Others disagree, saying that loss of habitat is caused mainly by \"the growth of commodities for export\" and that population has very little to do with overall consumption. More important are wealth disparities between and within countries.[13]\n Climate change is another threat to global biodiversity.[14][15] For example, coral reefs\u2014which are biodiversity hotspots\u2014will be lost by the year 2100 if global warming continues at the current rate.[16][17] Still, it is the general habitat destruction (often for expansion of agriculture), not climate change, that is currently the bigger driver of biodiversity loss.[18][19] Invasive species and other disturbances have become more common in forests in the last several decades. These tend to be directly or indirectly connected to climate change and can cause a deterioration of forest ecosystems.[20][21]\n Groups that care about the environment have been working for many years to stop the decrease in biodiversity. Nowadays, many global policies include activities to stop biodiversity loss. For example, the UN Convention on Biological Diversity aims to prevent biodiversity loss and to conserve wilderness areas. However, a 2020 United Nations Environment Programme report found that most of these efforts had failed to meet their goals.[22] For example, of the 20 biodiversity goals laid out by the Aichi Biodiversity Targets in 2010, only six were \"partially achieved\" by 2020.[23][24]\n This ongoing global extinction is also called the holocene extinction or sixth mass extinction.\n The current rate of global biodiversity loss is estimated to be 100 to 1000 times higher than the (naturally occurring) background extinction rate, faster than at any other time in human history,[25][26] and is expected to grow in the upcoming years.[27][28][29] The fast-growing extinction trends of various animal groups like mammals, birds, reptiles, amphibians, and fish have led scientists to declare a current biodiversity crisis in both land and ocean ecosystems.[30][31]\n In 2006, many more species were formally classified as rare or endangered or threatened; moreover, scientists have estimated that millions more species are at risk that have not been formally recognized.[32]\n Deforestation also plays a large role in biodiversity loss. More than half of the worlds biodiversity is hosted in tropical rainforest.[33] Regions that are subjected to exponential loss of biodiversity are referred to as biodiversity hotspots. Since 1988 the hotspots increased from 10 to 34. Of the total 34 hotspots currently present, 16 of them are in tropical regions (as of 2006).[34] Researchers have noted in 2006 that only 2.3% of the world is covered with biodiversity loss hotspots, and even though only a small percentage of the world is covered in hotspots, it host a large fraction (50%) of vascular plant species.[35]\n In 2021, about 28 percent of the 134,400 species assessed using the IUCN Red List criteria are now listed as threatened with extinction\u2014a total of 37,400 species compared to 16,119 threatened species in 2006.[36]\n A 2022 study that surveyed more than 3,000 experts found that \"global biodiversity loss and its impacts may be greater than previously thought\", and estimated that roughly 30% of species \"have been globally threatened or driven extinct since the year 1500.\"[37][38]\n Research published in 2023 found that, out of 70,000 species, about 48% are facing decreasing populations due to human activities, while only 3% are seeing an increase in populations.[39][40][41]\n Biologists define biodiversity as the \"totality of genes, species and ecosystems of a region\".[42][43] To measure biodiversity loss rates for a particular location, scientists record the species richness and its variation over time in that area. In ecology, local abundance is the relative representation of a species in a particular ecosystem.[44] It is usually measured as the number of individuals found per sample. The ratio of abundance of one species to one or multiple other species living in an ecosystem is called relative species abundance.[44] Both indicators are relevant for computing biodiversity. \n There are many different biodiversity indexes.[45] These investigate different scales and time spans.[46] Biodiversity has various scales and subcategories (e.g. phylogenetic diversity, species diversity, genetic diversity, nucleotide diversity).[46]\n The question of net loss in confined regions is often a matter of debate.[47]\n An October 2020 analysis by Swiss Re found that one-fifth of all countries are at risk of ecosystem collapse as the result of anthropogenic habitat destruction and increased wildlife loss.[51] If these losses are not reversed, a total ecosystem collapse could ensue.[52]\n In 2022, the World Wildlife Fund reported[53] an average population decline of 68% between 1970 and 2016 for 4,400 animal species worldwide, encompassing nearly 21,000 monitored populations.[54]\n Insects are the most numerous and widespread class in the animal kingdom, accounting for up to 90% of all animal species.[56][57] In the 2010s, reports emerged about the widespread decline in insect populations across multiple insect orders. The reported severity shocked many observers, even though there had been earlier findings of pollinator decline. There have also been anecdotal reports of greater insect abundance  earlier in the 20th century. Many car drivers know this anecdotal evidence through the windscreen phenomenon, for example.[58][59]  Causes for the decline in insect population are similar to those driving other biodiversity loss. They include habitat destruction, such as intensive agriculture, the use of pesticides (particularly insecticides), introduced species, and \u2013 to a lesser degree and only for some regions \u2013 the effects of climate change.[60] An additional cause that may be specific to insects is light pollution (research in that area is ongoing).[61][62][63]\n Scientists have studied loss of earthworms from several long-term agronomic trials. They found that relative biomass losses of  minus 50\u2013100% (with a mean of minus 83\u00a0%) match or exceed those reported for other faunal groups.[72] Thus it is clear that earthworms are similarly depleted in the soils of fields used for intensive agriculture.[72] Earthworms play an important role in ecosystem function,[72] helping with biological processing in soil, water, and even greenhouse gas balancing.[73] There are five reasons for the decline of earthworm diversity: \"(1) soil degradation and habitat loss, (2) climate change, (3) excessive nutrient and other forms of contamination load, (4) over-exploitation and unsustainable management of soil, and (5) invasive species\".[73]:\u200a26\u200a Factors like tillage practices and intensive land use decimate the soil and plant roots that earthworms use to create their biomass.[74] This interferes with carbon and nitrogen cycles. \n Knowledge of earthworm species diversity is quite limited as not even 50% of them have been described.[73] Sustainable agriculture methods could help prevent earthworm diversity decline, for example reduced tillage.[73]:\u200a32\u200a The Secretariat of the Convention on Biological Diversity is trying to take action and promote the restoration and maintenance of the many diverse species of earthworms.[73]\n Since the 1980s, decreases in amphibian populations, including population decline and localized mass extinctions, have been observed in locations all over the world. This type of biodiversity loss is known as one of the most critical threats to global biodiversity. The possible causes include habitat destruction and modification, diseases, exploitation, pollution, pesticide use, introduced species, and ultraviolet-B radiation (UV-B). However, many of the causes of amphibian declines are still poorly understood, and the topic is currently a subject of ongoing research. \n Biomass of mammals on Earth as of 2018[76][77]\n The decline of wild mammal populations globally has been an occurrence spanning over the past 50,000 years, at the same time as the populations of humans and livestock have increased. Nowadays, the total biomass of wild mammals on land is believed to be seven times lower than its prehistoric values, while the biomass of marine mammals had declined fivefold.  At the same time, the biomass of humans is \"an order of magnitude higher than that of all wild mammals\", and the biomass of livestock mammals like pigs and cattle is even larger than that. Even as wild mammals had declined, the growth in the numbers of humans and livestock had increased total mammal biomass fourfold. Only 4% of that increased number are wild mammals, while livestock and humans amount to 60% and 36%. Alongside the simultaneous halving of plant biomass, these striking declines are considered part of the prehistoric phase of the Holocene extinction.[77][76]\n Some pesticides, like insecticides, likely play a role in reducing the populations of specific bird species.[84] According to a study funded by BirdLife International, 51 bird species are critically endangered and eight could be classified as extinct or in danger of extinction. Nearly 30% of extinction is due to hunting and trapping for the exotic pet trade. Deforestation, caused by unsustainable logging and agriculture, could be the next extinction driver, because birds lose their habitat and their food.[85][86]\n While plants are essential for human survival, they have not received the same attention as the conservation of animals.[87] It is estimated that a third of all land plant species are at risk of extinction and 94% have yet to be evaluated in terms of their conservation status.[87] Plants existing at the lowest trophic level require increased conservation to reduce negative impacts at higher trophic levels.[88]\n In 2022, scientists warned that a third of tree species are threatened with extinction. This will significantly alter the world's ecosystems because their carbon, water and nutrient cycles will be affected.[89][90] Forest areas are degraded due to common factors such as logging, fire, and firewood harvesting.[91] The GTA (global tree assessment) has determined that \"17,510 (29.9%) tree species are considered threatened with extinction. In addition, there are 142 tree species recorded as Extinct or Extinct in the Wild.\"[90]\n Possible solutions can be found in some silvicultural methods of forest management that promote tree biodiversity, such as selective logging, thinning or crop tree management, and clear cutting and coppicing.[92] Without solutions, secondary forests recovery in species richness can take 50 years to recover the same amount as the primary forest, or 20 years to recover 80% of species richness.[93]\n Human impact on the environment has driven a range of species extinct and is threatening even more today. Multiple organizations such as IUCN and Royal Botanic Gardens, Kew suggest that around 40% of plant species are threatened with extinction.[95] The majority are threatened by habitat loss, but activities such as logging of wild timber trees and collection of medicinal plants, or the introduction of non-native invasive species, also play a role.[96][97][98]\n Freshwater ecosystems such as swamps, deltas, and rivers make up 1% of earth's surface. They are important because they are home to approximately one third of vertebrate species.[100] Freshwater species are beginning to decline at twice the rate of species that live on land or in the ocean. This rapid loss has already placed 27% of 29,500 species dependent on fresh water on the IUCN Red List.[100]\n Global populations of freshwater fish are collapsing due to water pollution and overfishing. Migratory fish populations have declined by 76% since 1970, and large \"megafish\" populations have fallen by 94% with 16 species declared extinct in 2020.[101]\n Marine biodiversity encompasses any living organism that resides in the ocean or in estuaries.[102] By 2018, approximately 240,000 marine species had been documented.[103] But many marine species\u2014estimates range between 178,000 and 10\u00a0million oceanic species\u2014remain to be described.[102] It is therefore likely that a number of rare species (not seen for decades in the wild) have already disappeared or are on the brink of extinction, unnoticed.[104]\n Human activities have a strong and detrimental influence on marine biodiversity. The main drivers of marine species extinction are habitat loss, pollution, invasive species, and overexploitation.[105][106] Greater pressure is placed on marine ecosystems near coastal areas because of the human settlements in those areas.[107]\n Overexploitation has resulted in the extinction of over 25 marine species. This includes seabirds, marine mammals, algae, and fish.[102][108] Examples of extinct marine species include Steller's sea cow (Hydrodamalis gigas) and the Caribbean monk seal (Monachus tropicalis). Not all extinctions are because of humans. For example, in the 1930s, the eelgrass limpet (Lottia alveus) became extinct in the Atlantic once the Zostera marina seagrass population declined upon exposure to a disease.[109] The Lottia alveus were greatly impacted because the Zostera marina were their sole habitats.[102]\n The main causes of current biodiversity loss are:\n Jared Diamond describes an \"Evil Quartet\" of habitat destruction, overkill, introduced species and secondary extinctions.[110] Edward O. Wilson suggested the acronym HIPPO for the main causes of biodiversity loss: Habitat destruction, Invasive species, Pollution, human over-Population and Over-harvesting.[111][112]\n Habitat destruction (also termed habitat loss and habitat reduction) occurs when a natural habitat is no longer able to support its native species. The organisms once living there have either moved elsewhere, or are dead, leading to a decrease in biodiversity and species numbers.[113][114] Habitat destruction is in fact the leading cause of biodiversity loss and species extinction worldwide.[115]\n For example, habitat loss is one of the causes in the decline of insect populations (see the section below on insects).\n The direct effects of urban growth on habitat loss are well understood: building construction often results in habitat destruction and fragmentation.[117] This leads to selection for species that are adapted to urban environments.[118] Small habitat patches cannot support the level of genetic or taxonomic diversity they formerly could while some more sensitive species may become locally extinct.[119] Species abundance populations are reduced due to the reduced fragmented area of habitat. This causes an increase of species isolation and forces species toward edge habitats and to adapt to foraging elsewhere.[117]\n Infrastructure development in Key Biodiversity Areas (KBA) is a major driver of biodiversity loss, with infrastructure present in roughly 80% of KBAs.[120] Infrastructure development leads to conversion and fragmentation of natural habitat, pollution and disturbance. There can also be direct harm to animals through collisions with vehicles and structures. This can have impacts beyond the infrastructure site.[120]\n Humans are changing the uses of land in various ways, and each can lead to habitat destruction and biodiversity loss. The 2019 Global Assessment Report on Biodiversity and Ecosystem Services found that industrial agriculture is the primary driver of biodiversity collapse.[121][8] The UN's Global Biodiversity Outlook 2014 estimated that 70% of the projected loss of terrestrial biodiversity is caused by agriculture use.[needs update] According to a 2005 publication, \"Cultivated systems [...] cover 24% of Earth's surface\".[122]:\u200a51\u200a The publication defined cultivated areas as \"areas in which at least 30% of the landscape is in croplands, shifting cultivation, confined livestock production, or freshwater aquaculture in any particular year\".[122]:\u200a51\u200a\n More than 17,000 species are at risk of losing habitat by 2050 as agriculture continues to expand to meet future food needs (as of 2020).[123] A global shift toward largely plant-based diets would free up land to allow for the restoration of ecosystems and biodiversity.[124] In the 2010s over 80% of all global farmland was used to rear animals.[124]\n As of 2022, 44% of Earth's land area required conservation attention, which may include declaring protected areas and following land-use policies.[125]\n Air pollution adversely affects biodiversity.[126] Pollutants are emitted into the atmosphere by the burning of fossil fuels and biomass, for example. Industrial and agricultural activity releases the pollutants sulfur dioxide and nitrogen oxides.[127] Once sulfur dioxide and nitrogen oxide are introduced into the atmosphere, they can react with cloud droplets (cloud condensation nuclei), raindrops, or snowflakes, forming sulfuric acid and nitric acid. With the interaction between water droplets and sulfuric and nitric acids, wet deposition occurs and creates acid rain.[128][129]\n A 2009 review studied four air pollutants (sulfur, nitrogen, ozone, and mercury) and several types of ecosystems.[130] Air pollution affects the functioning and biodiversity of terrestrial as well as aquatic ecosystems.[130] For example, \"air pollution causes or contributes to acidification of lakes, eutrophication of estuaries and coastal waters, and mercury bioaccumulation in aquatic food webs\".[130]\n Noise generated by traffic, ships, vehicles, and aircraft can affect the survivability of wildlife species and can reach undisturbed habitats.[131] Noise pollution is common in marine ecosystems, affecting at least 55 marine species.[132] One study found that as seismic noises and naval sonar increases in marine ecosystems, cetacean diversity decreases (including whales and dolphins).[133] Multiple studies have found that fewer fishes, such as cod, haddock, rockfish, herring, sand seal, and blue whiting, have been spotted in areas with seismic noises, with catch rates declining by 40\u201380%.[132][134][135][136]\n Noise pollution has also altered avian communities and diversity. Noise can reduce reproductive success, minimize nesting areas, increase stress response, and reduce species abundance.[137][132] Noise pollution can alter the distribution and abundance of prey species, which can then impact predator populations.[138]\n Fossil fuel extraction and associated oil and gas pipelines have major impacts on the biodiversity of many biomes due to land conversion, habitat loss and degradation, and pollution. An example is the Western Amazon region.[140] Exploitation of fossil fuels there has had significant impacts on biodiversity.[139] As of 2018, many of the protected areas with rich biodiversity were in areas containing unexploited fossil fuel reserves worth between $3 and $15 trillion.[139] The protected areas may be under threat in future.\n Continued overexploitation can lead to the destruction of the resource, as it will be unable to replenish. The term applies to natural resources such as water aquifers, grazing pastures and forests, wild medicinal plants, fish stocks and other wildlife. \n A 2019 Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services report found that overfishing is the main driver of mass species extinction in oceans.[142][143] Overfishing has reduced fish and marine mammal biomass by 60% since the 1800s.[144] It is currently pushing over one-third of sharks and rays toward extinction.[145]\n Many commercial fishes have been overharvested: a 2020 FAO report classified as overfished 34% of the fish stocks of the world's marine fisheries.[146] By 2020, global fish populations had declined 38% since 1970.[103]\n Many regulatory measures are available for controlling overfishing. These include fishing quotas, bag limits, licensing, closed seasons, size limits, and the creation of marine reserves and other marine protected areas. \n The world's population numbered nearly 7.6\u00a0billion as of mid-2017 and is forecast to peak toward the end of the 21st century at 10\u201312\u00a0billion people.[148] Scholars have argued that population size and growth, along with overconsumption, are significant factors in biodiversity loss and soil degradation.[149][150][1][11] Review articles, including the 2019 IPBES report, have also noted that human population growth and overconsumption are significant drivers of species decline.[8][9] A 2022 study warned that conservation efforts will continue to fail if the primary drivers of biodiversity loss continue to be ignored, including population size and growth.[10]\n Other scientists have criticized the assertion that population growth is a key driver for biodiversity loss.[13] They argue that the main driver is the loss of habitat, caused by \"the growth of commodities for export, particularly soybean and oil-palm, primarily for livestock feed or biofuel consumption in higher income economies.\"[13] Because of the wealth disparities between countries, there is a negative correlation between a country's total population and its per capita footprint. On the other hand, the correlation between a country's GDP and its footprint is strong.[13] The study argues that population as a metric is unhelpful and counterproductive for tackling environmental challenges.[13]\n The term invasive is poorly defined and often very subjective.[151] The European Union defines invasive alien species as those outside their natural distribution area that threaten biological diversity.[152][153] Biotic invasion is considered one of the five top drivers of global biodiversity loss and is increasing because of tourism and globalization.[154][155] This may be particularly true in poorly regulated fresh water systems, though quarantines and ballast water rules have improved the situation.[122]\n Invasive species may drive local native species to extinction via competitive exclusion, niche displacement, or hybridisation with related native species. Therefore, alien invasions may result in extensive changes in the structure, composition and global distribution of the biota at sites of introduction. This leads to the homogenisation of the world's fauna and flora and biodiversity loss.[156][157]\n Climate change is another threat to global biodiversity.[14][15] But habitat destruction, e.g., for the expansion of agriculture, is currently a more significant driver of biodiversity loss.[18][19]\n A 2021 collaborative report by scientists from the IPBES and the IPCC found that biodiversity loss and climate change must be addressed simultaneously, as they are inextricably linked and have similar effects on human well-being.[159] In 2022, Frans Timmermans, Vice-President of the European Commission, said that people are less aware of the threat of biodiversity loss than they are of the threat of climate change.[160]\n The interaction between climate change and invasive species is complex and not easy to assess. Climate change is likely to favour some invasive species and harm others,[161] but few authors have identified specific consequences of climate change for invasive species.[162]\n Invasive species and other disturbances have become more common in forests in the last several decades. These tend to be directly or indirectly connected to climate change and have negative consequences for forest ecosystems.[20][21]\n Climate change contributes to destruction of some habitats, endangering various species. For example:  \n There are several plausible pathways that could lead to an increased extinction risk from climate change. Every plant and animal species has evolved to exist within a certain ecological niche.[174] But climate change leads to changes of temperature and average weather patterns.[175][176] These changes can push climatic conditions outside of the species' niche, and ultimately render it extinct.[177] Normally, species faced with changing conditions can either adapt in place through microevolution or move to another habitat with suitable conditions. However, the speed of recent climate change is very fast. Due to this rapid change, for example Ectotherm cold-blooded animals (a category which includes amphibians, reptiles and all invertebrates) may struggle to find a suitable habitat within 50 km of their current location at the end of this century (for a mid-range scenario of future global warming).[178]\n Biodiversity loss has bad effects on the functioning of ecosystems. This in turn affects humans,[45] because affected ecosystems can no longer provide the same quality of ecosystem services, such as crop pollination, cleaning air and water, decomposing waste, and providing forest products as well as areas for recreation and tourism.[122]\n Two key statements of a 2012 comprehensive review of the previous 20 years of research include:[45]\n Permanent global species loss (extinction) is a more dramatic phenomenon than regional changes in species composition. But even minor changes from a healthy stable state can have a dramatic influence on the food web and the food chain, because reductions in one species can adversely affect the entire chain (coextinction). This can lead to an overall reduction in biodiversity, unless alternative stable states of the ecosystem are possible.[183]\n For example, a study on grasslands used manipulated grassland plant diversity and found that ecosystems with higher biodiversity show more resistance of their productivity to climate extremes.[184]\n In 2019, the UN's Food and Agriculture Organization (FAO) produced its first report on The State of the World's Biodiversity for Food and Agriculture. It warned that \"Many key components of biodiversity for food and agriculture at genetic, species and ecosystem levels are in decline.\"[185][186]\n The report also said, \"Many of the drivers that have negative impacts on BFA (biodiversity for food and agriculture), including overexploitation, overharvesting, pollution, overuse of external inputs, and changes in land and water management, are at least partially caused by inappropriate agricultural practices\"[187]:\u200a6\u200a and \"transition to intensive production of a reduced number of species, breeds and varieties, remain major drivers of loss of BFA and ecosystem services.\"[187]:\u200a6\u200a\n To reduce biodiversity loss related to agricultural practices, FAO encourages the use of \"biodiversity-friendly management practices in crop and livestock production, forestry, fisheries and aquaculture\".[187]:\u200a13\u200a\n The WHO has analyzed how biodiversity and human health are connected: \"Biodiversity and human health, and the respective policies and activities, are interlinked in various ways. First, biodiversity gives rise to health benefits. For example, the variety of species and genotypes provide nutrients and medicines.\"[188] The ongoing drivers and effects of biodiversity loss has the potential to lead to future zoonotic disease outbreaks like the COVID-19 pandemic.[189]\n Medicinal and aromatic plants are widely used in traditional medicine as well as in cosmetic and food industries.[188]:\u200a12\u200a The WHO estimated in 2015 that about \"60,000 species are used for their medicinal, nutritional and aromatic properties\".[188]:\u200a12\u200a There is a global trade in plants for medicinal purposes.[188]:\u200a12\u200a\n Biodiversity contributes to the development of pharmaceuticals.  A significant proportion of medicines are derived from natural products, either directly or indirectly. Many of these natural products come from marine ecosystems.[190] However, unregulated and inappropriate over-harvesting (bioprospecting) could potentially lead to overexploitation, ecosystem degradation and loss of biodiversity.[191][192] Users and traders harvest plants for traditional medicine either by planting them or by collecting them in the wild. In both cases, sustainable medicinal resource management is important.[188]:\u200a13\u200a\n Scientists are investigating what can be done to address biodiversity loss and climate change together. For both of these crises, there is a need to \"conserve enough nature and in the right places\".[194] A 2020 study found that \"beyond the 15% land area currently protected, 35% of land area is needed to conserve additional sites of particular importance for biodiversity and stabilize the climate.\"[194]\n Additional measures for protecting biodiversity, beyond just environmental protection, are important. Such measures include addressing drivers of land use change, increasing efficiency in agriculture, and reducing the need for animal agriculture. The latter could be achieved by increasing the shares of plant-based diets.[195][196]\n Many governments have conserved portions of their territories under the Convention on Biological Diversity (CBD), a multilateral treaty signed in 1992\u20133. The 20 Aichi Biodiversity Targets are part of the CBD's Strategic Plan 2011\u20132020 and were published in 2010.[197] Aichi Target Number 11 aimed to protect 17% of terrestrial and inland water areas and 10% of coastal and marine areas by 2020 .[198]\n Of the 20 biodiversity goals laid out by the Aichi Biodiversity Targets in 2010, only six were partially achieved by 2020.[23][24] The 2020 CBD report highlighted that if the status quo does not change, biodiversity will continue to decline due to \"currently unsustainable patterns of production and consumption, population growth and technological developments\".[199][200] The report also singled out Australia, Brazil, Cameroon and the Galapagos Islands (Ecuador) for having had one of its animals lost to extinction in the previous ten years.[201]\n Following this, the leaders of 64 nations and the European Union pledged to halt environmental degradation and restore the natural world. The pledge was not signed by leaders from some of the world's biggest polluters, namely China, India, Russia, Brazil and the United States.[202] Some experts contend that the United States' refusal to ratify the Convention on Biological Diversity is harming global efforts to halt the extinction crisis.[203]\n Scientists say that even if the targets for 2020 had been met, no substantial reduction of extinction rates would likely have resulted.[150][1] Others have raised concerns that the Convention on Biological Diversity does not go far enough, and argue the goal should be zero extinctions by 2050, along with cutting the impact of unsustainable food production on nature by half. That the targets are not legally binding has also been subject to criticism.[204]\n In December 2022, every country except the United States and the Holy See[205] signed onto the Kunming-Montreal Global Biodiversity Framework at the 2022 United Nations Biodiversity Conference. This framework calls for protecting 30% of land and oceans by 2030 (30 by 30). It also has 22 other targets intended to reduce biodiversity loss. At the time of signing the agreement, only 17% of land territory and 10% of ocean territory were protected. The agreement includes protecting the rights of Indigenous peoples and changing the current subsidy policy to one better for biodiversity protection, but it takes a step backward in protecting species from extinction in comparison to the Aichi Targets.[206][207] Critics said the agreement does not go far enough to protect biodiversity, and that the process was rushed.[206]\n In 2019 the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES) published the Global Assessment Report on Biodiversity and Ecosystem Services. This report said that up to a million plant and animal species are facing extinction because of human activity.[8] The IPBES is an international organization that has a similar role to the Intergovernmental Panel on Climate Change (IPCC),[208] except that it focuses on biodiversity and ecosystem services, not climate change.\n The United Nations' Sustainable Development Goal 15 (SDG 15), \"Life on Land\", includes biodiversity targets. Its fifth target is: \"Take urgent and significant action to reduce the degradation of natural habitats, halt the loss of biodiversity and, by 2020, protect and prevent the extinction of threatened species.\"[209] This target has one indicator: the Red List Index.[210]\n Nearly three-quarters of bird species, two thirds of mammals and more than half of hard corals have been recorded at World Heritage Sites, even though they cover less than 1% of the planet. Countries with World Heritage Sites can include them in their national biodiversity strategies and action plans.[211][212]\n"
    },
    {
        "title": "Network affiliate",
        "url": "https://en.wikipedia.org/wiki/Network_affiliate",
        "content": "In the broadcasting industry (particularly in North America, and even more in the United States), a network affiliate or affiliated station is a local broadcaster, owned by a company other than the owner of the network, which carries some or all of the lineup of television programs or radio programs of a television or radio network. This distinguishes such a television or radio station from an owned-and-operated station (O&O), which is owned by the parent network.\n Notwithstanding this distinction, it is common in informal speech (even for networks or O&Os themselves) to refer to any station, O&O or otherwise, that carries a particular network's programming as an affiliate, or to refer to the status of carrying such programming in a given market as an \"affiliation\".\n Stations which carry a network's programming by method of affiliation maintain a contractual agreement, which may allow the network to dictate certain requirements that a station must agree to as part of the contract (such as programming clearances, local programming quotas or reverse compensation of a share of a station's retransmission consent revenue to the network). Affiliation contracts normally last between three and five years, though contracts have run for as little as one year or as long as ten; in addition, if a company owns two or more stations affiliated with the same network, affiliation contracts may have end-of-term dates that are the same or differ among that company's affiliates, depending on when a particular station's affiliation agreement was either previously renewed or originally signed.\n While many television and radio stations maintain affiliations with the same network for decades, on occasion, there are certain factors that may lead a network to move its programming to another station (such as the owner of a network purchasing a station other than that which the network is already affiliated with, the network choosing to affiliate with another local station in order to improve local viewership of its programming by aligning with a stronger station, or a dispute between a network and station owner while negotiating a contract renewal for a particular station such as those over reverse compensation shares), often at the end of one network's existing contract with a station. One of the most notable and expansive affiliation changes occurred in the United States from September 1994 to September 1996, when television stations in 30 markets changed affiliations (through both direct swaps involving the new and original affiliates, and transactions involving multiple stations) as a result of a May 1994 agreement by New World Communications to switch twelve of its stations to Fox,[1] resulting in various other affiliation transactions including additional groupwide deals (such as those between ABC and the E. W. Scripps Company, and CBS and Westinghouse Broadcasting).\n In the United States, Federal Communications Commission (FCC) regulations limit the number of network-owned stations as a percentage of total national market reach. As such, networks tend to have O&Os only in the largest media markets (such as New York City and Los Angeles), and rely on affiliates to carry their programming in other, smaller markets. However, even the largest markets may have network affiliates in lieu of O&Os. For instance, Mission Broadcasting's WPIX serves as the New York City affiliate of The CW (which is 75% owned by station operator Nexstar Media Group, with Warner Bros. Discovery and Paramount Global each owning 12.5% stakes), while Paramount owns independent station WLNY-TV in that market. On the other hand, several other television stations in the same market \u2013 WABC-TV (ABC), WCBS-TV (CBS), WNBC (NBC), WNJU (Telemundo), WNYW (Fox), WWOR-TV (MyNetworkTV), WPXN-TV (Ion Television), WXTV-DT (Univision) and WFUT-DT (UniM\u00e1s) \u2013 are O&Os.\n A similar rule exists in Japan, in which regulations governed by the Ministry of Internal Affairs and Communications limit the number of network-owned commercial television stations as a percentage of total national market reach. As such, commercial networks tend to have O&Os only in the four largest media markets (Kant\u014d, Keihanshin, Ch\u016bky\u014d, and Fukuoka), and rely on affiliates to carry their programming in other prefectures. However, there are two major exceptions to the regulations. NHK is a government-owned, non-commercial television network and, since it is not covered by the ownership cap, owns and operates all of its stations. TXN Network is also not covered by the ownership cap due to the network's low number of affiliates (which are all owned by the network).\n In Brazil, government regulations limit the number of owned-and-operated stations that a television network can own based on the percentage of total national market reach. As a result, the five main national networks tend to have O&Os only in large metropolitan areas such as S\u00e3o Paulo, Rio de Janeiro and Belo Horizonte, and mainly rely on affiliates to carry their programming to other areas of the country. The metropolitan areas of Recife and Manaus are examples of those who have both O&Os and affiliates. For instance, TV Globo and RedeTV! have O&Os in Recife, but Record, SBT and Band do not. TV Cultura, Rede Brasil and TV Gazeta only have one owned-and-operated station each; those networks are smaller than the five major networks by market reach.\n In Canada, the Canadian Radio-Television and Telecommunications Commission (CRTC) has significantly more lenient rules regarding media ownership. As such, most television stations, regardless of market size, are now O&Os of their respective networks, with only a few true affiliates remaining (mainly located in smaller cities). The Canadian Broadcasting Corporation originally relied on a large number of privately owned affiliates to disseminate its radio and television programming. However, since the 1960s, most of the CBC Television affiliates have become network owned-and-operated stations or retransmitters. CBC Radio stations are now entirely O&O.\n While network-owned stations will normally carry the full programming schedule of the originating network (save for major local events), an affiliate is independently owned and typically under no obligation to do so.  This is especially the case for network shows airing outside the network's primetime hours.  Affiliated stations often buy supplementary programming from another source, such as a broadcast syndication service, or another television network which otherwise does not have coverage in the station's broadcast area. Some affiliates may air such programs instead of those from their primary network affiliation; a common example of this was the popular syndicated science fiction drama series Star Trek: The Next Generation (1987\u20131994).[2][3]:\u200a124\u200a  Some network affiliates may also choose to air season games involving local sports teams in lieu of network programming.\n A handful of networks, such as the U.S.-based Public Broadcasting Service (PBS) public television and National Public Radio (NPR), have been founded on a principle which effectively reverses the commercial broadcasting owned-and-operated station model and is called a state network. Instead of television networks owning stations, the stations collectively own the network and brand themselves as \"member stations\" or \"member networks\" instead of as affiliates or O&Os.\n Individual stations such as WPBS-TV (in Watertown, New York) and KPBS (in San Diego, California) are not allowed to be owned by the Public Broadcasting Service; most belong to local community non-profit groups, universities or local and state educational organizations. The national PBS system is owned collectively by hundreds of broadcasters in communities nationwide.[4] Individual member stations are free to carry large amounts of syndicated programming and many produce their own educational or edutainment content for distribution to other PBS member stations through services like American Public Television or the National Educational Television Association; likewise, most content on PBS's core national programming service is produced by various individual member stations such as WGBH-TV, WNET and WETA-TV. These are not affiliate stations in that the ownership of the main network is not independent of ownership of the individual local stations.\n Unlike the modern-day affiliation model with commercial stations, in which network programming is only shared between the main station in a given market and any repeaters it may operate to extend its coverage, PBS is not beholden to exclusive programming agreements with stations in the same metropolitan area. In some markets, the network maintains memberships with two noncommercial educational stations \u2013 in some cases, these are owned by the same entity \u2013 which split the programming rights. To avoid programming conflicts, the network utilizes a Program Differentiation Plan to assign programming quotas in these situations, resulting in the primary member station carrying more PBS-distributed programming than the secondary member; the number of two-to-a-market PBS members (not counting repeaters of the market's main PBS outlet) has been steadily decreasing since the early 2000s, with few remaining outside larger markets.\n The \"member station\" model had historically been used in Canada in the early days of privately owned networks CTV and TVA, but the original \"one station, one vote\" model has largely faltered as increasing numbers of stations are acquired by the same owners. In CTV's case, the systematic pattern of acquisition of CTV member stations by the owners of CFTO-TV in Toronto ultimately allowed control over the network as a whole, turning former member stations into CTV O&Os.[5]\n In some smaller markets in the United States, a station may even be simultaneously listed as an affiliate of two (or in rare cases, three) networks. A station which has a dual affiliation is typically expected to air all or most of both networks' core day time and/or prime time schedules \u2013 although programming from a station's secondary affiliation normally airs outside its usual network time slot, and some less popular programs may simply be left off of a station's schedule; this form of dual affiliation was the norm before the digital age. Dual affiliations are most commonly associated with the smaller American television networks, such as The CW and MyNetworkTV, which air fewer hours of prime time programming than the \"Big Four\" networks and can therefore be more easily combined into a single schedule, although historically the \"Big Four\" have had some dual-affiliate stations in small markets as well and in some cases, affiliates of more than two networks (including a few that had affiliations with ABC, NBC, CBS and DuMont during the late 1940s through the mid-1950s, when fewer television stations existed in a particular market, especially those that would eventually be able to support four commercial outlets).\n Historically, the sole commercial station in a market would commonly take affiliations or secondary affiliations from most or all of the major national networks. As a local monopoly, a station could become a primary affiliate of one of the stronger networks, carrying most of that network's programming while remaining free to \"cherry-pick\" popular programming from any or all of the rival networks. Similarly, some markets that had two commercial stations shared a secondary affiliation with one network, while maintaining separate primary affiliations (such as in the Ada, Oklahoma-Sherman, Texas market, where until 1985, KTEN and KXII shared secondary affiliations with NBC, while the former was primarily affiliated with ABC and the latter with CBS; the former station is now a primary NBC affiliate).\n As U.S.-marketed television receivers have been required to include factory-installed UHF tuners since 1964, the rapid expansion of broadcast television onto UHF channels in the 1970s and 1980s (along with increased deployment of cable and satellite television systems) has significantly reduced the number of one-station markets (limiting them to those with population densities too small to be able to make any additional stations economically viable), providing networks with a larger selection of stations as potential primary affiliates. A new station which could clear one network's entire programming lineup better serves the network's interests than the former pattern of partial access afforded by mixing various secondary affiliations on the schedule of a single local analog channel.\n In 2009, after many years of decline, the era of secondary affiliations to multiple major networks (once common in communities where fewer stations existed than networks seeking carriage) finally came to an end at the smallest-market U.S. station, KXGN-TV in Glendive, Montana (which was affiliated with both CBS and NBC). The digital conversion allowed KXGN to carry CBS and NBC programming side-by-side on separate subchannels, essentially becoming a primary affiliate of both networks. This is the most common type of \"dual affiliation\" existing today in the digital TV age.\n In larger markets, multiple full-service channels may be operated by the same broadcaster using broadcast automation, either openly as duopoly or twinstick operations, or through the use of local marketing agreements and shared services agreements to operate a second station nominally owned by another broadcaster. These may be supplemented by LPTV or repeater stations to allow more channels to be added without encountering federally imposed limits on concentration of media ownership. Often, the multiple commonly controlled stations will use the same news and local advertising sales operations, but carry different network feeds.\n Further, with the ability of digital television stations to offer a distinct programming stream on a digital subchannel, traditional dual affiliation arrangements in which programming from two networks is combined into a single schedule are becoming more rare. KEYC-TV in Mankato, Minnesota is one such example, carrying CBS programming on its 12.1 subchannel and Fox on 12.2. KEYC's Watertown, New York sister station WWNY-TV follows this same pattern (CBS on 7.1 and Fox on 7.2), but supplements this with a 15kW low-power station broadcasting in high definition on the same transmitter tower under the control of the same owners, using the same studios to provide a second high definition channel for the Fox affiliate.\n One notable exception to the survival of secondary affiliations are stations owned by West Virginia Media Holdings. WTRF-DT2 in Wheeling and WVNS-DT2 in Beckley, West Virginia both had Fox as their primary affiliation and MyNetworkTV as a secondary affiliation. Until WTRF lost its Fox affiliation in 2014 to NBC affiliate WTOV-TV (leaving WTRF-DT2 with MyNetworkTV and WVNS as the only one with affiliations from both), each network was carried on the second digital subchannel of WTRF-TV and WVNS-TV, respectively, both of which carry CBS programming on their main signals. Another example is WBKB-TV in Alpena, Michigan, owned by The Marks Group, which also carries CBS programming on its main signal and both Fox and MyNetworkTV on its second digital subchannel. In addition, however, WBKB-TV also has an ABC affiliate on WBKB-DT3, giving the station four different network affiliations between three subchannels.\n In Canada, affiliated stations may acquire broadcast rights to programs from a network other than their primary affiliation, but as such an agreement pertains only to a few specific programs, which are chosen individually, they are not normally considered to be affiliated with the second network. CJON-DT in St. John's, Newfoundland, nominally an independent station, uses this model to acquire programming from CTV and the Global Television Network. CJNT-DT in Montreal formerly maintained dual affiliations through both City and Omni Television to satisfy its ethnic programming requirements due to its sale to Rogers Media in 2012. This model eventually ceased as Rogers' was granted a request by the CRTC in late 2012 to change the station's format from a multicultural station to a conventional English-language station, and contribute funding and programming to a new independent multicultural station, CFHD-DT, which signed on in 2013.[6][7]\n This was also done by MyNetworkTV in the 2009\u201310 season in Des Moines, Iowa and Memphis, Tennessee after it lost their individual affiliates in those markets to other networks as it offered the network's last season of WWE Friday Night Smackdown to the local CW affiliates in both cities without forcing them to carry the remainder of MyNetworkTV's schedule.\n From September 1, 2016, to August 31, 2019, the largest current-day market example of a dual affiliation was with Fox Television Stations's WPWR-TV, a Gary, Indiana-licensed station serving the entire Chicago market, which carried a primary affiliation with The CW, while maintaining Fox's MyNetworkTV programming service in a late night timeslot.[citation needed] Beginning on September 1, 2019, The CW affiliation of WPWR-TV was changed to WCIU-TV.[8]\n"
    },
    {
        "title": "Women in Asia",
        "url": "https://en.wikipedia.org/wiki/Women_in_Asia",
        "content": "The evolution and history of women in Asia coincide with the evolution and history of Asian continent itself. They also correspond with the cultures that developed within the region. Asian women can be categorically grouped as women from the Asian subregions of Central Asia, East Asia, North Asia, South Asia, Southeast Asia, and Western Asia (aka The Middle East).\n Due to the patriarchal nature of traditional Armenian culture and society,[2] women in Armenia are often expected to be virtuous and submissive, to safeguard their virginity until marriage, and assume primarily domestic tasks.\n Traditional social norms and lagging economic development in Azerbaijan's rural regions continue to restrict women's roles in society and the economy, and there were reports that women had difficulty exercising their legal rights due to gender discrimination.[3]\n Women in Cambodia, sometimes referred to as Khmer women, are supposed to be modest, soft-spoken, \"light\" walkers, well-mannered,[4] industrious,[5] belong to the household, act as the family's caregivers and caretakers[4] and financial comptrollers,[5] perform as the \"preserver of the home\", maintain their virginity until marriage, become faithful wives,[4] and act as advisors and servants to their husbands.[5] The \"light\" walking and refinement of Cambodian women is further described as being \"quiet in [...] movements that one cannot hear the sound of their silk skirt rustling\".[5]\n Throughout the history of Persia, Persian women (presently known as women in Iran), like Persian men, used make-up, wore jewellery and coloured their body parts. Moreover, their garments were both elaborate and colorful. Rather than being marked by gender, clothing styles were distinguished by class and status.[6] Women in modern Iran (post 1935 \"Persia\") are of various mixes and appearances, both in fashion[7] and social norm.[8] Traditionally however, the \"Persian woman\" had a pre-defined appearance set by social norms that were the standard for all women in society.[9]\n Women in Kyrgyzstan traditionally had assigned roles, although only the religious elite sequestered women as was done in other Muslim societies.[10]\n Historically, women in Burma (Myanmar) have had a unique social status in Burmese society. According to the research made by Daw Mya Sein, Burmese women \"for centuries \u2013 even before recorded history\" owned a \"high measure of independence\" and had retained their \"legal and economic rights\" despite the influences of Buddhism and Hinduism. Burma once had a matriarchal system that includes the exclusive right to inherit oil wells and the right to inherit the position as village head. Burmese women were also appointed to high offices by Burmese kings, can become chieftainesses and queens.[11]\n Palestinian women were not expected to secure income for the family, but women were expected to adapt to the customary roles of women in Palestinian society wherein females were traditionally molded as inferior to men.[12]\n The role of women in Turkmenistan has never conformed to Western stereotypes about Muslim women.[13] Although a division of labor exists and women usually are not visible actors in political affairs outside the home, Turkmen women have never worn a veil similar to that of the women of some of its neighboring countries.[13] As Turkmenistan is a tribal nation, customs regarding women can vary within the country: for example, women in the eastern part of the country are permitted to drink some alcohol whereas women who live in the central portion of the country, particularly those of the Tekke tribe, are not permitted to imbibe alcohol.  Most women possess a host of highly specialized skills and crafts, especially those connected with the household and its maintenance.[13]\n Women in Azerbaijan nominally enjoy the same legal rights as men; however, societal discrimination is a problem.[3] Universal suffrage was introduced in Azerbaijan in 1918 by the Azerbaijan Democratic Republic, thus making Azerbaijan the first Muslim country ever to enfranchise women.[14]  Most Bahraini women are also well represented in all of the major professions, women's societies, and women's organizations.  Apart from having the right to vote, around one-quarter of the women of Bahrain are able to hold jobs outside the confines of the household.[15]\n Available data on health, nutrition, education, and economic performance indicated that in the 2014 women participation in the workforce was 57%.[16] Bangladesh has a Gender Development Index of .917.[17]\n As financial controllers, the women of Cambodia can be identified as having real household authority at the familial level.[18] In recent years, women have become more active in the traditionally male-dominated spheres of work and politics in Cambodia.\n October 1, 1949 marks the formal establishment of the People's Republic of China.[19] Since 1949, the government of the People's Republic of China has actively promoted the cultural, social, economic and political roles of women in order to improve women's liberation. The new government of the People's Republic made a commitment to achieve equality between women and men.[20]  While advancing towards equality among men and women, the efforts met resistance in a traditionally Confucian society of male superiority.\n Although equality among men and women has been a long-term goal of the People's Republic of China, the dramatic reformations that followed the Cultural Revolution (1966\u20131976) have inconsistently affected women's empowerment and status in China.[20] Studies shows that Chinese women experienced rapid progress in terms of gender equality during the Cultural Revolution.[20] When the People's Republic of China was established, employed women accounted for only 7 percent of the workforce; whereas in 1992 women's participation in the workforce had increased to account for 38 percent.[21]\nWomen's representation in higher educational institutions has also increased since the establishment of the People's Republic of China.[20] Under the traditional Chinese patriarchy structure, the society was male-dominated, and women in Hong Kong had a relatively subordinate familial role.[22] However, there is a cultural change  in Hong Kong during the British colonial period with an emergence of Western culture (i.e. \"Westernization\"). A mix of traditional Chinese culture and Western values creates a unique culture of Hong Kong. Along with the rapid economic and social development of Hong Kong since the end of the Second World War, a significant improvement in the role of men has been witnessed, while female dominance society structure is still taking in place.[23] Hence, women studies in Hong Kong are slightly differ from China's. Women in Hong Kong are generally more independent, monetarily autonomous, assertive, and career-focused; which makes them seem to be more prominent when comparing with women in some other Southeast Asian countries.[23] With the increase number of women in professional and managerial positions in recent decades, the terms of \"female strong person\" or \"superwomen\" are being used to describe women in Hong Kong.[23] Candice Chio Ngan Ieng, president of the Macau Women's General Association (AGMM), describes in 2010 that women are currently defining themselves as capable and irreplaceable powers to Macau's modern-day civilization.  This change is happening despite the slowness in the Macanese people's absorption of the ideological concept of gender equality.[24]\n The status of women in India has been subject to many great changes over the past few millennia.[25] From equal status with men in ancient times[26] through the low points of the medieval period,[27] to the promotion of equal rights by many reformers, the history of women in India has been eventful.\n The roles of Indonesian women today are being affected by many factors, including increased modernization, globalization, improved education and advances in technology (in particular communications technology). Many women in Indonesia choose to reside in cities instead of staying in townships to perform agricultural work because of personal, professional, and family-related necessities, and economic requirements. These women are moving away from the traditional dictates of Indonesian culture, wherein women act simply and solely as wives and mothers. At present, the women of Indonesia are also venturing actively into the realm of national development, and working as active members of organizations that focus and act on women's issues and concerns.[28][29]\n The Iranian women's movement is based on the Iranian women's social movement for women's rights. This movement first emerged some time after the Iranian Constitutional Revolution in 1910, the year in which the first Women Journal was published by women. The movement lasted until 1933 in which the last women's association was dissolved by the Reza Shah Pahlavi\u2019s government. It heightened again after the Iranian Revolution (1979).[30][31] Between 1962 and 1978, the Iranian women's movement gained tremendous victories: women won the right to vote in 1963 as part of Mohammad Reza Shah's White Revolution, and were allowed to stand for public office, and in 1975 the Family Protection Law provided new rights for women, including expanded divorce and custody rights and reduced polygamy.[32] Following the 1979 Revolution, several laws based on gender discrimination were established such as the introduction of mandatory veiling and public dress code of females.[33] Women's rights since the Islamic Revolution has varied. About 9% of the Iranian parliament members are women, while the global average is 13%.[34] Following the Revolution, women were allowed to join the police and military forces.\n The women's rights movement in Iran continues to attempt influencing reforms, particularly with the One Million Signatures Campaign to End Discrimination Against Women.[35]\n Women in the country went through a difficult period in the 1990s, when Kazakhstan's economy, being in a period of transition, experienced a strong decline and destabilization: by 1995 real GDP dropped to 61,4% of its 1990 level, resulting also in a brain drain.[36][37] Nevertheless, the 1990s also had some positives for women, such as the accession to the Convention on the Elimination of All Forms of Discrimination Against Women in 1998.[38]\n Women in Kuwait are considered to be among the most emancipated women in the Gulf region.[39][40] Women in Kuwait can travel, drive, and work without their fathers' or husbands' consent and they even hold senior government positions.[39] Women in Kuwait are able to work freely and can achieve positions of power and influence.[41]\n Women in Kuwait gained the right to vote and stand in parliamentary and local elections in May 2005.[42][43] And in October 2009 Kuwait's constitutional court ruled that women were able to gain their own passports, without the consent of their husbands.[44]\n Because of the demands of the nomadic economy, women in Kyrgyzstan worked as virtual equals with men, having responsibility for chores such as milking as well as child-rearing and the preparation and storage of food.[10] In the ordinary family, women enjoyed approximately equal status with their husbands, within their traditional roles.[10]\n Laotian women have long been active participants in their nation's society, involved in politics, driving social transformation and development, becoming active in the world of business and serving as nurses and food producers for the military. Due to modernization and rural uprooting, Lao women have begun to embrace lifestyles that are foreign to traditional Laotian ideals.[45]\n Due to the large number of officially recognized religions in Lebanon, Lebanese family matters are governed by at least 15 [46] personal statute codes. Lebanese women have legal protection that varies depending on their religion.[47]  Marriageable age can be as young as 12.5, polygamy is allowed if the male of the family is Muslim, parental authority belongs to the patriarch of the house and legal guardian of all children, and female children receive less inheritance than a male child would.[46] Children born to a Lebanese woman and a man from another country will not have their children granted Lebanese nationality.[48]\n Women in Malaysia receives support from the Malaysian government concerning their rights to advance, to make decisions, to health, education and social welfare, and to the removal of legal obstacles. The Malaysian government has ensured these factors through the establishment of Ministry of National Unity and Social Development in 1997 (formerly known in 1993 as Women's Affairs Secretariat or HAWA).  This was followed by the formation of the Women's Affairs Ministry in 2001 to recognize the roles and contributions of Malaysian women. Around 47% of Malaysian women are in the workforce.[49]\n Women now pursue careers and professional training in Oman, slowly moving from their previous household confinement to the public sphere.[50] In Oman, 17 October is celebrated every year as the Omani Women's Day with various pro-female events.[51]\n The Pakistani women of today enjoy a better status than most Muslim women. However, on an average, the women's situation vis-\u00e0-vis men is one of systemic gender subordination,[52] although there have been attempts by the government and enlightened groups to elevate the status of women in Pakistani society.[53] Now due to much awareness among people the educational opportunities for the Pakistani women increased in the previous years.[54] According to a Human Development Report released by the United Nations, Pakistan has better gender equality than neighbouring India.[55][56] However, in 2012, the World Economic Forum ranked the gender gap in Pakistan, Chad, and Yemen as the worst in their Global Gender Gap Report.[57]\n Although they generally define themselves in the milieu of a masculine dominated post-colonial Asian Catholic society, Filipino women live in a culture that is focused on the community, with the family as the main unit of society.  It is in this framework of Philippine hierarchical structure, class differences, religious justifications, and living in a globally developing nation that Filipino women struggle for respect. Compared to other parts of Southeast Asia, women in Philippine society have always enjoyed a greater share of legal equality.[58][59][60][61]\n All women, regardless of age, are required to have a male guardian in Saudi Arabia. The World Economic Forum 2009 Global Gender Gap Report ranked Saudi Arabia 130th out of 134 countries for gender parity. It was the only country to score a zero in the category of political empowerment. The report also noted that Saudi Arabia is one of the few Middle Eastern countries to improve from 2008, with small gains in economic opportunity.[62]\n 21% of Saudi women are in the workforce and make up 16.5% of the overall workforce.[63][64]\n There is evidence that some women in Saudi Arabia do not want change. Even many advocates of reform reject Western critics, for \"failing to understand the uniqueness of Saudi society.\"[65][66][67]\nJournalist Maha Akeel is a frequent critic of her country's patriarchal customs. Nonetheless, she agrees that Westerners criticize what they do not understand.  She has said: \"Look, we are not asking for\u00a0... women's rights according to Western values or lifestyles\u00a0... We want things according to what Islam says. Look at our history, our role models.\"[68]\n Women in Singapore, particularly those who have joined Singapore's workforce, are faced with balancing their traditional and modern-day roles in Singaporean society and economy. According to the book The Three Paradoxes: Working Women in Singapore written by Jean Lee S.K., Kathleen Campbell, and Audrey Chia, there are \"three paradoxes\" confronting and challenging the career women of Singapore.  Firstly, Singapore's society expects women to become creative and prolific corporate workers who are also expected to play the role of traditional women in the household, particularly as wife and mother.  Secondly, Singaporean women are confronted by the \"conflict between work and family\" resulting from their becoming members of the working population.  Thirdly, Singapore's female managers are still fewer in number despite their rising educational level and attainments when compared to male managers.[69]\n Syria Comment described that Syrian women have been able to acquire  several rights that have not been granted to their counterparts in other Arab nations.  Such rights include the custody of children aged 15 years old or younger; and the right to give their nationality to their offspring whose father is not a national of Syria.[70] A common attire of women, particularly in Damascus, are Western clothing that includes long skirts, pants, jeans, high-heeled shoes, in addition to the sporting of the hijab and the monteau (a type of coat), sometimes accented by a \u201ccoordinating purse\u201d.[70]\n The role of women in the United Arab Emirates has advanced greatly in recent years, making the UAE a leader in women's rights in the Arab world.  Though there were few opportunities for women outside the home before 1960, the discovery of oil led to advancement in women's position. The UAE constitution guarantees equality between men and women in areas including legal status, claiming of titles, and access to education. The General Women's Union (GWU), established by HH Sheikha Fatima bint Mubarak wife of then President Sheikh Zayed bin Sultan Al Nahyan, remains a strong component of the State's and participating organizations' various initiatives. In the 2007/2008 United Nations Development Programme report, the UAE ranked 29th among 177 countries in the Gender Empowerment Measures, the best rating received in the Arab World.[71] UNDP's Millennium Development Goal No. 3, to \u201cPromote Gender Equality and Empower Women\u201d has reached its targeted levels of female participation in primary education and continues to increase.[72]\n Women in Yemen have historically had much less power in society than men. Although the government of Yemen has made efforts that will improve the rights of women in Yemen (including the formation of a Women's Development Strategy and a Women Health Development Strategy),[73] many cultural and religious norms, along with poor enforcement of this legislation from the Yemeni government, have prevented Yemeni women from having equal rights to men.\n Today, Yemeni women do not hold many economic, social or cultural rights. Even more striking is the reality that while suffrage was gained in 1967 and constitutional and legal protection was extended to women during the first years of Yemen unity between 1990 and 1994, they continue to struggle \u201cin exercising their full political and civil rights\u201d.[74] History shows that women have played major roles in Yemeni society. Some women of pre-Islamic and early Islamic Yemen held elite status in society. The Queen of Sheba, for example, \u201cis a source of pride for the Yemeni nation\u201d.[74] In addition, Queen Arwa has been noted for her attention to infrastructure, which added to a documented time of prosperity under her rule.[74] Modern day women of Yemen, however, are subject to a society that reflects largely agrarian, tribal, and patriarchal traditions. This, combined with illiteracy and economic issues has led women to continuously be deprived of their rights as citizens of Yemen.\n The roles of women in Thailand's national development has not yet been fully established. Factors that affect women's participation in the socio-economic field include \"inadequate gender awareness in the policy and planning process\" and social stereotyping.[75]\n During the Soviet period, women in Turkmenistan assumed responsibility for the observance of some Muslim rites to protect their husbands' careers.[13] Many women entered the work force out of economic necessity, a factor that disrupted some traditional family practices and increased the incidence of divorce.[13] At the same time, educated urban women entered professional services and careers.[13] The social and legal situation of women in Uzbekistan has been influenced by local traditions, religion, the Soviet rule, and changing social norms since independence.[76]\n Women in Vietnam played a significant role in defending Vietnam during the Vietnam War from 1945 to 1975. They took roles such as village patrol guards, intelligence agents, propagandists, and military recruiters. By becoming \"active participants\" in the struggle to liberate their country from foreign occupation, Vietnamese women were able to free themselves from \"centuries of Confucian influence that had made them second-class citizens\".[77] Historically, this character and spirit of Vietnamese women were first exemplified by the conduct of the Trung sisters, the \u201cfirst historical figures\u201d in the history of Vietnam who revolted against Chinese control. These \"women warriors\" are not only fighting to protect their families, but also to preserve their culture. Trung Trac, the elder Trung sister, sacrificed herself and drowned over surrendering to foreign conquerors.[78] This trait is also epitomized in the old Vietnamese adage: \"When war comes, even women have to fight\",[77] and its variation: \"When the enemy is at the gate, the woman goes out fighting\".\"Ba Me Anh Hung\" (Hero Mothers), which is a term that has been coined recently, illustrates that even though Vietnamese women did not contribute greatly in combat, they were heroes that took care of children and overcame poverty. While the men were away in war, these women worked domestically to build up the next generation and the future of the country.[79] Vietnamese women's roles in society would in turn develop Vietnam's socioeconomic status. \"Women are laborers, citizens, mothers, and the first teachers of people...\"[80]  As of 2007, several women in Azerbaijan held senior government positions, including deputy speaker of parliament, several deputy ministers, and deputy chair of the Central Election Commission.[3] There are no legal restrictions on the participation of women in politics. As of 2011, there were 19 women in the 125-seat parliament. The percentage of female members of parliament increased from 11 to 16 percent between 2005 and 2010.[3][81][82]\n Bangladesh as of 2015 has a female speaker of parliament and Prime Minister.[83]  In modern India, women have held high offices in India including that of the President, Prime Minister, Speaker of the Lok Sabha and\nLeader of the Opposition. As of 2011[update], the Speaker of the Lok Sabha and the Leader of the Opposition in the Lok Sabha (Lower House of the parliament) were women.\n Women cannot vote or be elected to high political positions in Saudi Arabia.[84] However, King Abdullah has declared that women will be able to vote and run in the 2015 local elections, and be appointed to the Consultative Assembly.[85]\n Women in Taiwan have especially achieved prominent roles in politics.[86] In 2000, feminist movement advocator Annette Lu elected as first female vice president while Yeh Chu-lan was promoted to be the first female vice premier in 2004. In 2016, Tsai Ing-wen was elected as president of Taiwan. Nevertheless, as in other parts of East Asia, sex-selective abortion is reported to happen in Taiwan.[87][88]\n[89]\n Women in Thailand were among the first women in Asia who were granted the right to vote in 1932.  However, they are still underrepresented in Thai politics.[90][91]\n Violence against women in Afghanistan is high, although the situation is improving slowly as the country progresses with the help of the international community.[92]\n Bride kidnapping occurs in Azerbaijan.[93]  In the Azeri kidnap custom, a young woman is taken to the home of the abductor's parents through either deceit or force. Regardless of whether rape occurs or not, the woman is generally regarded as impure by her relatives, and is therefore forced to marry her abductor.[94]\n Women in India continue to face atrocities such as rape, acid throwing, dowry killings while young girls are forced into prostitution; as of late rape has seen a sharp increase following several high-profile cases of young girls brutally raped in public areas.[95][96][97] According to a global poll conducted by Thomson Reuters, India is the \"fourth most dangerous country\" in the world for women,[98][99] and the worst country for women among the G20 countries.[100]\n Societal discrimination and domestic violence against women has been identified as a significant problem, particularly in the Israeli Bedouin society.[101]\n In the 21st century, the issue of violence against women in Kazakhstan has come to public attention, resulting in the Law on the Prevention of Domestic Violence of 2009.[102] However, as in other parts of Central Asia, bride kidnapping remains a problem.[103][104]\n Local and regional NGOs have helped to increase awareness of violence against women in Lebanon.[105][106] Government policies regarding this are poor however, and attempts to improve this area have been met with resistance.[107] Lebanon's laws do not recognize the concept of spousal rape,[46] and attempt to add this to law have been attacked by Lebanese clerics.[108]\n Pakistani women face atrocities like rape, acid throwing, honour killings, forced marriages, forced prostitution and the buying and selling of women.[109] The past few years have been witness to a steep increase in such crimes.[109]\n Although Uzbek law provides some safeguards for the security of women in the country, women continue to face numerous problems, especially violence. There are reports that forced sterilization of women is practiced in Uzbekistan.[110][111][112] A BBC World Service \"Assignment\" report on 12 April 2012 uncovered evidence that women are being sterilized, often without their knowledge, in an effort by the government to control the population.[113]\n Armenia is one of the countries in Asia which faces the issue of sex-selective abortion.[114][115][116][117][118] Reports of female infanticide following the execution of the One-child policy indicated the persistence of women's low status in China.[20]\n Notable women from Asia include Qiu Jin (China), Trieu Thi Trinh (Vietnam), Miriam Defensor-Santiago (the Philippines), Sirimavo Bandaranaike (Sri Lanka) and \nMandukhai Khatun (Mongolia).[119]\n Kyrgyz oral literature includes the story of Janyl-myrza, a young woman who led her tribe to liberation from the enemy when no man in the tribe could do so.[10] In the nineteenth century, the wife of Khan Almyn-bek led a group of Kyrgyz tribes at the time of the Russian conquest of Quqon.[10]\n"
    },
    {
        "title": "Common law",
        "url": "https://en.wikipedia.org/wiki/Common_law",
        "content": "\n Common law (also known as judicial precedent, judge-made law, or case law) is the body of law primarily developed through judicial decisions rather than statutes.[2][3] Although common law may incorporate certain statutes, it is largely based on precedent\u2014judicial rulings made in previous similar cases.[4] The presiding judge determines which precedents to apply in deciding each new case.[4]\n Common law is deeply rooted in stare decisis (\"to stand by things decided\"), where courts follow precedents established by previous decisions.[5] When a similar case has been resolved, courts typically align their reasoning with the precedent set in that decision.[5] However, in a \"case of first impression\" with no precedent or clear legislative guidance, judges are empowered to resolve the issue and establish new precedent.[6][7]\n The common law, so named because it was \"common\" to all the king's courts across England, originated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066.[8][9] It established a unified legal system, gradually supplanting the local folk courts and manorial courts.[9][8] England spread the English legal system across the British Isles, first to Wales, and then to Ireland and overseas colonies; this was continued by the later British Empire. Many former colonies retain the common law system today. These common law systems are legal systems that give great weight to judicial precedent, and to the style of reasoning inherited from the English legal system.[10][11][12][13] Today, approximately one-third of the world's population lives in common law jurisdictions or in mixed legal systems that integrate common law and civil law.[14]\n According to Black's Law Dictionary, common law is \"the body of law derived from judicial decisions, rather than from statutes or constitutions.\"[15] Legal systems that rely on common law as precedent are known as \"common law jurisdictions.\"[15][11]\n Until the early 20th century, common law was widely considered to derive its authority from ancient Anglo-Saxon customs. Well into the 19th century, common law was still defined as \"unwritten law\" (lex non scripta) in legal dictionaries including Bouvier's Law Dictionary and Black's Law Dictionary.[16] According to William Blackstone's declaratory theory the common law reaffirmed pre-existing customs but did not make new law. The term \"judge-made law\" was introduced by Jeremy Bentham as a criticism of this pretense of the legal profession.[17]\n Many notable writers, including A. V. Dicey, William Markby, Oliver Wendell Holmes, John Austin, Roscoe Pound, and Ezra Ripley Thayer, eventually adopted the modern definition of common law as \"case law\" or ratio decidendi, which serves as binding precedent.[17]\n In a common law jurisdiction several stages of research and analysis are required to determine \"what the law is\" in a given situation.[18] First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. More recent decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts.[19] Finally, one integrates all the lines drawn and reasons given, and determines \"what the law is\". Then, one applies that law to the facts.\n In practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity.\n Oliver Wendell Holmes Jr. cautioned that \"the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions\".[20] Justice Cardozo noted the \"common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively\", but \"[i]ts method is inductive, and it draws its generalizations from particulars\".[21]\n The common law is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects.[22] In contrast to common law incrementalism, the legislative process is very difficult to get started, as the work begins much earlier than just introducing a bill. Once the legislation is introduced, the process to getting it passed is long, involving the committee system, debate, the potential of conference committee, voting, and President approval. Because of the involved process, many pieces must fall into place in order for it to be passed.[23]\n One example of the gradual change that typifies evolution of the common law is the gradual change in liability for negligence. The traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were parties to a contract (privity of contract). Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, Winterbottom v Wright,[24] the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The Winterbottom court recognized that there would be \"absurd and outrageous consequences\" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract (\"privity\") with the negligent party.\n A first exception to this rule arose in 1852, in the case of Thomas v. Winchester,[25] when New York's highest court held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put \"human life in imminent danger\". Thomas relied on this reason to create an exception to the \"privity\" rule. In 1909, New York held in Statler v. Ray Mfg. Co.[26] that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn \"was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed\".\n Yet the privity rule survived. In Cadillac Motor Car Co. v. Johnson[27] (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was \"no question that the wheel was made of dead and 'dozy' wood, quite insufficient for its purposes\". The Cadillac court was willing to acknowledge that the case law supported exceptions for \"an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor\". However, held the Cadillac court, \"one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud\".[28]\n Finally, in the famous case of MacPherson v. Buick Motor Co.,[29] in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to Cadillac a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:\n It may be that Statler v. Ray Mfg. Co. have extended the rule of Thomas v. Winchester. If so, this court is committed to the extension. The defendant argues that things imminently dangerous to life are poisons, explosives, deadly weapons\u2014things whose normal function it is to injure or destroy. But whatever the rule in Thomas v. Winchester may once have been, it has no longer that restricted meaning. A scaffold (Devlin v. Smith, supra) is not inherently a destructive instrument. It becomes destructive only if imperfectly constructed. A large coffee urn (Statler v. Ray Mfg. Co., supra) may have within itself, if negligently made, the potency of danger, yet no one thinks of it as an implement whose normal function is destruction. What is true of the coffee urn is equally true of bottles of aerated water (Torgesen v. Schultz, 192 N. Y. 156). We have mentioned only cases in this court. But the rule has received a like extension in our courts of intermediate appeal. In Burke v. Ireland (26 App. Div. 487), in an opinion by CULLEN, J., it was applied to a builder who constructed a defective building; in Kahner v. Otis Elevator Co. (96 App. Div. 169) to the manufacturer of an elevator; in Davies v. Pelham Hod Elevating Co. (65 Hun, 573; affirmed in this court without opinion, 146 N. Y. 363) to a contractor who furnished a defective rope with knowledge of the purpose for which the rope was to be used. We are not required at this time either to approve or to disapprove the application of the rule that was made in these cases. It is enough that they help to characterize the trend of judicial thought.\nWe hold, then, that the principle of Thomas v. Winchester is not limited to poisons, explosives, and things of like nature, to things which in their normal operation are implements of destruction. If the nature of a thing is such that it is reasonably certain to place life and limb in peril when negligently made, it is then a thing of danger. Its nature gives warning of the consequences to be expected. If to the element of danger there is added knowledge that the thing will be used by persons other than the purchaser, and used without new tests then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully. ... There must be knowledge of a danger, not merely possible, but probable. Cardozo's new \"rule\" exists in no prior case, but is inferrable as a synthesis of the \"thing of danger\" principle stated in them, merely extending it to \"foreseeable danger\" even if \"the purposes for which it was designed\" were not themselves \"a source of great danger\". MacPherson takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of Winterbottom, that \"absurd and outrageous consequences\" must be avoided, and he does so by drawing a new line in the last sentence quoted above: \"There must be knowledge of a danger, not merely possible, but probable.\" But while adhering to the underlying principle that some boundary is necessary, MacPherson overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.\n The example of the evolution of the law of negligence in the preceding paragraphs illustrates two crucial principles: (a) The common law evolves, this evolution is in the hands of judges, and judges have \"made law\" for hundreds of years.[30] (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.\n In common law systems, precedents are maintained over time through court records and historically documented in collections of case law referred to as yearbooks and law reports.[4]\n After the American Revolution, Massachusetts became the first state to establish an official Reporter of Decisions. As newer states needed law, they often looked first to the Massachusetts Reports for authoritative precedents as a basis for their own common law.[31] The United States federal courts relied on private publishers until after the Civil War, and only began publishing as a government function in 1874. West Publishing in Minnesota is the largest private-sector publisher of law reports in the United States. Government publishers typically issue only decisions \"in the raw\", while private sector publishers often add indexing, including references to the key principles of the common law involved, editorial analysis, and similar finding aids.\n Statutes are generally understood to supersede common law. They may codify existing common law, create new causes of action that did not exist in the common law,[a] or legislatively overrule the common law. Common law still has practical applications in some areas of law. Examples are contract law[32] and the law of torts.[30]\n At earlier stages in the development of modern legal systems and government, courts exercised their authority in performing what Roscoe Pound described as an essentially legislative function. As legislation became more comprehensive, courts began to operate within narrower limits of statutory interpretation.[33][34]\n Jeremy Bentham famously criticized judicial lawmaking when he argued in favor of codification and narrow judicial decisions. Pound comments that critics of judicial lawmaking are not always consistent - sometimes siding with Bentham and decrying judicial overreach, at other times unsatisfied with judicial reluctance to sweep broadly and employ case law as a means to redress certain challenges to established law.[35] Oliver Wendell Holmes once dissented: \"judges do and must legislate\".[36]\n There is a controversial legal maxim in American law that \"Statutes in derogation of the common law ought to be narrowly construed\". Henry Campbell Black once wrote that the canon \"no longer has any foundation in reason\". It is generally associated with the Lochner era.[37]\n The presumption is that legislatures may take away common law rights, but modern jurisprudence will look for the statutory purpose or legislative intent and apply rules of statutory construction like the plain meaning rule to reach decisions.[33] As the United States Supreme Court explained in United States v Texas, 507 U.S. 529 (1993):[non-primary source needed]\n Just as longstanding is the principle that \"[s]tatutes which invade the common law ... are to be read with a presumption favoring the retention of long-established and familiar principles, except when a statutory purpose to the contrary is evident. Isbrandtsen Co. v. Johnson, 343 U.S. 779, 783 (1952); Astoria Federal Savings & Loan Assn. v. Solimino, 501 U.S. 104, 108 (1991). In such cases, Congress does not write upon a clean slate. Astoria, 501 U.S. at 108. In order to abrogate a common-law principle, the statute must \"speak directly\" to the question addressed by the common law. Mobil Oil Corp. v. Higginbotham, 436 U. S. 618, 625 (1978); Milwaukee v. Illinois, 451 U. S. 304, 315 (1981). As another example, the Supreme Court of the United States in 1877,[38] held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.\n Court decisions that analyze, interpret and determine the fine boundaries and distinctions in law promulgated by other bodies are sometimes called \"interstitial common law,\" which includes judicial interpretation of fundamental laws, such as the US Constitution, of legislative statutes, and of agency regulations, and the application of law to specific facts.[39]\n The United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.\n Most of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting en banc (that is, all active judges of the court) or by a higher court.[40] In these courts, the older decision remains controlling when an issue comes up the third time.\n Other courts, for example, the Court of Appeals for the Federal Circuit (formerly known as Court of Customs and Patent Appeals) and the US Supreme Court, always sit en banc, and thus the later decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts\u2014for example, Supreme Court interpretations of the constitution or federal statutes\u2014are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.\n In the jurisdictions of England and Wales and of Northern Ireland, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify criminal law decisions of lower courts; it is the final court of appeal for civil law cases in all three of the UK jurisdictions, but not for criminal law cases in Scotland, where the High Court of Justiciary has this power instead (except on questions of law relating to reserved matters such as devolution and human rights). From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.[41]\n Canada's federal system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.\n The reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful, and have some assurance of consistency.[42] As Justice Brandeis famously expressed it, \"in most matters it is more important that the applicable rule of law be settled than that it be settled right.\"[43] This ability to predict gives more freedom to come close to the boundaries of the law.[44] For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.\n In contrast, in jurisdictions with very weak respect for precedent,[45] fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides.[42] In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less a priori guidance (unless the written law is very clear and kept updated) and must often leave a bigger \"safety margin\" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.\n This is the reason[46] for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York\u2014and remarkably often even when neither party has contacts with the United States.[46] Commercial contracts almost always include a \"choice of law clause\" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues.[47] On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law.[48] Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.\n Likewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.[49]\n This is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures\u2014but, conversely, that tends to make the statute more difficult to read.\n The common law\u2014so named because it was \"common\" to all the king's courts across England\u2014originated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066.[8]  Prior to the Norman Conquest, much of England's legal business took place in the local folk courts of its various shires and hundreds.[8]  A variety of other individual courts also existed across the land: urban boroughs and merchant fairs held their own courts, and large landholders also held their own manorial and seigniorial courts as needed.[8] The degree to which common law drew from earlier Anglo-Saxon traditions such as the jury, ordeals, the penalty of outlawry, and writs \u2013 all of which were incorporated into the Norman common law \u2013 is still a subject of much discussion. Additionally, the Catholic Church operated its own court system that adjudicated issues of canon law.[8]\n The main sources for the history of the common law in the Middle Ages are the plea rolls and the Year Books. The plea rolls, which were the official court records for the Courts of Common Pleas and King's Bench, were written in Latin. The rolls were made up in bundles by law term: Hilary, Easter, Trinity, and Michaelmas, or winter, spring, summer, and autumn. They are currently deposited in the UK National Archives, by whose permission images of the rolls for the Courts of Common Pleas, King's Bench, and Exchequer of Pleas, from the 13th century to the 17th, can be viewed online at the Anglo-American Legal Tradition site (The O'Quinn Law Library of the University of Houston Law Center).[50][51]\n The doctrine of precedent developed during the 12th and 13th centuries,[52] as the collective judicial decisions that were based in tradition, custom and precedent.[53]\n The form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.\n In 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law \"common\" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system\u2014citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.\n At the time, royal government centered on the Curia Regis (king's court), the body of aristocrats and prelates who assisted in the administration of the realm and the ancestor of Parliament, the Star Chamber, and Privy Council. Henry II developed the practice of sending judges (numbering around 20 to 30 in the 1180s) from his Curia Regis to hear the various disputes throughout the country, and return to the court thereafter.[54] The king's itinerant justices would generally receive a writ or commission under the great seal.[54] They would then resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as stare decisis (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name \"common law\".\n The king's object was to preserve public order, but providing law and order was also extremely profitable \u2013 cases on forest use as well as fines and forfeitures can generate \"great treasure\" for the government.[55][54] Eyres (a Norman French word for judicial circuit, originating from Latin iter) are more than just courts; they would supervise local government, raise revenue, investigate crimes, and enforce feudal rights of the king.[54] There were complaints of the eyre of 1198 reducing the kingdom to poverty[56] and Cornishmen fleeing to escape the eyre of 1233.[57]\n Henry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. The murder of the archbishop gave rise to a wave of popular outrage against the King. International pressure on Henry grew, and in May 1172 he negotiated a settlement with the papacy in which the King swore to go on crusade as well as effectively overturned the more controversial clauses of the Constitutions of Clarendon. Henry nevertheless continued to exert influence in any ecclesiastical case which interested him and royal power was exercised more subtly with considerable success.\n The English Court of Common Pleas was established after Magna Carta to try lawsuits between commoners in which the monarch had no interest. Its judges sat in open court in the Great Hall of the king's Palace of Westminster, permanently except in the vacations between the four terms of the Legal year.\n Judge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. In England, judges have devised a number of rules as to how to deal with precedent decisions. The early development of case-law in the thirteenth century has been traced to Bracton's On the Laws and Customs of England and led to the yearly compilations of court cases known as Year Books, of which the first extant was published in 1268, the same year that Bracton died.[58] The Year Books are known as the law reports of medieval England, and are a principal source for knowledge of the developing legal doctrines, concepts, and methods in the period from the 13th to the 16th centuries, when the common law developed into recognizable form.[59][60]\n The term \"common law\" is often used as a contrast to Roman-derived \"civil law\", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.\n By the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent.[61] However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law.[62] One of the first and throughout its history one of the most significant treatises of the common law, Bracton's De Legibus et Consuetudinibus Angliae (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian's Institutes.[63] The impact of Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into in rem (typically, actions against a thing or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and in personam (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone's Commentaries on the Laws of England,[64] and Roman law ideas regained importance with the revival of academic law schools in the 19th century.[65] As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.[66]\n The \"ancient unwritten universal custom\" view was the foundation of the first treatises by Blackstone and Coke, and was universal among lawyers and judges from the earliest times to the mid-19th century.[17]  However, for 100 years, lawyers and judges have recognized that the \"ancient unwritten universal custom\" view does not accord with the facts of the origin and growth of the law.[17]\n West's encyclopedia of American law, defines common law as \"The ancient law of England based upon societal customs and recognized and enforced by the judgments and decrees of the courts.\"[67]\n The first attempt at a comprehensive compilation of centuries of common law was by Lord Chief Justice Edward Coke, in his treatise, Institutes of the Lawes of England in the 17th century.\n As Sir Edward Coke (1552\u20131634) put it in the preface to the eighth volume of his Reports (1600\u20131615), 'the grounds of our common laws' were 'beyond the memorie or register of any beginning.'\"[68]\n According to William Blackstone the unwritten law derived its authority from immemorial usage and 'universal reception throughout the kingdom'[69][70] While its precise meaning may have changed since Blackstone's time, in modern usage it is generally understood to mean law that is independent of statutes. This was repeated by the United States Supreme Court in Levy v. McCartee: \"It is too plain for argument that the common law is here spoken of, in its appropriate sense, as the unwritten law of the land, independent of statutory enactments\".[70]\n More specifically, in modern usage, this is understood to mean law that is made by judges, not the declaratory statutes of Blackstone's era.[34][71]\n The term \"judge made law\" comes from Jeremy Bentham and the modern practice of adjudication as application of precedent derived from case law begins with Jeremy Bentham's attack on the legitimacy of the common law. The modern legal practice of applying case law as precedent made obsolete the declaratory theory of common law that prevailed in Blackstone's time.[72][73]\n A reception statute is a statutory law adopted as a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence common law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedent originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, with the partial exception of Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.[74]\n Other examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.\n Yet, adoption of the common law in the newly independent United States was not a foregone conclusion, and was controversial. Immediately after the American Revolution, there was widespread distrust and hostility to anything British, and the common law was no exception.[31] Jeffersonians decried lawyers and their common law tradition as threats to the new republic. The Jeffersonians preferred a legislatively enacted civil law under the control of the political process, rather than the common law developed by judges that\u2014by design\u2014were insulated from the political process. The Federalists believed that the common law was the birthright of Independence: after all, the natural rights to \"life, liberty, and the pursuit of happiness\" were the rights protected by common law. Even advocates for the common law approach noted that it was not an ideal fit for the newly independent colonies: judges and lawyers alike were severely hindered by a lack of printed legal materials. Before Independence, the most comprehensive law libraries had been maintained by Tory lawyers, and those libraries vanished with the loyalist expatriation, and the ability to print books was limited. Lawyer (later President) John Adams complained that he \"suffered very much for the want of books\". To bootstrap this most basic need of a common law system\u2014knowable, written law\u2014in 1803, lawyers in Massachusetts donated their books to found a law library.[31]  A Jeffersonian newspaper criticized the library, as it would carry forward \"all the old authorities practiced in England for centuries back ... whereby a new system of jurisprudence [will be founded] on the high monarchical system [to] become the Common Law of this Commonwealth... [The library] may hereafter have a very unsocial purpose.\"[31]\n For several decades after independence, English law still exerted influence over American common law\u2014for example, with Byrne v Boadle (1863), which first applied the res ipsa loquitur doctrine.\n Well into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, \"One cannot be a judge in one's own cause\" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such as those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims.\n Reliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, came under critical discussion in the late 19th century, starting in the United States. Oliver Wendell Holmes Jr. in his famous article, \"The Path of the Law\",[75] commented, \"It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past.\" Justice Holmes noted that study of maxims might be sufficient for \"the man of the present\", but \"the man of the future is the man of statistics and the master of economics\". In an 1880 lecture at Harvard, he wrote:[76]\n The life of the law has not been logic; it has been experience. The felt necessities of the time, the prevalent moral and political theories, intuitions of public policy, avowed or unconscious, even the prejudices which judges share with their fellow men, have had a good deal more to do than the syllogism in determining the rules by which men should be governed. The law embodies the story of a nation's development through many centuries, and it cannot be dealt with as if it contained only the axioms and corollaries of a book of mathematics. In the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.\n Reliance on old maxims is now deprecated.[77] Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like.[78] The degree to which these external factors should influence adjudication is the subject of active debate, but it is indisputable that judges do draw on experience and learning from everyday life, from other fields, and from other jurisdictions.[79]\n As early as the 15th century, it became the practice that litigants who felt they had been cheated by the common law system would petition the King in person. For example, they might argue that an award of damages (at common law (as opposed to equity)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other,[80] even though it was established by the 17th century that equity should prevail.\n In England, courts of law (as opposed to equity) were merged with courts of equity by the Judicature Acts of 1873 and 1875, with equity prevailing in case of conflict.[81]\n In the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the \"civil action\". Fed.R.Civ.P. 2. The distinction survives to the extent that issues that were \"common law (as opposed to equity)\" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and \"equity\" issues are decided by a judge.[82]\n The states of Delaware, Illinois, Mississippi, South Carolina, and Tennessee continue to have divided courts of law and courts of chancery, for example, the Delaware Court of Chancery. In New Jersey, the appellate courts are unified, but the trial courts are organized into a Chancery Division and a Law Division.\n For centuries, through to the 19th century, the common law acknowledged only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into exactly one of them: debt, detinue, covenant, special assumpsit, general assumpsit, trespass, trover, replevin, case (or trespass on the case), and ejectment.[83] To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific legal terms and phrases that had been traditional for centuries. Under the old common law pleading standards, a suit by a pro se (\"for oneself\", without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.\n One of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements.[84] A plaintiff can initiate a case by giving the defendant \"a short and plain statement\" of facts that constitute an alleged wrong.[85] This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.[86]\n Common law is usually contrasted with the civil law system, which is used in Continental Europe, most of Central and South America, and some African countries including Egypt and the Francophone countries of the Maghreb and west Africa.[87]\n Common law systems trace their history to the English common law, while civil law systems trace their history through the Napoleonic Code back to the Corpus Juris Civilis of Roman law.[88][89]\n The primary contrast between the two systems is the role of written decisions and precedent as a source of law (one of the defining features of common law legal systems).[42][15]\nWhile Common law systems place great weight on precedent, [90] civil law judges tend to give less weight to judicial precedent.[91] For example, the Napoleonic Code expressly forbade French judges to pronounce general principles of law.[92]\n In some civil law jurisdictions the judiciary does not have the authority to invalidate legislative provisions.[93] For example, after the fall of the Soviet Union the Armenian Parliament, with substantial support from USAID, adopted new legal codes. Some of the codes introduced problems which the judiciary was not empowered to adjudicate under the established principles of the common law of contracts - they could only apply the code as written.[94][95]\n There is no doctrine of stare decisis in the French civil law tradition. Civil law codes must be changed constantly because the precedent of courts is not binding and because courts lack authority to act if there is no statute.[96] There are regular, good quality law reports in France, but it is not a consistent practice in many of the existing civil law jurisdictions. In French-speaking colonial Africa there were no law reports and what little we know of those historical cases comes from publication in journals.[97]\n Common law systems tend to give more weight to separation of powers between the judicial branch and the executive branch. In contrast, civil law systems are typically more tolerant of allowing individual officials to exercise both powers.  One example of this contrast is the difference between the two systems in allocation of responsibility between prosecutor and adjudicator.[98][99]\n Common law courts usually use an adversarial system, in which two sides present their cases to a neutral judge.[98][99]  For example, in criminal cases, in adversarial systems, the prosecutor and adjudicator are two separate people.  The prosecutor is lodged in the executive branch, and conducts the investigation to locate evidence.  That prosecutor presents the evidence to a neutral adjudicator, who makes a decision.\n In contrast,  in civil law systems, criminal proceedings proceed under an inquisitorial system in which an examining magistrate serves two roles by first developing the evidence and arguments for one side and then the other during the investigation phase.[98][99]  The examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore, the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier.[citation needed] Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.\n The proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change his or her evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.\n In contrast, in an adversarial system, on issues of fact, the onus of framing the case rests on the parties, and judges generally decide the case presented to them, rather than acting as active investigators, or actively reframing the issues presented.  \"In our adversary system, in both civil and criminal cases, in the first instance and on appeal, we follow the principle of party presentation. That is, we rely on the parties to frame the issues for decision and assign to courts the role of neutral arbiter of matters the parties present.\"[100]  This principle applies with force in all issues in criminal matters, and to factual issues: courts seldom engage in fact gathering on their own initiative, but decide facts on the evidence presented (even here, there are exceptions, for \"legislative facts\" as opposed to \"adjudicative facts\").\n On the other hand, on issues of law, common law courts regularly raise new issues (such as matters of jurisdiction or standing), perform independent research, and reformulate the legal grounds on which to analyze the facts presented to them.  The United States Supreme Court regularly decides based on issues raised only in amicus briefs from non-parties.  One of the most notable such cases was Erie Railroad v. Tompkins, a 1938 case in which neither party questioned the ruling from the 1842 case Swift v. Tyson that served as the foundation for their arguments, but which led the Supreme Court to overturn Swift during their deliberations.[101]  To avoid lack of notice, courts may invite briefing on an issue to ensure adequate notice.[102]  However, there are limits\u2014an appeals court may not introduce a theory that contradicts the party's own contentions.[103]\n There are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages (e.g., a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.\n The contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.\n Common law countries are increasingly adopting codes, similar to civil law systems, in areas such as bankruptcy, intellectual property, antitrust, banking regulation, securities, and tax law.[104](p5) In the United States, the Uniform Commercial Code (UCC) is an example of a codified framework governing various aspects of commercial law.[104](p6) Widely regarded as one of the most significant developments in American law, the UCC has been enacted, with some local variations, in all 50 states, the District of Columbia, Puerto Rico, and the Virgin Islands.[105][106]\n An example of convergence from the other direction is shown in the 1982 decision Srl CILFIT and Lanificio di Gavardo SpA v Ministry of Health (ECLI:EU:C:1982:335), in which the European Court of Justice held that questions it has already answered need not be resubmitted. This showed how a historically distinctly common law principle is used by a court composed of judges (at that time) of essentially civil law jurisdiction.\n The common law constitutes the basis of the legal systems of:\n and many other generally English-speaking countries or Commonwealth countries (except Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the bijuridicial law or civil code of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. Guyana and Saint Lucia have mixed common law and civil law systems.\n The remainder of this section discusses jurisdiction-specific variants, arranged chronologically.\n \n Scotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the Corpus Juris Civilis with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scottish common law differed in that the use of precedent was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a precedent,[107] and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a stare decisis akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same, but in many matters (particularly those of UK-wide interest), they are similar.\n Scotland shares the Supreme Court with England, Wales and Northern Ireland for civil cases; the court's decisions are binding on the jurisdiction from which a case arises but only influential on similar cases arising in Scotland. This has had the effect of converging the law in certain areas. For instance, the modern UK law of negligence is based on Donoghue v Stevenson, a case originating in Paisley, Scotland.\n Scotland maintains a separate criminal law system from the rest of the UK, with the High Court of Justiciary being the final court for criminal appeals. The highest court of appeal in civil cases brought in Scotland is now the Supreme Court of the United Kingdom (before October 2009, final appellate jurisdiction lay with the House of Lords).[108]\n The original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. In 1664, the colony of New York had two distinct legal systems: on Manhattan Island and along the Hudson River, sophisticated courts modeled on those of the Netherlands were resolving disputes learnedly in accordance with Dutch customary law. On Long Island, Staten Island, and in Westchester, on the other hand, English courts were administering a crude, untechnical variant of the common law carried from Puritan New England and practiced without the intercession of lawyers.[109]  When the English finally regained control of New Netherland they imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. New York began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure.  The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.\n Under Louisiana's codified system, the Louisiana Civil Code, private law\u2014that is, substantive law between private sector parties\u2014is based on principles of law from continental Europe, with some common law influences. These principles derive ultimately from Roman law, transmitted through French law and Spanish law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.\n Louisiana's criminal law largely rests on English common law. Louisiana's administrative law is generally similar to the administrative law of the U.S. federal government and other U.S. states. Louisiana's procedural law is generally in line with that of other U.S. states, which in turn is generally based on the U.S. Federal Rules of Civil Procedure.\n Historically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows.[110]\n The U.S. state of California has a system based on common law, but it has codified the law in the manner of civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case Li v. Yellow Cab Co., 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)\n After Erie v. Tompkins, 304 U.S. 64, 78 (1938) overruled Joseph Storey's decision in Swift v. Tyson, the federal common law was limited to some jurisdictions stated in the Constitution, such as admiralty, and possibly some areas that may not be the traditional jurisdiction of state law.[111] \nLater courts have limited Erie slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government.[b] Except on Constitutional issues, and some procedural issues, Congress is free to legislatively overrule federal courts' common law.[112]\n In Swift, the United States Supreme Court had held that federal courts hearing cases brought under their diversity jurisdiction (allowing them to hear cases between parties from different states) had to apply the statutory law of the states, but not the common law developed by state courts. Instead, the Supreme Court permitted the federal courts to make their own common law based on general principles of law. Erie overruled Swift v. Tyson, and instead held that federal courts exercising diversity jurisdiction had to use all of the same substantive law as the courts of the states in which they were located. As the Erie Court put it, there is no \"general federal common law\". \n Post-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution.[c] Outside diversity jurisdiction and when there is no federal statute,[d] post-Erie federal courts have continued to create causes of action.[114] Justice Lewis Powell strongly objected to this practice in an influential dissent for the case Cannon v. University of Chicago.[33]\n Most executive branch agencies in the United States federal government have some adjudicatory authority. To greater or lesser extent, agencies honor their own precedent to ensure consistent results. Agency decision making is governed by the Administrative Procedure Act of 1946.\n For example, the National Labor Relations Board issues relatively few regulations, but instead promulgates most of its substantive rules through common law (connotation 1).\n The law of India, Pakistan, and Bangladesh are largely based on English common law because of the long period of British colonial influence during the period of the British Raj.\n Ancient India represented a distinct tradition of law, and had a historically independent school of legal theory and practice. The Arthashastra, dating from 400 BCE and the Manusmriti, from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance.[115] Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia.[116] Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition.[117] Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character.[118]\n When India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law.[119] After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act 1858 to this effect, which set up the structure of British government in India.[120] It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.[121][verification needed]\n Post-partition, India retained its common law system.[122] Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.\n Post-partition, Pakistan retained its common law system.[123]\n Post-partition, Bangladesh retained its common law system.\n Canada has separate federal and provincial legal systems.[124]\n Each province and territory is considered a separate jurisdiction with respect to case law. Each has its own procedural law in civil matters, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. These Courts of Appeal are then subject to the Supreme Court of Canada in terms of appeal of their decisions.\n All but one of the provinces of Canada use a common law system for civil matters (the exception being Quebec, which uses a French-heritage civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).\n Canadian Federal Courts operate under a separate system throughout Canada and deal with narrower range of subject matter than superior courts in each province and territory. They only hear cases on subjects assigned to them by federal statutes, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal is the appellate court for federal courts and hears cases in multiple cities; unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.[125]\n Canadian federal statutes must use the terminology of both the common law and civil law for civil matters; this is referred to as legislative bijuralism.[126]\n Criminal law is uniform throughout Canada. It is based on the federal statutory Criminal Code, which in addition to substance also details procedural law. The administration of justice are the responsibilities of the provinces. Canadian criminal law uses a common law system no matter which province a case proceeds.\n Nicaragua's legal system is also a mixture of the English Common Law and Civil Law. This situation was brought through the influence of British administration of the Eastern half of the Mosquito Coast from the mid-17th century until about 1894, the William Walker period from about 1855 through 1857, US interventions/occupations during the period from 1909 to 1933, the influence of US institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of US culture and institutions.[127][128]\n Israel has no formal written constitution.  Its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court[129] in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system.  However, because Israel has no written constitution, basic laws can be changed by a vote of 61 out of 120 votes in the parliament.[130] One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).\n Roman Dutch common law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French code civil in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 17th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.\n Ghana follows the English common law[131] tradition which was inherited from the British during her colonisation. Consequently, the laws of Ghana are, for the most part, a modified version of imported law that is continuously adapting to changing socio-economic and political realities of the country.[132] The Bond of 1844[133] marked the period when the people of Ghana (then Gold Coast) ceded their independence to the British[134] and gave the British judicial authority. Later, the Supreme Court Ordinance of 1876 formally introduced British law, be it the common law or statutory law, in the Gold Coast.[135] Section 14[136] of the Ordinance formalised the application of the common-law tradition in the country.\n Ghana, after independence, did not do away with the common law system inherited from the British, and today it has been enshrined in the 1992 Constitution of the country. Chapter four of Ghana's Constitution, entitled \"The Laws of Ghana\", has in Article 11(1) the list of laws applicable in the state. This comprises (a) the Constitution; (b) enactments made by or under the authority of the Parliament established by the Constitution; (c) any Orders, Rules and Regulations made by any person or authority under a power conferred by the Constitution; (d) the existing law; and (e) the common law.[137] Thus, the modern-day Constitution of Ghana, like those before it, embraced the English common law by entrenching it in its provisions. The doctrine of judicial precedence which is based on the principle of stare decisis as applied in England and other pure common law countries also applies in Ghana.\n Edward Coke, a 17th-century Lord Chief Justice of the English Court of Common Pleas and a Member of Parliament (MP), wrote several legal texts that collected and integrated centuries of case law. Lawyers in both England and America learned the law from his Institutes and Reports until the end of the 18th century. His works are still cited by common law courts around the world.\n The next definitive historical treatise on the common law is Commentaries on the Laws of England, written by Sir William Blackstone and first published in 1765\u20131769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.\n While he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Justice Oliver Wendell Holmes Jr. published a short volume called The Common Law, which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law is; rather, Holmes describes the common law process. Law professor John Chipman Gray's The Nature and Sources of the Law, an examination and survey of the common law, is also still commonly read in U.S. law schools.\n In the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.\n Scots common law covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called Institutional Texts and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, Jus Feudale (1655) and Stair, The Institutions of the Law of Scotland (1681).\n"
    },
    {
        "title": "Bolivia",
        "url": "https://en.wikipedia.org/wiki/Bolivia#Geography",
        "content": "\n in South America\u00a0(gray) Bolivia,[c] officially the Plurinational State of Bolivia,[d] is a landlocked country located in central South America. The country features diverse geography, including vast Amazonian plains, tropical lowlands, mountains, the Gran Chaco Province, warm valleys, high-altitude Andean plateaus, and snow-capped peaks, encompassing a wide range of climates and biomes across its regions and cities. It includes part of the Pantanal, the largest tropical wetland in the world, along its eastern border. It is bordered by Brazil to the north and east, Paraguay to the southeast, Argentina to the south, Chile to the southwest, and Peru to the west.  The seat of government is La Paz, which contains the executive, legislative, and electoral branches of government, while the constitutional capital is Sucre, the seat of the judiciary. The largest city and principal industrial center is Santa Cruz de la Sierra, located on the Llanos Orientales (eastern tropical lowlands), a mostly flat region in the east of the country with a diverse non-Andean culture.\n The sovereign state of Bolivia is a constitutionally unitary state divided into nine departments. Its geography varies as the elevation fluctuates, from the western snow-capped peaks of the Andes to the eastern lowlands, situated within the Amazon basin. One-third of the country is within the Andean mountain range. With an area of 1,098,581\u00a0km2 (424,164\u00a0sq\u00a0mi), Bolivia is the fifth-largest country in South America after Brazil, Argentina, Peru and Colombia, and, alongside Paraguay, is one of two landlocked countries in the Americas. It is the largest landlocked country in the Southern Hemisphere. The country's population, estimated at 12\u00a0million,[11] is multiethnic, including Amerindians, Mestizos, and the descendants of Europeans and Africans. Spanish is the official and predominant language, although 36 indigenous languages also have official status, of which the most commonly spoken are Guaran\u00ed, Aymara, and Quechua.\n Well before Spanish colonization, the third part of the high region of Bolivia was largelly part of the Tiwanaku Polity which collapsed about 1000 AD. The Colla\u2013Inca War of the 1440s marked the beginning of Inca rule in western Bolivia. The eastern and northern lowlands of Bolivia was inhabited by independent non-Andean Amazonian and Guaran\u00ed tribes. Spanish conquistadores, arriving from Cusco, Peru, forcibly took control of the region in the 16th century.\n During the subsequent Spanish colonial period, Bolivia was administered by the Real Audiencia of Charcas. Spain built its empire in large part upon the silver that was extracted from Cerro Rico in Potos\u00ed. Following an unsuccessfull rebellion in Sucre on May 25, 1809, sixteen years of fighting would follow before the establishment of the Republic, named for Sim\u00f3n Bol\u00edvar.[12] Over the course of the 19th and early 20th centuries, Bolivia lost control of several peripheral territories to neighboring countries, such as Brazil's of the Acre territory, and the War of the Pacific (1879), in which Chile seized the country's Pacific coastal region.\n 20th century Bolivia experienced a succession of military and civilian governments until Hugo Banzer led a US-backed coup d'\u00e9tat in 1971, replacing the socialist government of Juan Jos\u00e9 Torres with a military dictatorship. Banzer's regime cracked down on left-wing and socialist opposition parties, and other perceived forms of dissent, resulting in the torturing and murders of countless Bolivian citizens. Banzer was ousted in 1978 and, twenty years later, returned as the democratically elected President of Bolivia (1997\u20132001). Under the 2006\u20132019 presidency of Evo Morales, the country saw significant economic growth and political stability but was also accused of democratic backsliding,[13][14] and was described as a competitive authoritarian regime.[15][16][17] Freedom House classifies Bolivia as a partly-free democracy as of 2023, with a 66/100 score.[18]\n Modern Bolivia is a member of the Non-Aligned Movement (NAM),[19] Organization of American States (OAS), Amazon Cooperation Treaty Organization (ACTO), Bank of the South, ALBA, and the Union of South American Nations (USAN). Bolivia remains a developing country, and the second-poorest in South America, though it has slashed poverty rates and now has one of the fastest-growing economies on the continent (in terms of GDP). Its main economic resources include agriculture, forestry, fishing, mining, and goods such as textiles and clothing, refined metals, and refined petroleum. Bolivia is very geologically rich, with mines producing tin, silver, lithium, and copper. The country is also known for its production of coca plants and refined cocaine. In 2021, estimated coca cultivation and cocaine production was 39,700 hectares and 317 metric tons, respectively.[20]\n Bolivia is named after Sim\u00f3n Bol\u00edvar, a Venezuelan leader in the Spanish American wars of independence.[21] The leader of Venezuela, Antonio Jos\u00e9 de Sucre, had been given the option by Bol\u00edvar to either unify Charcas (present-day Bolivia) with the newly formed Republic of Peru, to unify with the United Provinces of the R\u00edo de la Plata, or to formally declare its independence from Spain as a wholly independent state. Sucre opted to create a brand new state and on 6 August 1825, with local support, named it in honor of Sim\u00f3n Bol\u00edvar.[22]\n The original name was Republic of Bol\u00edvar. Some days later, congressman Manuel Mart\u00edn Cruz proposed: \"If from Romulus, Rome, then from Bol\u00edvar, Bolivia\" (Spanish: Si de R\u00f3mulo, Roma; de Bol\u00edvar, Bolivia). The name was approved by the Republic on 3 October 1825. In 2009, a new constitution changed the country's official name to \"Plurinational State of Bolivia\" to reflect the multi-ethnic nature of the country and the strengthened rights of Bolivia's indigenous peoples under the new constitution.[23][24]\n The region now known as Bolivia had been occupied for over 2,500 years when the Aymara arrived; however, present-day Aymara associate themselves with the ancient civilization of the Tiwanaku Empire, which had its capital at Tiwanaku, in Western Bolivia. The capital city of Tiwanaku dates-back as early as 1500 BC, when it was a small, agriculturally-based village.[25]\n The Aymara community grew to urban proportions between AD 600 and AD 800, becoming an important regional power in the southern Andes. According to early estimates,[when?] the city covered approximately 6.5 square kilometers (2.5 square miles) at its peak, and had between 15,000 and 30,000 inhabitants.[26] However, in 1996, satellite imaging was used to map the extent of preserved suka kollus (flooded raised fields) across the three primary valleys of Tiwanaku, with the results suggesting a population-carrying capacity of anywhere between 285,000 and 1,482,000 people.[27]\n Around AD 400, Tiwanaku went from being a locally-dominant force to a 'predatory' state, aggressively expanding its reach into the Yungas and bringing its culture and ways to new peoples in Peru, Bolivia, and Chile. Nonetheless, Tiwanaku was not a violent or domineering culture; to expand its reach, the state exercised great political astuteness, created colonies, fostered local trade agreements (which made other cultures rather dependent), and instituted state cults.[28]\n As rainfall gradually decreased, the stores of food supplies decreased, and thus the elites lost power. Tiwanaku disappeared around AD 1000. The area remained uninhabited for centuries thereafter.[29]\n Between 1438 and 1527, Incan Empire expanded from its capital at Cusco, gaining control over much of what is now the Bolivian Andes, and extending its control into the fringes of the Amazon basin.\n The Spanish conquest of the Inca empire began in 1524 and was mostly completed by 1533. The territory now called Bolivia was known as Charcas, and was under the authority of Spain. Local government came from the Audiencia de Charcas located in Chuquisaca (La Plata\u2014modern Sucre). Founded in 1545 as a mining town, Potos\u00ed soon produced fabulous wealth, becoming the largest city in the New World with a population exceeding 150,000 people.[30]\n By the late 16th century, Bolivian silver was an important source of revenue for the Spanish Empire.[31] A steady stream of natives served as labor force under the brutal, slave conditions of the Spanish version of the pre-Columbian draft system called the mita.[32] Charcas was transferred to the Viceroyalty of the R\u00edo de la Plata in 1776 and the people from Buenos Aires, the capital of the Viceroyalty, coined the term \"Upper Peru\" (Spanish: Alto Peru) as a popular reference to the Royal Audiencia of Charcas. T\u00fapac Katari led the indigenous rebellion that laid siege to La Paz in March 1781,[33] during which 20,000 people died.[34] As Spanish royal authority weakened during the Napoleonic Wars, sentiment against colonial rule grew.\n The struggle for independence started in the city of Sucre on 25 May 1809 and the Chuquisaca Revolution (Chuquisaca was then the name of the city) is known as the first cry of Freedom in Latin America. That revolution was followed by the La Paz revolution on 16 July 1809. The La Paz revolution marked a complete split with the Spanish government, while the Chuquisaca Revolution established a local independent junta in the name of the Spanish King deposed by Napoleon Bonaparte. Both revolutions were short-lived and defeated by the Spanish authorities in the Viceroyalty of the Rio de La Plata, but the following year the Spanish American wars of independence raged across the continent.\n Bolivia was captured and recaptured many times during the war by the royalists and patriots. Buenos Aires sent three military campaigns, all of which were defeated, and eventually limited itself to protecting the national borders at Salta. Bolivia was finally freed of Royalist dominion by Marshal Antonio Jos\u00e9 de Sucre, with a military campaign coming from the North in support of the campaign of Sim\u00f3n Bol\u00edvar. After 16 years of war the Republic was proclaimed on 6 August 1825.\n In 1836, Bolivia, under the rule of Marshal Andr\u00e9s de Santa Cruz, invaded Peru to reinstall the deposed president, General Luis Jos\u00e9 de Orbegoso. Peru and Bolivia formed the Peru-Bolivian Confederation, with de Santa Cruz as the Supreme Protector. Following tension between the Confederation and Chile, Chile declared war on 28 December 1836. Argentina separately declared war on the Confederation on 9 May 1837. The Peruvian-Bolivian forces achieved several major victories during the War of the Confederation: the defeat of the Argentine expedition and the defeat of the first Chilean expedition on the fields of Paucarpata near the city of Arequipa. The Chilean army and its Peruvian rebel allies surrendered unconditionally and signed the Paucarpata Treaty. The treaty stipulated that Chile would withdraw from Peru-Bolivia, Chile would return captured Confederate ships, economic relations would be normalized, and the Confederation would pay Peruvian debt to Chile. However, the Chilean government and public rejected the peace treaty. Chile organized a second attack on the Confederation and defeated it in the Battle of Yungay. After this defeat, Santa Cruz resigned and went to exile in Ecuador and then Paris, and the Peruvian-Bolivian Confederation was dissolved.\n Following the renewed independence of Peru, Peruvian president General Agust\u00edn Gamarra invaded Bolivia. On 18 November 1841, the battle de Ingavi took place, in which the Bolivian Army defeated the Peruvian troops of Gamarra (killed in the battle). After the victory, Bolivia invaded Peru on several fronts. The eviction of the Bolivian troops from the south of Peru would be achieved by the greater availability of material and human resources of Peru; the Bolivian Army did not have enough troops to maintain an occupation. In the district of Locumba \u2013 Tacna, a column of Peruvian soldiers and peasants defeated a Bolivian regiment in the so-called Battle of Los Altos de Chipe (Locumba). In the district of Sama and in Arica, the Peruvian colonel Jos\u00e9 Mar\u00eda Lavay\u00e9n organized a troop that managed to defeat the Bolivian forces of Colonel Rodr\u00edguez Magari\u00f1os and threaten the port of Arica. In the battle of Tarapac\u00e1 on 7 January 1842, Peruvian militias formed by the commander Juan Buend\u00eda defeated a detachment led by Bolivian colonel Jos\u00e9 Mar\u00eda Garc\u00eda, who died in the confrontation. Bolivian troops left Tacna, Arica and Tarapac\u00e1 in February 1842, retreating towards Moquegua and Puno.[35] The battles of Motoni and Orurillo forced the withdrawal of Bolivian forces occupying Peruvian territory and exposed Bolivia to the threat of counter-invasion. The Treaty of Puno was signed on 7 June 1842, ending the war. However, the climate of tension between Lima and La Paz would continue until 1847, when the signing of a Peace and Trade Treaty became effective.\n A period of political and economic instability in the early-to-mid-19th century weakened Bolivia. In addition, during the War of the Pacific (1879\u201383), Chile occupied vast territories rich in natural resources south west of Bolivia, including the Bolivian coast. Chile took control of today's Chuquicamata area, the adjoining rich salitre (saltpeter) fields, and the port of Antofagasta among other Bolivian territories.\n Since independence, Bolivia has lost over half of its territory to neighboring countries.[36] Through diplomatic channels in 1909, it lost the basin of the Madre de Dios River and the territory of the Purus in the Amazon, yielding 250,000\u00a0km2 to Peru.[37] It also lost the state of Acre, in the Acre War, important because this region was known for its production of rubber. Peasants and the Bolivian army fought briefly but after a few victories, and facing the prospect of a total war against Brazil, it was forced to sign the Treaty of Petr\u00f3polis in 1903, in which Bolivia lost this rich territory. Popular myth has it that Bolivian president Mariano Melgarejo (1864\u201371) traded the land for what he called \"a magnificent white horse\" and Acre was subsequently flooded with Brazilians, which ultimately led to confrontation and fear of war with Brazil.[38]\n In the late 19th century, an increase in the world price of silver brought Bolivia relative prosperity and political stability.\n During the early 20th century, tin replaced silver as the country's most important source of wealth. A succession of governments controlled by the economic and social elite followed laissez-faire capitalist policies through the first 30 years of the 20th century.[39]\n Living conditions of the native people, who constitute most of the population, remained deplorable. With work opportunities limited to primitive conditions in the mines and in large estates having nearly feudal status, they had no access to education, economic opportunity, and political participation. Bolivia's defeat by Paraguay in the Chaco War (1932\u20131935), where Bolivia lost a great part of the Gran Chaco region in dispute, marked a turning-point.[40][41][42]\n On 7 April 1943, Bolivia entered World War II, joining part of the Allies, which caused president Enrique Pe\u00f1aranda to declare war on the Axis powers of Germany, Italy and Japan.\n In 1945, Bolivia became a founding member of the United Nations.\n The Revolutionary Nationalist Movement (MNR), the most historic political party, emerged as a broad-based party. Denied its victory in the 1951 presidential elections, the MNR led a successful revolution in 1952. Under President V\u00edctor Paz Estenssoro, the MNR, having strong popular pressure, introduced universal suffrage into his political platform and carried out a sweeping land-reform promoting rural education and nationalization of the country's largest tin mines.\n Twelve years of tumultuous rule left the MNR divided. In 1964, a military junta overthrew President Paz Estenssoro at the outset of his third term. The 1969 death of President Ren\u00e9 Barrientos, a former member of the junta who was elected president in 1966, led to a succession of weak governments. Alarmed by the rising Popular Assembly and the increase in the popularity of President Juan Jos\u00e9 Torres, the military, the MNR, and others installed Hugo Banzer as president in 1971. He returned to the presidency in 1997 through 2001. Torres, who had fled Bolivia, was kidnapped and assassinated in 1976 as part of Operation Condor, the U.S.-supported campaign of political repression by South American right-wing dictators.[43]\n The United States' Central Intelligence Agency (CIA) financed and trained the Bolivian military dictatorship in the 1960s. The revolutionary leader Che Guevara was killed by a team of CIA officers and members of the Bolivian Army on 9 October 1967, in Bolivia. F\u00e9lix Rodr\u00edguez was a CIA officer on the team with the Bolivian Army that captured and shot Guevara.[44] Rodriguez said that after he received a Bolivian presidential execution order, he told \"the soldier who pulled the trigger to aim carefully, to remain consistent with the Bolivian government's story that Che had been killed in action during a clash with the Bolivian army.\" Rodriguez said the US government had wanted Che in Panama, and \"I could have tried to falsify the command to the troops, and got Che to Panama as the US government said they had wanted\", but that he had chosen to \"let history run its course\" as desired by Bolivia.[45]\n Elections in 1978 were marked by fraud and those in 1979 were inconclusive. There were coups d'\u00e9tat, counter-coups, and caretaker governments. Following the 1980 election, General Luis Garc\u00eda Meza carried out a coup d'\u00e9tat. The Bolivian Workers' Center, which tried to resist the putsch, was violently repressed. More than a thousand people were killed in less than a year. Cousin of one of the most important narco-trafficker of the country, Garc\u00eda Meza favored the production of cocaine.[46][47] After a military rebellion forced out Garc\u00eda Meza in 1981, three other military governments in fourteen months struggled with Bolivia's growing economic problems. Unrest forced the military to convoke the Congress elected in 1980, and allow it to choose a new president. In October 1982, Hern\u00e1n Siles Zuazo again became president, twenty-two years after the end of his first term of office (1956\u20131960).\n In 1993, Gonzalo S\u00e1nchez de Lozada was elected president in alliance with the Tupac Katari Revolutionary Liberation Movement, which inspired indigenous-sensitive and multicultural-aware policies.[48] S\u00e1nchez de Lozada pursued an aggressive economic and social reform agenda. The most dramatic reform was privatization under the \"capitalization\" program, under which investors, typically foreign, acquired 50% ownership and management control of public enterprises in return for agreed upon capital investments.[49][50] In 1993, Sanchez de Lozada introduced the Plan de Todos, which led to the decentralization of government, introduction of intercultural bilingual education, implementation of agrarian legislation, and privatization of state owned businesses. The plan explicitly stated that Bolivian citizens would own a minimum of 51% of enterprises; under the plan, most state-owned enterprises (SOEs), though not mines, were sold.[51] This privatization of SOEs led to a neoliberal structuring.[52]\n The reforms and economic restructuring were strongly opposed by certain segments of society, which instigated frequent and sometimes violent protests, particularly in La Paz and the Chapare coca-growing region, from 1994 through 1996. The indigenous population of the Andean region was not able to benefit from government reforms.[53] During this time, the umbrella labor-organization of Bolivia, the Central Obrera Boliviana (COB), became increasingly unable to effectively challenge government policy. A teachers' strike in 1995 was defeated because the COB could not marshal the support of many of its members, including construction and factory workers.\n In the 1997 elections, General Hugo Banzer, leader of the Nationalist Democratic Action party (ADN) and former dictator (1971\u20131978), won 22% of the vote, while the MNR candidate won 18%. At the outset of his government, President Banzer launched a policy of using special police-units to eradicate physically the illegal coca of the Chapare region. The Revolutionary Left Movement (MIR) of Jaime Paz Zamora remained a coalition-partner throughout the Banzer government, supporting this policy (called the Dignity Plan).[54] The Banzer government basically continued the free-market and privatization-policies of its predecessor. The relatively robust economic growth of the mid-1990s continued until about the third year of its term in office. After that, regional, global and domestic factors contributed to a decline in economic growth. Financial crises in Argentina and Brazil, lower world prices for export commodities, and reduced employment in the coca sector depressed the Bolivian economy. The public also perceived a significant amount of public sector corruption. These factors contributed to increasing social protests during the second half of Banzer's term.\n Between January 1999 and April 2000, large-scale protests erupted in Cochabamba, Bolivia's third largest city at the time, in response to the privatization of water resources by foreign companies and a subsequent doubling of water prices. On 6 August 2001, Banzer resigned from office after being diagnosed with cancer. He died less than a year later. Vice President Jorge Fernando Quiroga Ram\u00edrez completed the final year of his term.\n In the June 2002 national elections, former President Gonzalo S\u00e1nchez de Lozada (MNR) placed first with 22.5% of the vote, followed by coca-advocate and native peasant-leader Evo Morales (Movement Toward Socialism, MAS) with 20.9%. A July agreement between the MNR and the fourth-place MIR, which had again been led in the election by former President Jaime Paz Zamora, virtually ensured the election of S\u00e1nchez de Lozada in the congressional run-off, and on 6 August he was sworn in for the second time. The MNR platform featured three overarching objectives: economic reactivation (and job creation), anti-corruption, and social inclusion.\n In 2003, the Bolivian gas conflict broke out. On 12 October 2003, the government imposed martial law in El Alto after 16 people were shot by the police and several dozen wounded in violent clashes. Faced with the option of resigning or more bloodshed, S\u00e1nchez de Lozada offered his resignation in a letter to an emergency session of Congress. After his resignation was accepted and his vice president, Carlos Mesa, invested, he left on a commercially scheduled flight for the United States.\n The country's internal situation became unfavorable for such political action on the international stage. After a resurgence of gas protests in 2005, Carlos Mesa attempted to resign in January 2005, but his offer was refused by Congress. On 22 March 2005, after weeks of new street protests from organizations accusing Mesa of bowing to U.S. corporate interests, Mesa again offered his resignation to Congress, which was accepted on 10 June. The chief justice of the Supreme Court, Eduardo Rodr\u00edguez, was sworn as interim president to succeed the outgoing Carlos Mesa.\n Evo Morales won the 2005 presidential election with 53.7% of the votes.[55] On 1 May 2006, Morales announced his intent to re-nationalize Bolivian hydrocarbon assets following protests which demanded this action.[56] Fulfilling a campaign promise, on 6 August 2006, Morales opened the Bolivian Constituent Assembly to begin writing a new constitution aimed at giving more power to the indigenous majority.[57]\n 2009 marked the creation of a new constitution and the renaming of the country to the Plurinational State of Bolivia. The previous constitution did not allow a consecutive reelection of a president, but the new constitution allowed for just one reelection, starting the dispute if Evo Morales was enabled to run for a second term arguing he was elected under the last constitution. This also triggered a new general election in which Evo Morales was re-elected with 61.36% of the vote. His party, Movement for Socialism, also won a two-thirds majority in both houses of the National Congress.[58] By 2013, after being reelected under the new constitution, Evo Morales and his party attempted a third term as President of Bolivia. The opposition argued that a third term would be unconstitutional, but the Bolivian Constitutional Court ruled that Morales' first term under the previous constitution did not count towards his term limit.[59] This allowed Evo Morales to run for a third term in 2014, and he was re-elected with 64.22% of the vote.[60] During his third term, Evo Morales began to plan for a fourth, and the 2016 Bolivian constitutional referendum asked voters to override the constitution and allow Evo Morales to run for an additional term in office. Morales narrowly lost the referendum;[61] however, in 2017 his party then petitioned the Bolivian Constitutional Court to override the constitution on the basis that the American Convention on Human Rights made term limits a human rights violation.[62] The Inter-American Court of Human Rights determined that term limits are not a human rights violation in 2018;[63][64] however, once again the Bolivian Constitutional Court ruled that Morales has permission to run for a fourth term in the 2019 elections, and this permission was not retracted. \"[T]he country's highest court overruled the constitution, scrapping term limits altogether for every office. Morales can now run for a fourth term in 2019 \u2013 and for every election thereafter.\"[65]\n The revenues generated by the partial nationalization of hydrocarbons made it possible to finance several social measures: the Renta Dignidad (or old age minimum) for people over 60 years old; the Juana Azurduy voucher (named after the revolutionary Juana Azurduy de Padilla, 1780\u20131862), which ensures the complete coverage of medical expenses for pregnant women and their children in order to fight infant mortality; the Juancito Pinto voucher (named after a child hero of the Pacific War, 1879\u20131884), an aid paid until the end of secondary school to parents whose children are in school in order to combat school dropout, and the Single Health System, which since 2018 has offered all Bolivians free medical care.[66]\n The reforms adopted made the Bolivian economic system the most successful and stable in the region. Between 2006 and 2019, GDP grew from $9 billion to over $40 billion, real wages increased, GDP per capita tripled, foreign exchange reserves rose, inflation was essentially eliminated, and extreme poverty fell from 38% to 15%, a 23-point drop.[67]\n During the 2019 elections, the Transmisi\u00f3n de Resultados Electorales Preliminares (TREP) (a quick count process used in Latin America as a transparency measure in electoral processes) was interrupted; at the time, Morales had a lead of 46.86 percent to Mesa's 36.72, after 95.63 percent of tally sheets were counted.[68] Two days after the interruption, the official count showed Morales fractionally clearing the 10-point margin he needed to avoid a runoff election, with the final official tally counted as 47.08 percent to Mesa's 36.51 percent, starting a wave of protests and tension in the country.\n Amidst allegations of fraud perpetrated by the Morales government, widespread protests were organized to dispute the election. On 10 November, the Organization of American States (OAS) released a preliminary report concluding several irregularities in the election,[69][70][71] though these findings were heavily disputed.[72] The New York Times reported on 7 June 2020 that the OAS analysis immediately after the 20 October election was flawed yet fuelled \"a chain of events that changed the South American nation's history\".[73][74][75]\n After weeks of protests, Morales resigned on national television shortly after the Commander-in-Chief of the armed forces General Williams Kaliman had urged that he do so to restore \"peace and stability\".[76][77] Opposition Senator Jeanine \u00c1\u00f1ez declared herself interim president, claiming constitutional succession after the president, vice president and both head of the legislature chambers. She was confirmed as interim president by the constitutional court who declared her succession to be constitutional and automatic.[78][79] International politicians, scholars and journalists are divided between describing the event as a coup or a spontaneous social uprising against an unconstitutional fourth term.[80][81] Protests to reinstate Morales as president continued becoming highly violent: burning public buses and private houses, destroying public infrastructure and harming pedestrians.[82][83][84][85][86] The protests were met with more violence by security forces against Morales supporters after \u00c1\u00f1ez exempted police and military from criminal responsibility in operations for \"the restoration of order and public stability\".[87][88]\n In April 2020, the interim government took out a loan of more than $327\u00a0million from the International Monetary Fund to meet the country's needs during the COVID-19 pandemic.[89] New elections were scheduled for 3 May 2020. In response to the coronavirus pandemic, the Bolivian electoral body, the TSE, made an announcement postponing the election. MAS reluctantly agreed with the first delay only. A date for the new election was delayed twice more, in the face of massive protests and violence.[90][91][92] The final proposed date for the elections was 18 October 2020.[93] Observers from the OAS, UNIORE, and the UN all reported that they found no fraudulent actions in the 2020 elections.[94]\n The general election had a record voter turnout of 88.4% and ended in a landslide win for MAS which took 55.1% of the votes compared to 28.8% for centrist former president Carlos Mesa. Both Mesa and \u00c1\u00f1ez conceded defeat.[95][96]\n On 8 November 2020, Luis Arce was sworn in as President of Bolivia alongside his Vice President David Choquehuanca.[97] In February 2021, the Arce government returned an amount of around $351\u00a0million to the IMF. This comprised a loan of $327\u00a0million taken out by the interim government in April 2020 and interest of around $24\u00a0million. The government said it returned the loan to protect Bolivia's economic sovereignty and because the conditions attached to the loan were unacceptable.[89]\n On 26 June 2024, a military coup attempt led by Juan Jos\u00e9 Z\u00fa\u00f1iga ended after lasting only 5 hours. In the evening of 26 June, Bolivian police arrested Z\u00fa\u00f1iga.[98][99][100]\n Bolivia is located in the central zone of South America, between 57\u00b026'\u201369\u00b038'W and 9\u00b038'\u201322\u00b053'S. With an area of 1,098,581 square kilometers (424,164\u00a0sq\u00a0mi), Bolivia is the world's 28th-largest country, and the fifth largest country in South America,[101] extending from the Central Andes through part of the Gran Chaco, Pantanal and as far as the Amazon. The geographic center of the country is the so-called Puerto Estrella (\"Star Port\") on the R\u00edo Grande, in \u00d1uflo de Ch\u00e1vez Province, Santa Cruz Department.\n The geography of the country exhibits a great variety of terrain and climates. Bolivia has a high level of biodiversity,[102] considered one of the greatest in the world, as well as several ecoregions with ecological sub-units such as the Altiplano, tropical rainforests (including Amazon rainforest), dry valleys, and the Chiquitania, which is a tropical savanna.[citation needed] These areas feature enormous variations in altitude, from an elevation of 6,542 meters (21,463\u00a0ft) above sea level in Nevado Sajama to nearly 70 meters (230\u00a0ft) along the Paraguay River. Although a country of great geographic diversity, Bolivia has remained a landlocked country since the War of the Pacific. Puerto Su\u00e1rez, San Mat\u00edas and Puerto Quijarro are located in the Bolivian Pantanal. In Bolivia forest cover is around 47% of the total land area, equivalent to 50,833,760\u00a0ha of forest in 2020, down from 57,804,720\u00a0ha in 1990. In 2020, naturally regenerating forest covered 50,771,160\u00a0ha and planted forest covered 62,600\u00a0ha. Of the naturally regenerating forest 0% was reported to be primary forest (consisting of native tree species with no clearly visible indications of human activity) and around 24% of the forest area was found within protected areas. For the year 2015, 100% of the forest area was reported to be under public ownership.[103][104]\n Bolivia can be divided into three physiographic regions:\n The geology of Bolivia comprises a variety of different lithologies as well as tectonic and sedimentary environments. On a synoptic scale, geological units coincide with topographical units. Most elementally, the country is divided into a mountainous western area affected by the subduction processes in the Pacific and an eastern lowlands of stable platforms and shields.\n The climate of Bolivia varies drastically from one eco-region to the other, from the tropics in the eastern llanos to a polar climate in the western Andes. The summers are warm, humid in the east and dry in the west, with rains that often modify temperatures, humidity, winds, atmospheric pressure and evaporation, yielding very different climates in different areas. When the climatological phenomenon known as El Ni\u00f1o[107][108] takes place, it causes great alterations in the weather. Winters are very cold in the west, and it snows in the mountain ranges, while in the western regions, windy days are more common. The autumn is dry in the non-tropical regions.\n Bolivia is especially vulnerable to the negative consequences of climate change. Twenty percent of the world's tropical glaciers are located within the country,[109] and are more sensitive to change in temperature due to the tropical climate they are located in. Temperatures in the Andes increased by 0.1\u00a0\u00b0C per decade from 1939 to 1998, and more recently the rate of increase has tripled (to 0.33\u00a0\u00b0C per decade from 1980 to 2005),[110] causing glaciers to recede at an accelerated pace and create unforeseen water shortages in Andean agricultural towns. Farmers have taken to temporary city jobs when there is poor yield for their crops, while others have started permanently leaving the agricultural sector and are migrating to nearby towns for other forms of work;[111] some view these migrants as the first generation of climate refugees.[112] Cities that are neighbouring agricultural land, like El Alto, face the challenge of providing services to the influx of new migrants; because there is no alternative water source, the city's water source is now being constricted.\n Bolivia's government and other agencies have acknowledged the need to instill new policies battling the effects of climate change. The World Bank has provided funding through the Climate Investment Funds (CIF) and are using the Pilot Program for Climate Resilience (PPCR II) to construct new irrigation systems, protect riverbanks and basins, and work on building water resources with the help of indigenous communities.[113]\n Bolivia, with an enormous variety of organisms and ecosystems, is part of the \"Like-Minded Megadiverse Countries\".[114]\n Bolivia's variable altitudes, ranging from 90\u20136,542 meters (295\u201321,463\u00a0ft) above sea level, allow for a vast biologic diversity. The territory of Bolivia comprises four types of biomes, 32 ecological regions, and 199 ecosystems. Within this geographic area there are several natural parks and reserves such as the Noel Kempff Mercado National Park, the Madidi National Park, the Tunari National Park, the Eduardo Avaroa Andean Fauna National Reserve, and the Kaa-Iya del Gran Chaco National Park and Integrated Management Natural Area, among others.\n Bolivia boasts over 17,000 species of seed plants, including over 1,200 species of fern, 1,500 species of marchantiophyta and moss, and at least 800 species of fungus. In addition, there are more than 3,000 species of medicinal plants. Bolivia is considered the place of origin for such species as peppers and chili peppers, peanuts, the common beans, yucca, and several species of palm. Bolivia also naturally produces over 4,000 kinds of potatoes. The country had a 2018 Forest Landscape Integrity Index mean score of 8.47/10, ranking it 21st globally out of 172 countries.[115]\n Bolivia has more than 2,900 animal species, including 398 mammals, over 1,400 birds (about 14% of birds known in the world, being the sixth most diverse country in terms of bird species)[116][unreliable source?], 204 amphibians, 277 reptiles, and 635 fish, all fresh water fish as Bolivia is a landlocked country. In addition, there are more than 3,000 types of butterfly, and more than 60 domestic animals.\n In 2020 a new species of snake, the mountain fer-de-lance viper, was discovered in Bolivia.[117]\n A Ministry of Environment and Water was created in 2006 after the election of Evo Morales, who reversed the privatization of the water distribution sector in the 1990s by President Gonzalo S\u00e1nchez de Lozada. The new Constitution, approved by referendum in 2009, makes access to water a fundamental right. In July 2010, at the initiative of Bolivia, the United Nations passed a resolution recognizing as \"fundamental\" the \"right to safe and clean drinking water\".[118]\n In 2013, the Law of the Rights of Mother Earth was passed, which accords nature the same rights as humans.[119]\n Bolivia has been governed by democratically elected governments since 1982; prior to that, it was governed by various dictatorships. Presidents Hern\u00e1n Siles Zuazo (1982\u20131985) and V\u00edctor Paz Estenssoro (1985\u20131989) began a tradition of ceding power peacefully which has continued, although three presidents have stepped down in the face of extraordinary circumstances: Gonzalo S\u00e1nchez de Lozada in 2003, Carlos Mesa in 2005, and Evo Morales in 2019.\n Bolivia's multiparty democracy has seen a wide variety of parties in the presidency and parliament, although the Revolutionary Nationalist Movement, Nationalist Democratic Action, and the Revolutionary Left Movement predominated from 1985 to 2005. On 11 November 2019, all senior governmental positions were vacated following the resignation of Evo Morales and his government. On 13 November 2019, Jeanine \u00c1\u00f1ez, a former senator representing Beni, declared herself acting President of Bolivia. Luis Arce was elected on 23 October 2020; he took office as president on 8 November 2020.\n The constitution, drafted in 2006\u201307 and approved in 2009, provides for balanced executive, legislative, judicial, and electoral powers, as well as several levels of autonomy. The traditionally strong executive branch tends to overshadow the Congress, whose role is generally limited to debating and approving legislation initiated by the executive. The judiciary, consisting of the Supreme Court and departmental and lower courts, has long been riddled with corruption and inefficiency. Through revisions to the constitution in 1994, and subsequent laws, the government has initiated potentially far-reaching reforms in the judicial system as well as increasing decentralizing powers to departments, municipalities, and indigenous territories.\n The executive branch is headed by a president and vice president, and consists of a variable number (currently, 20) of government ministries. The president is elected to a five-year term by popular vote, and governs from the Presidential Palace (popularly called the Burnt Palace, Palacio Quemado) in La Paz. In the case that no candidate receives an absolute majority of the popular vote or more than 40% of the vote with an advantage of more than 10% over the second-place finisher, a run-off is to be held among the two candidates most voted.[120]\n The Asamblea Legislativa Plurinacional (Plurinational Legislative Assembly or National Congress) has two chambers. The C\u00e1mara de Diputados (Chamber of Deputies) has 130 members elected to five-year terms, 63 from single-member districts (circunscripciones), 60 by proportional representation, and seven by the minority indigenous peoples of seven departments. The C\u00e1mara de Senadores (Chamber of Senators) has 36 members (four per department). Members of the Assembly are elected to five-year terms. The body has its headquarters on the Plaza Murillo in La Paz, but also holds honorary sessions elsewhere in Bolivia. The Vice President serves as titular head of the combined Assembly.\n The judiciary consists of the Supreme Court of Justice, the Plurinational Constitutional Court, the Judiciary Council, Agrarian and Environmental Court, and District (departmental) and lower courts. In October 2011, Bolivia held its first judicial elections to choose members of the national courts by popular vote, a reform brought about by Evo Morales.\n The Plurinational Electoral Organ is an independent branch of government which replaced the National Electoral Court in 2010. The branch consists of the Supreme Electoral Courts, the nine Departmental Electoral Court, Electoral Judges, the anonymously selected Juries at Election Tables, and Electoral Notaries.[121] Wilfredo Ovando presides over the seven-member Supreme Electoral Court. Its operations are mandated by the Constitution and regulated by the Electoral Regime Law (Law 026, passed 2010). The Organ's first elections were the country's first judicial election in October 2011, and five municipal special elections held in 2011.\n Bolivia has its constitutionally recognized capital in Sucre, while La Paz is the seat of government. La Plata (now Sucre) was proclaimed the provisional capital of the newly independent Alto Peru (later, Bolivia) on 1 July 1826.[122] On 12 July 1839, President Jos\u00e9 Miguel de Velasco proclaimed a law naming the city as the capital of Bolivia, and renaming it in honor of the revolutionary leader Antonio Jos\u00e9 de Sucre.[122] The Bolivian seat of government moved to La Paz at the start of the twentieth century as a consequence of Sucre's relative remoteness from economic activity after the decline of Potos\u00ed and its silver industry and of the Liberal Party in the War of 1899.\n The 2009 Constitution assigns the role of national capital to Sucre, not referring to La Paz in the text.[120] In addition to being the constitutional capital, the Supreme Court of Bolivia is located in Sucre, making it the judicial capital. Nonetheless, the Palacio Quemado (the Presidential Palace and seat of Bolivian executive power) is located in La Paz, as are the National Congress and Plurinational Electoral Organ. La Paz thus continues to be the seat of government.\n Despite losing its maritime coast, the so-called Litoral Department, after the War of the Pacific, Bolivia has historically maintained, as a state policy, a maritime claim to that part of Chile; the claim asks for sovereign access to the Pacific Ocean and its maritime space. The issue has also been presented before the Organization of American States; in 1979, the OAS passed the 426 Resolution,[123] which declared that the Bolivian problem is a hemispheric problem. On 4 April 1884, a truce was signed with Chile, whereby Chile gave facilities of access to Bolivian products through Antofagasta, and freed the payment of export rights in the port of Arica. In October 1904, the Treaty of Peace and Friendship was signed, and Chile agreed to build a railway between Arica and La Paz, to improve access of Bolivian products to the ports.\n The Special Economical Zone for Bolivia in Ilo (ZEEBI) is a special economic area of 5 kilometers (3.1 miles) of maritime coast, and a total extension of 358 hectares (880 acres), called Mar Bolivia (\"Sea Bolivia\"), where Bolivia may maintain a free port near Ilo, Peru under its administration and operation[124][unreliable source?] for a period of 99 years starting in 1992; once that time has passed, all the construction and territory revert to the Peruvian government. Since 1964, Bolivia has had its own port facilities in the Bolivian Free Port in Rosario, Argentina. This port is located on the Paran\u00e1 River, which is directly connected to the Atlantic Ocean.\n In 2018, Bolivia signed the UN treaty on the Prohibition of Nuclear Weapons.[125][126]\n The dispute with Chile was taken to the International Court of Justice. The court ruled in support of the Chilean position, and declared that although Chile may have held talks about a Bolivian corridor to the sea, the country was not required to negotiate one or to surrender its territory.[127]\n Bolivia is the 68th most peaceful country in the world, according to the 2024 Global Peace Index.[128]\n The Bolivian military comprises three branches: Ej\u00e9rcito (Army), Naval (Navy) and Fuerza A\u00e9rea (Air Force).\n The Bolivian army has around 31,500 men. There are six military regions (regiones militares\u2014RMs) in the army. The army is organized into ten divisions. Although it is landlocked, Bolivia keeps a navy. The Bolivian Naval Force (Fuerza Naval Boliviana in Spanish) is a naval force about 5,000 strong in 2008.[129] The Bolivian Air Force ('Fuerza A\u00e9rea Boliviana' or \"FAB\") has nine air bases, located at La Paz, Cochabamba, Santa Cruz, Puerto Su\u00e1rez, Tarija, Villamontes, Cobija, Riberalta, and Robor\u00e9.\n There are 54 prisons in Bolivia, which incarcerate around 8,700 people as of 2010[update]. The prisons are managed by the Penitentiary Regime Directorate (Spanish: Direcci\u00f3n de R\u00e9gimen Penitenciario). There are 17 prisons in departmental capital cities and 36 provincial prisons.[130]\n Bolivia has nine departments\u2014Pando, La Paz, Beni, Oruro, Cochabamba, Santa Cruz, Potos\u00ed, Chuquisaca, Tarija.\n According to what is established by the Bolivian Political Constitution, the Law of Autonomies and Decentralization regulates the procedure for the elaboration of Statutes of Autonomy, the transfer and distribution of direct competences between the central government and the autonomous entities.[131]\n There are four levels of decentralization: 1) Departmental government is constituted by the Departmental Assembly, with rights over the legislation of the department. The department governor is chosen by universal suffrage. 2) Municipal government is constituted by a Municipal Council which is responsible for legislation of the municipality. The municipality's mayor is chosen by universal suffrage. 3) Regional government is formed by several provinces or municipalities of geographical continuity within a department. It is constituted by a Regional Assembly. 4) Original indigenous government is constituted by self-governance of original indigenous people on the ancient territories where they live.\n While Bolivia's administrative divisions have similar status under governmental jurisprudence, each department varies in quantitative and qualitative factors. Generally speaking, Departments can be grouped either by geography or by political-cultural orientation. For example, Santa Cruz, Beni and Pando make up the low-lying \"Camba\" heartlands of the Amazon, Moxos and Chiquitan\u00eda. When considering political orientation, Beni, Pando, Santa Cruz, Tarija are generally grouped for regionalist autonomy movements; this region is known as the \"Media Luna\". Conversely, La Paz, Oruro, Potos\u00ed, Cochabamba have been traditionally associated with Andean politics and culture. Today, Chuquisaca vacillates between the Andean cultural bloc and the Camba bloc.[citation needed]\n Driven largely by its natural resources Bolivia has become a regional leader in measures of economic growth, fiscal stability and foreign reserves,[132] although it remains a historically poor country. Bolivia's estimated 2012 gross domestic product (GDP) totaled $27.43\u00a0billion at official exchange rate and $56.14\u00a0billion at purchasing power parity. Despite a series of mostly political setbacks, between 2006 and 2009 the Morales administration spurred growth higher than at any point in the preceding 30 years. The growth was accompanied by a moderate decrease in inequality.[133] Under Morales, per capita GDP doubled from US$1,182 in 2006 to US$2,238 in 2012. GDP growth under Morales averaged 5 percent a year, and in 2014 only Panama and the Dominican Republic performed better in all of Latin America.[134] Bolivia's nominal GDP increased from 11.5\u00a0billion in 2006 to 41\u00a0billion in 2019.[135]\n Bolivia in 2014, before a strong decline, boasted the highest proportional rate of financial reserves of any nation in the world, with Bolivia's rainy day fund totaling some US$15\u00a0billion or nearly two-thirds of total annual GDP, up from a fifth of GDP in 2005.[134]\n Agriculture is less relevant in the country's GDP compared to the rest of Latin America. The country produces close to 10 million tons of sugarcane per year and is the 10th largest producer of soybean in the world. It also has considerable yields of maize, potato, sorghum, banana, rice, and wheat. The country's largest exports are based on soy (soybean meal and soybean oil).[136] The culture of soy was brought by Brazilians to the country: in 2006, almost 50% of soy producers in Bolivia were people from Brazil, or descendants of Brazilians. The first Brazilian producers began to arrive in the country in the 1990s. Before that, there was a lot of land in the country that was not used, or where only subsistence agriculture was practiced.[137]\n Bolivia's most lucrative agricultural product continues to be coca, of which Bolivia is the world's third largest cultivator.[138][139]\n Bolivia, while historically renowned for its vast mineral wealth, is relatively under-explored in geological and mineralogical terms. The country is rich in various mineral and natural resources, sitting at the heart of South America in the Central Andes.\n \nMining is a major sector of the economy, with most of the country's exports being dependent on it.[140] In 2023, the country was the seventh largest world producer of silver;[141] fifth largest world producer of tin[142] and antimony;[143] seventh largest producer of zinc,[144] eighth largest producer of lead,[145] fourth largest world producer of boron;[146] and the sixth largest world producer of tungsten.[147] The country also has considerable gold production, which varies close to 25 tons/year, and also has amethyst extraction.[148] The country's gold production in 2015 is 12 metric tons.[149]  Bolivia has the world's largest lithium reserves, second largest antimony reserves, third largest iron ore reserves, sixth largest tin reserves, ninth largest lead, silver, and copper reserves, tenth largest zinc reserves, and undisclosed but productive reserves of gold and tungsten. Additionally, there is believed to be considerable reserves of uranium and nickel present in the country's largely under-explored eastern regions. Diamond reserves may also be present in some formations of the Serran\u00edas Chiquitanas in Santa Cruz Department.\n Bolivia has the second largest natural gas reserves in South America.[150] Its natural gas exports bring in millions of dollars per day, in royalties, rents, and taxes.[132] From 2007 to 2017, what is referred to as the \"government take\" on gas totaled approximately $22\u00a0billion.[132]\n The government held a binding referendum in 2005 on the Hydrocarbon Law. Among other provisions, the law requires that companies sell their production to the state hydrocarbons company Yacimientos Petroliferos Fiscales Bolivianos (YPFB) and for domestic demand to be met before exporting hydrocarbons and increased the state's royalties from natural gas.[151] The passage of the Hydrocarbon law in opposition to then-President Carlos Mesa can be understood as part of the Bolivian gas conflict which ultimately resulted in election of Evo Morales, Bolivia's first indigenous president.[152]\nThe US Geological Service estimates that Bolivia has 21 million tonnes of lithium, which represent at least 25% of world reserves \u2013 the largest in the world. However, to mine for it would involve disturbing the country's salt flats (called Salar de Uyuni), an important natural feature which boosts tourism in the region. The government does not want to destroy this unique natural landscape to meet the rising world demand for lithium.[153] On the other hand, sustainable extraction of lithium is attempted by the government. This project is carried out by the public company \"Recursos Evapor\u00edticos\" subsidiary of COMIBOL.\n The income from tourism has become increasingly important. Bolivia's tourist industry has placed an emphasis on attracting ethnic diversity.[154] The most visited places include Nevado Sajama, Torotoro National Park, Madidi National Park, Tiwanaku and the city of La Paz.\n The best known of the various festivals found in the country is the \"Carnaval de Oruro\", which was among the first 19 \"Masterpieces of the Oral and Intangible Heritage of Humanity\", as proclaimed by UNESCO in May 2001.[155]\n Bolivia's Yungas Road was called the \"world's most dangerous road\" by the Inter-American Development Bank, called (El Camino de la Muerte) in Spanish.[156] The northern portion of the road, much of it unpaved and without guardrails, was cut into the Cordillera Oriental Mountain in the 1930s. The fall from the narrow 12 feet (3.7\u00a0m) path is as much as 2,000 feet (610\u00a0m) in some places and due to the humid weather from the Amazon there are often poor conditions like mudslides and falling rocks.[157] Each year over 25,000 bikers cycle along the 40 miles (64\u00a0km) road. In 2018, an Israeli woman was killed by a falling rock while cycling on the road.[158]\n The Apolo road goes deep into La Paz. Roads in this area were originally built to allow access to mines located near Charazani. Other noteworthy roads run to Coroico, Sorata, the Zongo Valley (Illimani mountain), and along the Cochabamba highway (carretera).[159] According to researchers with the Center for International Forestry Research (CIFOR), Bolivia's road network was still underdeveloped as of 2014. In lowland areas of Bolivia there is less than 2,000 kilometers (2,000,000\u00a0m) of paved road. There have been some recent investments; animal husbandry has expanded in Guayaramer\u00edn, which might be due to a new road connecting Guayaramer\u00edn with Trinidad.[160] The country only opened its first duplicated highway in 2015: a 203\u00a0km stretch between the capital La Paz and Oruro.[161]\n The General Directorate of Civil Aeronautics (Direcci\u00f3n General de Aeron\u00e1utica Civil\u2014DGAC) formerly part of the FAB, administers a civil aeronautics school called the National Institute of Civil Aeronautics (Instituto Nacional de Aeron\u00e1utica Civil\u2014INAC), and two commercial air transport services TAM and TAB.\n TAM \u2013 Transporte A\u00e9reo Militar (the Bolivian Military Airline) was an airline based in La Paz, Bolivia. It was the civilian wing of the 'Fuerza A\u00e9rea Boliviana' (the Bolivian Air Force), operating passenger services to remote towns and communities in the North and Northeast of Bolivia. TAM (a.k.a. TAM Group 71) has been a part of the FAB since 1945. The airline suspended its operations since September 2019.[162]\n Boliviana de Aviaci\u00f3n, often referred to as simply BoA, is the flag carrier airline of Bolivia and is wholly owned by the country's government.[163]\n A private airline serving regional destinations is L\u00ednea A\u00e9rea Amaszonas,[164] with services including some international destinations.\n Although a civil transport airline, TAB \u2013 Transportes A\u00e9reos Bolivianos, was created as a subsidiary company of the FAB in 1977. It is subordinate to the Air Transport Management (Gerencia de Transportes A\u00e9reos) and is headed by an FAB general. TAB, a charter heavy cargo airline, links Bolivia with most countries of the Western Hemisphere; its inventory includes a fleet of Hercules C130 aircraft. TAB is headquartered adjacent to El Alto International Airport. TAB flies to Miami and Houston, with a stop in Panama.\n The three largest, and main international airports in Bolivia are El Alto International Airport in La Paz, Viru Viru International Airport in Santa Cruz, and Jorge Wilstermann International Airport in Cochabamba. There are regional airports in other cities that connect to these three hubs.[165]\n Bolivia owns a communications satellite which was offshored/outsourced and launched by China, named T\u00fapac Katari 1.[166] In 2015, it was announced that electrical power advancements include a planned $300\u00a0million nuclear reactor developed by the Russian nuclear company Rosatom.[167] Bolivia was ranked 100th in the Global Innovation Index in 2024.[168]\n Bolivia's drinking water and sanitation coverage has greatly improved since 1990 due to a considerable increase in sectoral investment. However, the country has the continent's lowest coverage levels and services are of low quality. Political and institutional instability have contributed to the weakening of the sector's institutions at the national and local levels.\n Two concessions to foreign private companies in two of the three largest cities \u2013 Cochabamba and La Paz/El Alto \u2013 were prematurely ended in 2000 and 2006 respectively. The country's second largest city, Santa Cruz de la Sierra, manages its own water and sanitation system relatively successfully by way of cooperatives. The government of Evo Morales intends to strengthen citizen participation within the sector. Increasing coverage requires a substantial increase of investment financing.\n According to the government the main problems in the sector are low access to sanitation throughout the country; low access to water in rural areas; insufficient and ineffective investments; a low visibility of community service providers; a lack of respect of indigenous customs; \"technical and institutional difficulties in the design and implementation of projects\"; a lack of capacity to operate and maintain infrastructure; an institutional framework that is \"not consistent with the political change in the country\"; \"ambiguities in the social participation schemes\"; a reduction in the quantity and \nquality of water due to climate change; pollution and a lack of integrated water resources management; and the lack of policies and programs for the reuse of wastewater.[169]\n Only 27% of the population has access to improved sanitation, 80 to 88% has access to improved water sources. Coverage in urban areas is bigger than in rural ones.[170]\n The agrarian reform promised by Evo Morales \u2013 and approved in a referendum by nearly 80 per cent of the population \u2013 has never been implemented. Intended to abolish latifundism by reducing the maximum size of properties that do not have an \"economic and social function\" to 5,000 hectares, with the remainder to be distributed among small agricultural workers and landless indigenous people, it was strongly opposed by the Bolivian oligarchy. In 2009, the government gave in to the agribusiness sector, which in return committed to end the pressure it was exerting and jeopardizing until the new constitution was in place.[171]\n However, a series of economic reforms and projects have improved the condition of modest peasant families. They received farm machinery, tractors, fertilizers, seeds and breeding stock, while the state built irrigation systems, roads and bridges to make it easier for them to sell their produce in the markets. The situation of many indigenous people and small farmers was regularized through the granting of land titles for the land they were using.[171]\n In 2007, the government created a \"Bank for Productive Development\" through which small workers and agricultural producers can borrow easily, at low rates and with repayment terms adapted to agricultural cycles. As a result of improved banking supervision, borrowing rates have been reduced by a factor of three between 2014 and 2019 across all banking institutions for small and medium-sized agricultural producers. In addition, the law now requires banks to devote at least 60% of their resources to productive credits or to the construction of social housing.[171]\n With the creation of the Food Production Support Enterprise (Emapa), the government sought to stabilize the domestic market for agricultural products by buying the best prices for the production of small and medium-sized farmers, thus forcing agribusinesses to offer them fairer remuneration. According to Vice President \u00c0lvaro Garc\u00eda Linera, \"by setting the rules of the game, the State establishes a new balance of power that gives more power to small producers. Wealth is better redistributed to balance the power of the agribusiness sector. This generates stability, which allows the economy to flourish and benefits everyone.[171]\n According to the last two censuses carried out by the Bolivian National Statistics Institute (Instituto Nacional de Estad\u00edstica, INE), the population increased from 8,274,325 (from which 4,123,850 were men and 4,150,475 were women) in 2001 to 10,059,856 in 2012.[172]\n In the last fifty years the Bolivian population has tripled, reaching a population growth rate of 2.25%. The growth of the population in the inter-census periods (1950\u20131976 and 1976\u20131992) was approximately 2.05%, while between the last period, 1992\u20132001, it reached 2.74% annually.\n Some 67.49% of Bolivians live in urban areas, while the remaining 32.51% in rural areas. The most part of the population (70%) is concentrated in the departments of La Paz, Santa Cruz and Cochabamba. In the Andean Altiplano region the departments of La Paz and Oruro hold the largest percentage of population, in the valley region the largest percentage is held by the departments of Cochabamba and Chuquisaca, while in the Llanos region by Santa Cruz and Beni. At national level, the population density is 8.49, with variations marked between 0.8 (Pando Department) and 26.2 (Cochabamba Department).\n The largest population center is located in the so-called \"central axis\" and in the Llanos region. Bolivia has a young population. According to the 2011 census, 59% of the population is between 15 and 59 years old, 39% is less than 15 years old. Almost 60% of the population is younger than 25 years of age.\n Ethno-racial groups in Bolivia (2012 census)[173]\n The vast majority of Bolivians are mestizo (with the indigenous component higher than the European one), although the government has not included the cultural self-identification \"mestizo\" in the November 2012 census.[174] There are approximately three dozen native groups totaling approximately half of the Bolivian population \u2013 the largest proportion of indigenous people in the Americas. A 2018 estimate of racial classification put mestizo (mixed White and Amerindian) at 68%, indigenous at 20%, white at 5%, cholo at 2%, black at 1%, other at 4%, while 2% were unspecified; 44% attributed themselves to some indigenous group, predominantly the linguistic categories of Quechuas or Aymaras.[4] White Bolivians comprised about 14% of the population in 2006, and are usually concentrated in the largest cities: La Paz, Santa Cruz de la Sierra and Cochabamba, but as well in some minor cities like Tarija and Sucre. The ancestry of whites and the white ancestry of mestizos lies within Europe and the Middle East, most notably Spain, Italy, Germany, Croatia, Lebanon and Syria. In the Santa Cruz Department, there are several dozen colonies of German-speaking Mennonites from Russia totaling around 40,000 inhabitants (as of 2012[update]).[175]\n Afro-Bolivians, descendants of African slaves who arrived in the time of the Spanish Empire, inhabit the department of La Paz, and are located mainly in the provinces of Nor Yungas and Sud Yungas. Slavery was abolished in Bolivia in 1831.[176] There are also important communities of Japanese (14,000[177]) and Lebanese (12,900[178]).\n Indigenous peoples, also called \"originarios\" (\"native\" or \"original\") and less frequently, Amerindians, could be categorized by geographic area, such as Andean, like the Aymaras and Quechuas (who formed the ancient Inca Empire), who are concentrated in the western departments of La Paz, Potos\u00ed, Oruro, Cochabamba and Chuquisaca. There also are ethnic populations in the east, composed of the Chiquitano, Chan\u00e9, Guaran\u00ed and Moxos, among others, who inhabit the departments of Santa Cruz, Beni, Tarija and Pando.\n There are small numbers of European citizens from Germany, France, Italy and Portugal, as well as from other countries of the Americas, as Argentina, Brazil, Chile, Colombia, Cuba, Ecuador, the United States, Paraguay, Peru, Mexico and Venezuela, among others. There are important Peruvian colonies in La Paz, El Alto and Santa Cruz de la Sierra.\n There are around 140,000 Mennonites in Bolivia of Friesian, Flemish and German ethnic origins.[179][180]\n Bolivia has great linguistic diversity as a result of its multiculturalism. The Constitution of Bolivia recognizes 36 official languages besides Spanish: Aymara, Araona, Baure, B\u00e9siro, Canichana, Cavine\u00f1o, Cayubaba, Ch\u00e1cobo, Chim\u00e1n, Ese Ejja, Guaran\u00ed, Guarasu'we, Guarayu, Itonama, Leco, Machajuyai-Kallawaya, Machineri, Maropa, Moje\u00f1o-Ignaciano, Moje\u00f1o-Trinitario, Mor\u00e9, Moset\u00e9n, Movima, Pacawara, Puquina, Quechua, Sirion\u00f3, Tacana, Tapiet\u00e9, Toromona, Uru-Chipaya, Weenhayek, Yaminawa, Yuki, Yuracar\u00e9, and Zamuco.[181]\n Spanish is the most spoken official language in the country, according to the 2001 census; as it is spoken by two-thirds of the population. All legal and official documents issued by the State, including the Constitution, the main private and public institutions, the media, and commercial activities, are in Spanish.\n The main indigenous languages are: Quechua (21.2% of the population in the 2001 census), Aymara (14.6%), Guarani (0.6%) and others (0.4%) including the Moxos in the department of Beni.[4]\n Plautdietsch, a German dialect, is spoken by about 70,000 Mennonites in Santa Cruz. Portuguese is spoken mainly in the areas close to Brazil.\n Religion in Bolivia (2014)[182]\n Bolivia is a constitutionally secular state that guarantees the freedom of religion and the independence of government from religion.[183]\n According to the 2001 census conducted by the National Institute of Statistics of Bolivia, 78% of the population is Roman Catholic, followed by 19% that are Protestant, as well as a small number of Bolivians that are Orthodox, and 3% non-religious.[184][185]\n The Association of Religion Data Archives (relying on the World Christian Database) records that in 2010, 92.5% of Bolivians identified as Christian (of any denomination), 3.1% identified with indigenous religion, 2.2% identified as Bah\u00e1\u02bc\u00ed, 1.9% identified as agnostic, and all other groups constituted 0.1% or less.[186]\n Much of the indigenous population adheres to different traditional beliefs marked by inculturation or syncretism with Christianity. The cult of Pachamama,[187] or \"Mother Earth\", is notable. The veneration of the Virgin of Copacabana, Virgin of Urkupi\u00f1a and Virgin of Socav\u00f3n, is also an important feature of Christian pilgrimage. There also are important Aymaran communities near Lake Titicaca that have a strong devotion to James the Apostle.[188] Deities worshiped in Bolivia include Ekeko, the Aymaran god of abundance and prosperity, whose day is celebrated every 24 January, and Tup\u00e1, a god of the Guaran\u00ed people.\n Approximately 67% of Bolivians live in urban areas,[189] among the lowest proportion in South America. Nevertheless, the rate of urbanization is growing steadily, at around 2.5% annually. According to the 2012 census, there are total of 3,158,691 households in Bolivia \u2013 an increase of 887,960 from 2001.[172] In 2009, 75.4% of homes were classified as a house, hut, or Pahuichi; 3.3% were apartments; 21.1% were rental residences; and 0.1% were mobile homes.[190] Most of the country's largest cities are located in the highlands of the west and central regions.\n [191]\n Bolivian culture has been heavily influenced by the Spanish, the Aymara, the Quechua, as well as the popular cultures of Latin America as a whole.\n The cultural development is divided into three distinct periods: precolumbian, colonial, and republican. Important archaeological ruins, gold and silver ornaments, stone monuments, ceramics, and weavings remain from several important pre-Columbian cultures. Major ruins include Tiwanaku, El Fuerte de Samaipata, Inkallaqta and Iskanwaya. The country abounds in other sites that are difficult to reach and have seen little archaeological exploration.[192]\n The Spanish brought their own tradition of religious art which, in the hands of local native, mestizo and some criollo builders and artisans, developed into a rich and distinctive style of architecture, painting, and sculpture known as Andean Baroque. The colonial period produced not only the paintings of P\u00e9rez de Holgu\u00edn, Flores, Bitti, and others but also the works of skilled but unknown stonecutters, woodcarvers, goldsmiths, and silversmiths. An important body of Native Baroque religious music of the colonial period was recovered and has been performed internationally to wide acclaim since 1994.[192]\n Bolivian artists of stature in the 20th century include Mar\u00eda Luisa Pacheco, Roberto Mamani Mamani, Alejandro Mario Yllanes, Alfredo Da Silva, and Marina N\u00fa\u00f1ez del Prado.\n Bolivia has a rich folklore. Its regional folk music is distinctive and varied. The \"devil dances\" at the annual carnival of Oruro are one of the great folkloric events of South America, as is the lesser known carnival at Tarabuco.[192]\n In 2008, following UNESCO standards, Bolivia was declared free of illiteracy, making it the fourth country in South America to attain this status.[193]\n Bolivia has public and private universities. Among them: Universidad Mayor, Real y Pontificia de San Francisco Xavier de Chuquisaca USFX \u2013 Sucre, founded in 1624; Universidad Mayor de San Andr\u00e9s UMSA \u2013 La Paz, founded in 1830; Universidad Mayor de San Simon UMSS \u2013 Cochabamba, founded in 1832; Universidad Aut\u00f3noma Gabriel Ren\u00e9 Moreno UAGRM \u2013 Santa Cruz de la Sierra, founded in 1880; Universidad T\u00e9cnica de Oruro UTO \u2013 Oruro, founded in 1892; Universidad Evang\u00e9lica Boliviana UEB \u2013 Santa Cruz de la Sierra, founded in 1980; and Universidad Aut\u00f3noma Tom\u00e1s Fr\u00edas UATF \u2013 Potosi, founded in 1892.\n According to UNICEF under-five mortality rate in 2006 was 52.7 per 1000 and was reduced to 26 per 1000 by 2019.[194] The infant mortality rate was 40.7 per 1000 in 2006 and was reduced to 21.2 per 1000 in 2019.[195] Before Morales took office, nearly half of all infants were not vaccinated; now nearly all are vaccinated. Morales also put into place several supplemental nutrition programs, including an effort to supply free food in public health and social security offices, and his desnutrici\u00f3n cero (zero malnutrition) program provides free school lunches.[134]\n Between 2006 and 2016, extreme poverty in Bolivia fell from 38.2% to 16.8%. Chronic malnutrition in children under five years of age also went down by 14% and the child mortality rate was reduced by more than 50%, according to World Health Organization.[196] In 2019 the Bolivian government created a universal healthcare system which has been cited as a model for all by the World Health Organization.[197]\n Bolivia has one of the highest rates of femicide and gender-based violence in Latin America.[198] In 2013, the Comprehensive Law to Guarantee Women a Life Free from Violence was passed, which codified sixteen types of gender-based violence and implemented measures for prevention of violence, protection for victims, and the punishment of aggressors.\n As of 2022, 46% of parliamentary seats are held by women.[199] A 1997 law established quotas whereby candidates for public office fielded by political parties must be at least 30% women.[200]\n Football is popular. The national team is the Bolivia national football team.\n Racquetball is the second most popular sport in Bolivia as for the results in the Odesur 2018 Games held in Cochabamba.[201][202] Bolivia has won 18 medals at the Pan American Games and 15 of them came from racquetball events, including their only gold medals, won in the Men's Team event in 2019 and 2023, plus a Men's Singles Gold in 2023 by world champion Conrrado Moscoso.\n Basketball is especially popular and influential in the Potos\u00ed Department.[203]\n Attribution:\n 16\u00b042\u203243\u2033S 64\u00b039\u203258\u2033W\ufeff / \ufeff16.712\u00b0S 64.666\u00b0W\ufeff / -16.712; -64.666\n"
    },
    {
        "title": "Mikhail Gorbachev",
        "url": "https://en.wikipedia.org/wiki/Mikhail_Gorbachev",
        "content": "\n Former General Secretary of the CPSU\nFormer President of the Soviet Union\n General Secretariate (1985\u20131991)\n Presidency (1990\u20131991)\n Foreign policy\n Post-leadership\n \n Mikhail Sergeyevich Gorbachev[f][g] (2 March 1931 \u2013 30 August 2022) was a Russian politician and statesman who served as the last leader of the Soviet Union from 1985 to the country's dissolution in 1991. He served as General Secretary of the Communist Party of the Soviet Union from 1985 and additionally as head of state beginning in 1988, as Chairman of the Presidium of the Supreme Soviet from 1988 to 1989, Chairman of the Supreme Soviet from 1989 to 1990 and the president of the Soviet Union from 1990 to 1991. Ideologically, Gorbachev initially adhered to Marxism\u2013Leninism but moved towards social democracy by the early 1990s. He was the only Soviet leader born after the country's foundation.\n Gorbachev was born in Privolnoye, Russian SFSR, to a poor peasant family of Russian and Ukrainian heritage. Growing up under the rule of Joseph Stalin in his youth, he operated combine harvesters on a collective farm before joining the Communist Party, which then governed the Soviet Union as a one-party state. Studying at Moscow State University, he married fellow student Raisa Titarenko in 1953 and received his law degree in 1955. Moving to Stavropol, he worked for the Komsomol youth organization and, after Stalin's death, became a keen proponent of the de-Stalinization reforms of Soviet leader Nikita Khrushchev. He was appointed the First Party Secretary of the Stavropol Regional Committee in 1970, overseeing the construction of the Great Stavropol Canal. In 1978, he returned to Moscow to become a Secretary of the party's Central Committee; he joined the governing Politburo (25th term) as a non-voting member in 1979 and a voting member in 1980. Three years after the death of Soviet leader Leonid Brezhnev\u2014following the brief tenures of Yuri Andropov and Konstantin Chernenko\u2014in 1985, the Politburo elected Gorbachev as general secretary, the de facto leader.\n Although committed to preserving the Soviet state and its Marxist\u2013Leninist ideals, Gorbachev believed significant reform was necessary for its survival. He withdrew troops from the Soviet\u2013Afghan War and embarked on summits with United States president Ronald Reagan to limit nuclear weapons and end the Cold War. Domestically, his policy of glasnost (\"openness\") allowed for enhanced freedom of speech and press, while his perestroika (\"restructuring\") sought to decentralize economic decision-making to improve its efficiency. Ultimately, Gorbachev's democratization measures and formation of the elected Congress of People's Deputies undermined the one-party state. When various Warsaw Pact countries abandoned Marxist\u2013Leninist governance in 1989, he declined to intervene militarily. Growing nationalist sentiment within constituent republics threatened to break up the Soviet Union, leading the hardliners within the Communist Party to launch an unsuccessful coup against Gorbachev in August 1991. In the coup's wake, the Soviet Union dissolved against Gorbachev's wishes. After resigning from the presidency, he launched the Gorbachev Foundation, became a vocal critic of Russian presidents Boris Yeltsin and Vladimir Putin, and campaigned for Russia's social-democratic movement.\n Gorbachev is considered one of the most significant figures of the second half of the 20th century. The recipient of a wide range of awards, including the Nobel Peace Prize, in the West he is praised for his role in ending the Cold War, introducing new political and economic freedoms in the Soviet Union, and tolerating both the fall of Marxist\u2013Leninist administrations in eastern and central Europe and the German reunification. Gorbachev has a complicated legacy in Russia. While in power, he had net positive approval ratings, being viewed as a reformer and changemaker. However, as the Soviet Union collapsed as a result of these reforms, so did his approval rating; contemporary Russians often deride him for weakening Russia's global influence and precipitating an economic collapse in the country. His unsuccessful run for president in 1996 showed, despite neoliberal reforms in Russia at the time, mass unpopularity with the results of his administration and possibly regret for the collapse of the USSR.\n Gorbachev was born on 2 March 1931 in the village of Privolnoye, then in the North Caucasus Krai of the Russian Soviet Federative Socialist Republic, Soviet Union.[4] At the time, Privolnoye was divided almost evenly between ethnic Russians and ethnic Ukrainians.[5] Gorbachev's paternal family were ethnic Russians and had moved to the region from Voronezh several generations before; his maternal family were of ethnic Ukrainian heritage and had migrated from Chernihiv.[6] His parents named him Viktor at birth, but at the insistence of his mother\u2014a devout Orthodox Christian\u2014he had a secret baptism, where his grandfather christened him Mikhail.[7] His relationship with his father, Sergey Andreyevich Gorbachev, was close; his mother, Maria Panteleyevna Gorbacheva (n\u00e9e Gopkalo), was colder and punitive.[8] His parents were poor,[9] and lived as peasants.[10] They had married as teenagers in 1928,[11] and in keeping with local tradition had initially resided in Sergey's father's house, an adobe-walled hut, before a hut of their own could be built.[12]\n The Soviet Union was a one-party state governed by the Communist Party, and during Gorbachev's childhood was under the leadership of Joseph Stalin. Stalin had initiated a project of mass rural collectivization which, in keeping with his Marxist\u2013Leninist ideas, he believed would help convert the country into a socialist society.[13] Gorbachev's maternal grandfather joined the Communist Party and helped form the village's first kolkhoz (collective farm) in 1929, becoming its chair.[14] This farm was 19 kilometres (12\u00a0mi) outside Privolnoye village and when he was three years old, Gorbachev left his parental home and moved into the kolkhoz with his maternal grandparents.[15]\n The country was then experiencing the famine of 1930\u20131933, in which two of Gorbachev's paternal uncles and an aunt died.[16] This was followed by the Great Purge, in which individuals accused of being \"enemies of the people\", including those sympathetic to rival interpretations of Marxism like Trotskyism, were arrested and interned in labor camps, if not executed. Both of Gorbachev's grandfathers were arrested (his maternal in 1934 and his paternal in 1937) and spent time in Gulag labor camps before being released.[17] After his December 1938 release, Gorbachev's maternal grandfather discussed having been tortured by the secret police, an account that influenced the young boy.[18]\n Following on from the outbreak of the Second World War in 1939, in June 1941 the German Army invaded the Soviet Union. German forces occupied Privolnoye for four and a half months in 1942.[19] Gorbachev's father had joined the Red Army and fought on the frontlines; he was wrongly declared dead during the conflict and fought in the Battle of Kursk before returning to his family, injured.[20] After Germany was defeated, Gorbachev's parents had their second son, Aleksandr, in 1947; he and Mikhail would be their only children.[11]\n The village school was closed during much of the war but re-opened in autumn 1944.[21] Gorbachev did not want to return but when he did he excelled academically.[22] He read voraciously, moving from the Western novels of Thomas Mayne Reid to the works of Vissarion Belinsky, Alexander Pushkin, Nikolai Gogol, and Mikhail Lermontov.[23] In 1946, he joined the Komsomol, the Soviet political youth organization, becoming leader of his local group and then being elected to the Komsomol committee for the district.[24] From primary school he moved to the high school in Molotovskoye; he stayed there during the week while walking the 19\u00a0km (12\u00a0mi) home during weekends.[25] As well as being a member of the school's drama society,[26] he organized sporting and social activities and led the school's morning exercise class.[27] Over the course of five consecutive summers from 1946 onward he returned home to assist his father in operating a combine harvester, during which they sometimes worked 20-hour days.[28] In 1948, they harvested over 8,000 centners of grain, a feat for which Sergey was awarded the Order of Lenin and his son the Order of the Red Banner of Labour.[29]\n I would consider it a high honour to be a member of the highly advanced, genuinely revolutionary Communist Party of Bolsheviks. I promise to be faithful to the great cause of Lenin and Stalin, to devote my entire life to the party's struggle for Communism.\n In June 1950, Gorbachev became a candidate member of the Communist Party.[30] He also applied to study at the law school of Moscow State University (MSU), then the most prestigious university in the country. They accepted him without asking for an exam, likely because of his worker-peasant origins and his possession of the Order of the Red Banner of Labour.[31] His choice of law was unusual; it was not a well-regarded subject in Soviet society at that time.[32] At age 19, he traveled by train to Moscow, the first time he had left his home region.[33]\n In Moscow, Gorbachev resided with fellow MSU students at a dormitory in the Sokolniki District.[34] He and other rural students felt at odds with their Muscovite counterparts, but he soon came to fit in.[35] Fellow students recall him working especially hard, often late into the night.[36] He gained a reputation as a mediator during disputes[37] and was also known for being outspoken in class, although he would reveal some of his views only privately; for instance, he confided in some students his opposition to the Soviet jurisprudential norm that a confession proved guilt, noting that confessions could have been forced.[38] During his studies, an antisemitic campaign spread through the Soviet Union, culminating in the Doctors' plot; Gorbachev publicly defended Volodya Liberman, a Jewish student who was accused of disloyalty to the country by one of his fellows.[39]\n At MSU, Gorbachev became the Komsomol head of his entering class, and then Komsomol's deputy secretary for agitation and propaganda at the law school.[40] One of his first Komsomol assignments in Moscow was to monitor the election polling in Presnensky District to ensure the government's desire for near-total turnout; Gorbachev found that most of those who voted did so \"out of fear\".[41] In 1952, he was appointed a full member of the Communist Party.[42] As a party and Komsomol member, he was tasked with monitoring fellow students for potential subversion; some of his fellow students said that he did so only minimally and that they trusted him to keep confidential information secret from the authorities.[43] Gorbachev became close friends with Zden\u011bk Mlyn\u00e1\u0159, a Czechoslovak student who later became a primary ideologist of the 1968 Prague Spring. Mlyn\u00e1\u0159 recalled that the duo remained committed Marxist\u2013Leninists despite their growing concerns about the Stalinist system.[44] After Stalin died in March 1953, Gorbachev and Mlyn\u00e1\u0159 joined the crowds massing to see Stalin's body lying in state.[45]\n At MSU, Gorbachev met Raisa Titarenko, who was studying in the university's philosophy department.[46] She was engaged to another man, but after that engagement fell apart, she began a relationship with Gorbachev;[47] together they went to bookstores, museums, and art exhibits.[48] In early 1953, he took an internship at the procurator's office in Molotovskoye district, but he was angered by the incompetence and arrogance of those working there.[49] That summer, he returned to Privolnoye to work with his father on the harvest; the money earned allowed him to pay for a wedding.[50] On 25 September 1953 he and Raisa registered their marriage at Sokolniki Registry Office[50] and in October moved in together at the Lenin Hills dormitory.[51] Raisa discovered that she was pregnant and although the couple wanted to keep the child she fell ill and required a life-saving abortion.[52]\n In June 1955, Gorbachev graduated with a distinction;[53] his final paper had been on the advantages of \"socialist democracy\" (the Soviet political system) over \"bourgeois democracy\" (liberal democracy).[54] He was subsequently assigned to the Soviet Procurator's office, which was then focusing on the rehabilitation of the innocent victims of Stalin's purges, but found that they had no work for him.[55] He was then offered a place on an MSU graduate course specializing in kolkhoz law, but declined.[56] He had wanted to remain in Moscow, where Raisa was enrolled in a PhD program, but instead gained employment in Stavropol; Raisa abandoned her studies to join him there.[57]\n In August 1955, Gorbachev started work at the Stavropol regional procurator's office, but disliked the job and used his contacts to get a transfer to work for Komsomol,[58] becoming deputy director of Komsomol's agitation and propaganda department for that region.[59] In this position, he visited villages in the area and tried to improve the lives of their inhabitants; he established a discussion circle in Gorkaya Balka village to help its peasant residents gain social contacts.[60]\n Mikhail Gorbachev and his wife Raisa initially rented a small room in Stavropol,[61] taking daily evening walks around the city and on weekends hiking in the countryside.[62] In January 1957, Raisa gave birth to a daughter, Irina,[63] and in 1958 they moved into two rooms in a communal apartment.[64] In 1961, Gorbachev pursued a second degree, in agricultural production; he took a correspondence course from the local Stavropol Agricultural Institute, receiving his diploma in 1967.[65] His wife had also pursued a second degree, attaining a PhD in sociology in 1967 from the Moscow State Pedagogical University;[66] while in Stavropol she too joined the Communist Party.[67]\n Stalin was ultimately succeeded as Soviet leader by Nikita Khrushchev, who denounced Stalin and his cult of personality in a speech given in February 1956, after which he launched a de-Stalinization process throughout Soviet society.[68] Later biographer William Taubman suggested that Gorbachev \"embodied\" the \"reformist spirit\" of the Khrushchev era.[69] Gorbachev was among those who saw themselves as \"genuine Marxists\" or \"genuine Leninists\" in contrast to what they regarded as the perversions of Stalin.[70] He helped spread Khrushchev's anti-Stalinist message in Stavropol, but encountered many who continued to regard Stalin as a hero or who praised the Stalinist purges as just.[71]\n Gorbachev rose steadily through the ranks of the local administration.[72] The authorities regarded him as politically reliable,[73] and he would flatter his superiors, for instance gaining favor with prominent local politician Fyodor Kulakov.[74] With an ability to outmanoeuvre rivals, some colleagues resented his success.[75] In September 1956, he was promoted First Secretary of the Stavropol city's Komsomol, placing him in charge of it;[76] in April 1958 he was made deputy head of the Komsomol for the entire region.[77] At this point he was given better accommodation: a two-room flat with its own private kitchen, toilet, and bathroom.[78] In Stavropol, he formed a discussion club for youths,[79] and helped mobilize local young people to take part in Khrushchev's agricultural and development campaigns.[80]\n In March 1961, Gorbachev became First Secretary of the regional Komsomol,[81] in which position he went out of his way to appoint women as city and district leaders.[82] In 1961, Gorbachev played host to the Italian delegation for the World Youth Festival in Moscow;[83] that October, he also attended the 22nd Congress of the Communist Party of the Soviet Union.[84] In January 1963, Gorbachev was promoted to personnel chief for the regional party's agricultural committee,[85] and in September 1966 became First Secretary of the Stavropol City Party Organization (\"Gorkom\").[86] By 1968 he was increasingly frustrated with his job\u2014in large part because Khrushchev's reforms were stalling or being reversed\u2014and he contemplated leaving politics to work in academia.[87] However, in August 1968, he was named Second Secretary of the Stavropol Kraikom, making him the deputy of First Secretary Leonid Yefremov and the second most senior figure in Stavropol Krai.[88] In 1969, he was elected as a deputy to the Supreme Soviet of the Soviet Union and made a member of its Standing Commission for the Protection of the Environment.[89]\n Cleared for travel to Eastern Bloc countries, in 1966 he was part of a delegation which visited East Germany, and in 1969 and 1974 visited Bulgaria.[90] In August 1968 the Soviet Union led an invasion of Czechoslovakia to put an end to the Prague Spring, a period of political liberalization in the Marxist\u2013Leninist country. Although Gorbachev later stated that he had had private concerns about the invasion, he publicly supported it.[91] In September 1969 he was part of a Soviet delegation sent to Czechoslovakia, where he found the Czechoslovak people largely unwelcoming to them.[92] That year, the Soviet authorities ordered him to punish Fagim B. Sadygov, a philosophy professor of the Stavropol agricultural institute whose ideas were regarded as critical of Soviet agricultural policy; Gorbachev ensured that Sadykov was fired from teaching but ignored calls for him to face tougher punishment.[93] Gorbachev later related that he was \"deeply affected\" by the incident; \"my conscience tormented me\" for overseeing Sadykov's persecution.[94]\n In April 1970, Yefremov was promoted to a higher position in Moscow and Gorbachev succeeded him as the First Secretary of the Stavropol kraikom. This granted Gorbachev significant power over the Stavropol region.[95] He had been personally vetted for the position by senior Kremlin leaders and was informed of their decision by the Soviet leader, Leonid Brezhnev.[96] Aged 39, he was considerably younger than his predecessors in the position.[97] As head of the Stavropol region, he automatically became a member of the Central Committee of the Communist Party of the Soviet Union (24th term) in 1971.[98] According to biographer Zhores Medvedev, Gorbachev \"had now joined the Party's super-elite\".[99] As regional leader, Gorbachev initially attributed economic and other failures to \"the inefficiency and incompetence of cadres, flaws in management structure or gaps in legislation\", but eventually concluded that they were caused by an excessive centralization of decision making in Moscow.[100] He began reading translations of restricted texts by Western Marxist authors such as Antonio Gramsci, Louis Aragon, Roger Garaudy, and Giuseppe Boffa, and came under their influence.[100]\n Gorbachev's main task as regional leader was to raise agricultural production levels, a task hampered by severe droughts in 1975 and 1976.[101] He oversaw the expansion of irrigation systems through construction of the Great Stavropol Canal.[102] For overseeing a record grain harvest in Ipatovsky district, in March 1972 he was awarded the Order of the October Revolution by Brezhnev in a Moscow ceremony.[103] Gorbachev always sought to maintain Brezhnev's trust;[104] as regional leader, he repeatedly praised Brezhnev in his speeches, for instance referring to him as \"the outstanding statesman of our time\".[105] Gorbachev and his wife holidayed in Moscow, Leningrad, Uzbekistan, and resorts in the North Caucasus;[106] he holidayed with the head of the KGB, Yuri Andropov, who was favorable towards him and who became an important patron.[107] Gorbachev also developed good relationships with senior figures including the Soviet prime minister, Alexei Kosygin,[108] and the longstanding senior party member Mikhail Suslov.[109]\n The government considered Gorbachev sufficiently reliable that he was sent as part of Soviet delegations to Western Europe; he made five trips there between 1970 and 1977.[110] In September 1971 he was part of a delegation that traveled to Italy, where they met with representatives of the Italian Communist Party; Gorbachev loved Italian culture but was struck by the poverty and inequality he saw in the country.[111] In 1972, he visited Belgium and the Netherlands, and in 1973 West Germany.[112] Gorbachev and his wife visited France in 1976 and 1977, on the latter occasion touring the country with a guide from the French Communist Party.[113] He was surprised by how openly West Europeans offered their opinions and criticized their political leaders, something absent from the Soviet Union, where most people did not feel safe speaking so openly.[114] He later related that for him and his wife, these visits \"shook our a priori belief in the superiority of socialist over bourgeois democracy\".[115]\n Gorbachev had remained close to his parents; after his father became terminally ill in 1974, Gorbachev traveled to be with him in Privolnoe shortly before his death.[116] His daughter, Irina, married fellow student Anatoly Virgansky in April 1978.[117] In 1977, the Supreme Soviet appointed Gorbachev to chair the Standing Commission on Youth Affairs due to his experience with mobilizing young people in Komsomol.[118]\n In November 1978, Gorbachev was appointed a Secretary of the Central Committee.[119] His appointment had been approved unanimously by the Central Committee's members.[120] To fill this position, Gorbachev and his wife moved to Moscow, where they were initially given an old dacha outside the city. They then moved to another, at Sosnovka, before finally being allocated a newly built brick house.[121] He was also given an apartment inside the city, but gave that to his daughter and son-in-law; Irina had begun work at Moscow's Second Medical Institute.[122] As part of the Moscow political elite, Gorbachev and his wife now had access to better medical care and to specialized shops; they were also given cooks, servants, bodyguards, and secretaries, although many of these were spies for the KGB.[123] In his new position, Gorbachev often worked twelve to sixteen hour days.[123] He and his wife socialized little, but liked to visit Moscow's theaters and museums.[124]\n In 1978, Gorbachev was appointed to the Central Committee's Secretariat for Agriculture (25th term), replacing his old patron Kulakov, who had died of a heart attack.[125] Gorbachev concentrated his attentions on agriculture: the harvests of 1979, 1980, and 1981 were all poor, due largely to weather conditions,[126] and the country had to import increasing quantities of grain.[127] He had growing concerns about the country's agricultural management system, coming to regard it as overly centralized and requiring more bottom-up decision making;[128] he raised these points at his first speech at a Central Committee Plenum, given in July 1978.[129] He began to have concerns about other policies too. In December 1979, the Soviets sent the armed forces into neighbouring Afghanistan to support its Soviet-aligned government against Islamist insurgents; Gorbachev privately thought it a mistake.[130] At times he openly supported the government position; in October 1980 he for instance endorsed Soviet calls for Poland's Marxist\u2013Leninist government to crack down on growing internal dissent in that country.[130] That same month, he was promoted from a candidate member to a full member of the Politburo (25th term), becoming the youngest member of the highest decision-making authority in the Communist Party.[131] After Brezhnev's death in November 1982, Andropov succeeded him as General Secretary of the Communist Party, the de facto leader in the Soviet Union. Gorbachev was enthusiastic about the appointment.[132] However, although Gorbachev hoped that Andropov would introduce liberalizing reforms, the latter carried out only personnel shifts rather than structural change.[133] Gorbachev became Andropov's closest ally in the Politburo;[134] with Andropov's encouragement, Gorbachev sometimes chaired Politburo meetings.[135] Andropov encouraged Gorbachev to expand into policy areas other than agriculture, preparing him for future higher office.[136] In April 1983, in a sign of growing ascendancy, Gorbachev delivered the annual speech marking the birthday of the Soviet founder Vladimir Lenin;[137] this required him re-reading many of Lenin's later writings, in which the latter had called for reform in the context of the New Economic Policy of the 1920s, and encouraged Gorbachev's own conviction that reform was needed.[138] In May 1983, Gorbachev was sent to Canada, where he met Prime Minister Pierre Trudeau and spoke to the Canadian Parliament.[139] There, he met and befriended the Soviet ambassador, Aleksandr Yakovlev, who later became a key political ally.[140]\n In February 1984, Andropov died; on his deathbed he indicated his desire that Gorbachev succeed him.[141] Many in the Central Committee nevertheless thought the 53-year-old Gorbachev was too young and inexperienced.[142] Instead, Konstantin Chernenko\u2014a longstanding Brezhnev ally\u2014was appointed general secretary, but he too was in very poor health.[143] Chernenko was often too sick to chair Politburo meetings, with Gorbachev stepping in last minute.[144] Gorbachev continued to cultivate allies both in the Kremlin and beyond,[145] and also gave the main speech at a conference on Soviet ideology, where he angered party hardliners by implying that the country required reform.[146]\n In April 1984, Gorbachev was appointed chair of the Foreign Affairs Committee of the Soviet legislature, a largely honorific position.[147] In June he traveled to Italy as a Soviet representative for the funeral of Italian Communist Party leader Enrico Berlinguer,[148] and in September to Sofia, Bulgaria to attend celebrations of the fortieth anniversary of its liberation from the Nazis by the Red Army.[149] In December, he visited Britain at the request of its prime minister Margaret Thatcher; she was aware that he was a potential reformer and wanted to meet him.[150] At the end of the visit, Thatcher said: \"I like Mr. Gorbachev. We can do business together\".[151] He felt that the visit helped to erode Andrei Gromyko's dominance of Soviet foreign policy while at the same time sending a signal to the United States government that he wanted to improve Soviet\u2013US relations.[152]\n On 10 March 1985, Chernenko died, forcing the Politburo to choose a new General Secretary.[153] Gromyko proposed Gorbachev as the next general secretary; as a longstanding party member, Gromyko's recommendation carried great weight among the Central Committee.[154] Gorbachev expected much opposition to his nomination as general secretary, but ultimately the rest of the Politburo supported him.[155] Shortly after Chernenko's death, the Politburo unanimously elected Gorbachev as his successor; they wanted him rather than another elderly leader.[156] He thus became the eighth leader of the Soviet Union.[10] Few in the government imagined that he would be as radical a reformer as he proved.[157] Although he was not a well-known figure to the Soviet public, there was widespread relief that the new leader was not elderly and ailing.[158] Gorbachev's first public appearance as leader was at Chernenko's Red Square funeral, held on 14 March.[159]\n Gorbachev's leadership style differed from that of his predecessors. He would stop to talk to civilians on the street, forbade the display of his portrait at the 1985 Red Square holiday celebrations, and encouraged frank and open discussions at Politburo meetings.[160] To the West, Gorbachev was seen as a more moderate and less threatening Soviet leader; some Western commentators however believed this an act to lull Western governments into a false sense of security.[161] His wife was his closest adviser, and took on the unofficial role of a \"first lady\" by appearing with him on foreign trips; her public visibility was a breach of standard practice and generated resentment.[162] His other close aides were Georgy Shakhnazarov and Anatoly Chernyaev.[163]\n Gorbachev was aware that the Politburo could remove him from office, and that he could not pursue more radical reform without a majority of supporters in the Politburo.[164] He sought to remove several older members from the Politburo, encouraging Grigory Romanov, Nikolai Tikhonov, and Viktor Grishin into retirement.[165] He promoted Gromyko to head of state, a largely ceremonial role with little influence, and moved his own ally, Eduard Shevardnadze, to Gromyko's former post in charge of foreign policy.[166] Other allies whom he saw promoted were Yakovlev, Anatoly Lukyanov, and Vadim Medvedev.[167] Another of those promoted by Gorbachev was Boris Yeltsin, who was made a Secretary of the Central Committee (26th term) in July 1985.[168] Most of these appointees were from a new generation of well-educated officials who had been frustrated during the Brezhnev era.[169] In his first year, 14 of the 23 heads of department in the Secretariat were replaced.[170] Doing so, Gorbachev secured dominance in the Politburo within a year, faster than either Stalin, Khrushchev, or Brezhnev had achieved.[171]\n Gorbachev recurrently employed the term perestroika, first used publicly in March 1984.[172] He saw perestroika as encompassing a complex series of reforms to restructure society and the economy.[173] He was concerned by the country's low productivity, poor work ethic, and inferior quality goods;[174] like several economists, he feared this would lead to the country becoming a second-rate power.[175] The first stage of Gorbachev's perestroika was uskoreniye (\"acceleration\"), a term he used regularly in the first two years of his leadership.[176] The Soviet Union was behind the United States in many areas of production,[177] but Gorbachev claimed that it would accelerate industrial output to match that of the US by 2000.[178] The Five Year Plan of 1985\u20131990 was targeted to expand machine building by 50 to 100%.[179] To boost agricultural productivity, he merged five ministries and a state committee into a single entity, Agroprom, although by late 1986 he acknowledged this merger as a failure.[180]\n The purpose of reform was to prop up the centrally planned economy\u2014not to transition to market socialism. Speaking in late summer 1985 to the secretaries for economic affairs of the central committees of the East European communist parties, Gorbachev said: \"Many of you see the solution to your problems in resorting to market mechanisms in place of direct planning. Some of you look at the market as a lifesaver for your economies. But, comrades, you should not think about lifesavers but about the ship, and the ship is socialism.\"[181] Gorbachev's perestroika also[182] entailed attempts to move away from technocratic management of the economy by increasingly involving the labor force in industrial production.[183] He was of the view that once freed from the strong control of central planners, state-owned enterprises would act as market agents.[184] Gorbachev and other Soviet leaders did not anticipate opposition to the perestroika reforms; according to their interpretation of Marxism, they believed that in a socialist society like the Soviet Union there would not be \"antagonistic contradictions\".[185] However, there would come to be a public perception in the country that many bureaucrats were paying lip service to the reforms while trying to undermine them.[186] He also initiated the concept of gospriyomka (state acceptance of production) during his time as leader,[187] which represented quality control.[188] In April 1986, he introduced an agrarian reform which linked salaries to output and allowed collective farms to sell 30% of their produce directly to shops or co-operatives rather than giving it all to the state for distribution.[189] In a September 1986 speech, he embraced the idea of reintroducing market economics to the country alongside limited private enterprise, citing Lenin's New Economic Policy as a precedent; he nevertheless stressed that he did not regard this as a return to capitalism.[189]\n In the second year of his leadership, Gorbachev began speaking of glasnost, or \"openness\".[190] According to Doder and Branson, this meant \"greater openness and candour in government affairs and for an interplay of different and sometimes conflicting views in political debates, in the press, and in Soviet culture\".[191] Encouraging reformers into prominent media positions, he brought in Sergei Zalygin as head of Novy Mir magazine and Yegor Yakovlev as editor-in-chief of Moscow News.[192] He made the historian Yury Afanasyev dean of the State Historical Archive Faculty, from where Afansiev could press for the opening of secret archives and the reassessment of Soviet history.[169] Prominent dissidents like Andrei Sakharov were freed from internal exile or prison.[193] Gorbachev saw glasnost as a necessary measure to ensure perestroika by alerting the Soviet populace to the nature of the country's problems in the hope that they would support his efforts to fix them.[194] Particularly popular among the Soviet intelligentsia, who became key Gorbachev supporters,[195] glasnost boosted his domestic popularity but alarmed many Communist Party hardliners.[196] For many Soviet citizens, this newfound level of freedom of speech and press\u2014and its accompanying revelations about the country's past\u2014was uncomfortable.[197]\n Some in the party thought Gorbachev was not going far enough in his reforms; a prominent liberal critic was Yeltsin. He had risen rapidly since 1985, attaining the role of party secretary in Moscow.[198] Like many members of the government, Gorbachev was skeptical of Yeltsin, believing that he engaged in too much self-promotion.[199] Yeltsin was also critical of Gorbachev, regarding him as patronizing.[198] In early 1986, Yeltsin began sniping at Gorbachev in Politburo meetings.[199] At the Twenty-Seventh Party Congress in February, Yeltsin called for more far-reaching reforms than Gorbachev was initiating and criticized the party leadership, although he did not cite Gorbachev by name, claiming that a new cult of personality was forming. Gorbachev then opened the floor to responses, after which attendees publicly criticized Yeltsin for several hours.[200] After this, Gorbachev also criticized Yeltsin, claiming that he cared only for himself and was \"politically illiterate\".[201] Yeltsin then resigned both as Moscow party secretary and as a member of the Politburo.[201] From this point, tensions between the two men developed into a mutual hatred.[202]\n In April 1986 the Chernobyl disaster occurred.[203] In the immediate aftermath, officials fed Gorbachev incorrect information to downplay the incident. As the scale of the disaster became apparent, 336,000 people were evacuated from the area around Chernobyl.[204] Taubman noted that the disaster marked \"a turning point for Gorbachev and the Soviet regime\".[205] Several days after it occurred, he gave a televised report to the nation.[206] He cited the disaster as evidence for what he regarded as widespread problems in Soviet society, such as shoddy workmanship and workplace inertia.[207] Gorbachev later described the incident as one which made him appreciate the scale of incompetence and cover-ups in the Soviet Union.[205] From April to the end of the year, Gorbachev became increasingly open in his criticism of the Soviet system, including food production, state bureaucracy, the military draft, and the large size of the prison population.[208]\n In a May 1985 speech given to the Soviet Foreign Ministry\u2014the first time a Soviet leader had directly addressed his country's diplomats\u2014Gorbachev spoke of a \"radical restructuring\" of foreign policy.[209] A major issue facing his leadership was Soviet involvement in the Afghan Civil War, which had then been going on for over five years.[210] Over the course of the war, the Soviet Army took heavy casualties and there was much opposition to Soviet involvement among both the public and military.[210] On becoming leader, Gorbachev saw withdrawal from the war as a key priority.[211] In October 1985, he met with Afghan Marxist leader Babrak Karmal, urging him to acknowledge the lack of widespread public support for his government and pursue a power sharing agreement with the opposition.[211] That month, the Politburo approved Gorbachev's decision to withdraw combat troops from Afghanistan, although the last troops did not leave until February 1989.[212]\n Gorbachev had inherited a renewed period of high tension in the Cold War.[213] He believed strongly in the need to sharply improve relations with the United States; he was appalled at the prospect of nuclear war, was aware that the Soviet Union was unlikely to win the arms race and thought that the continued focus on high military spending was detrimental to his desire for domestic reform.[213] US president Ronald Reagan publicly appeared to not want a de-escalation of tensions, having scrapped d\u00e9tente and arms controls, initiating a military build-up, and calling the Soviet Union the \"evil empire\".[214]\n Both Gorbachev and Reagan wanted a summit to discuss the Cold War, but each faced some opposition to such a move within their respective governments.[215] They agreed to hold a summit in Geneva, Switzerland, in November 1985.[216] In the buildup to this, Gorbachev sought to improve relations with the US's NATO allies, visiting France in October 1985 to meet with President Fran\u00e7ois Mitterrand.[217]\n In January 1986, Gorbachev publicly proposed a three-stage programme for abolishing the world's nuclear weapons by the end of the 20th century.[219] An agreement was then reached to meet with Reagan in Reykjav\u00edk, Iceland, in October 1986. Gorbachev wanted to secure guarantees that SDI would not be implemented, and in return was willing to offer concessions, including a 50% reduction in Soviet long range nuclear missiles.[220] Both leaders agreed with the shared goal of abolishing nuclear weapons, but Gorbachev ultimately thought that too out of reach and instead proposed a mutual elimination of all medium-range nuclear missiles. Reagan refused to terminate the SDI program and no deal was reached.[221] After the summit, many of Reagan's allies criticized him for going along with the idea of abolishing nuclear weapons.[222] Gorbachev meanwhile told the Politburo that Reagan was \"extraordinarily primitive, troglodyte, and intellectually feeble\".[222]\n In his relations with the developing world, Gorbachev found many of its leaders professing revolutionary socialist credentials or a pro-Soviet attitude\u2014such as Libya's Muammar Gaddafi and Syria's Hafez al-Assad\u2014frustrating, and his best personal relationship was instead with India's prime minister, Rajiv Gandhi.[210] He thought that the \"socialist camp\" of Marxist\u2013Leninist governed states\u2014the Eastern Bloc countries, North Korea, Vietnam, and Cuba\u2014were a drain on the Soviet economy, receiving a far greater amount of goods from the Soviet Union than they collectively gave in return.[223] He sought improved relations with China, a country whose Marxist government had severed ties with the Soviets in the Sino-Soviet split and had since undergone its own structural reform. In June 1985 he signed a US$14\u00a0billion five-year trade agreement with the country and in July 1986, he proposed troop reductions along the Soviet-Chinese border, hailing China as \"a great socialist country\".[224] He made clear his desire for Soviet membership of the Asian Development Bank and for greater ties to Pacific countries, especially China and Japan.[225]\n In January 1987, Gorbachev attended a Central Committee plenum where he talked about perestroika and democratization while criticizing widespread corruption.[226] He considered putting a proposal to allow multi-party elections into his speech, but decided against doing so.[227] After the plenum, he focused his attentions on economic reform, holding discussions with government officials and economists.[228] Many economists proposed reducing ministerial controls on the economy and allowing state-owned enterprises to set their own targets; Ryzhkov and other government figures were skeptical.[229] In June, Gorbachev finished his report on economic reform. It reflected a compromise: ministers would retain the ability to set output targets but these would not be considered binding.[230] That month, a plenum accepted his recommendations and the Supreme Soviet passed a \"law on enterprises\" implementing the changes.[231] Economic problems remained: by the late 1980s there were still widespread shortages of basic goods, rising inflation, and declining living standards.[232] These stoked a number of miners' strikes in 1989.[233]\n By 1987, the ethos of glasnost had spread through Soviet society: journalists were writing increasingly openly,[234] many economic problems were being publicly revealed,[235] and studies appeared that critically reassessed Soviet history.[236] Gorbachev was broadly supportive, describing glasnost as \"the crucial, irreplaceable weapon of perestroika\".[234] He nevertheless insisted that people should use the newfound freedom responsibly, stating that journalists and writers should avoid \"sensationalism\" and be \"completely objective\" in their reporting.[237] Nearly two hundred previously restricted Soviet films were publicly released, and a range of Western films were also made available.[238] In 1989, Soviet responsibility for the 1940 Katyn massacre was finally revealed.[239]\n In September 1987, the government stopped jamming the signal of the British Broadcasting Corporation and Voice of America.[240] The reforms also included greater tolerance of religion;[241] an Easter service was broadcast on Soviet television for the first time and the millennium celebrations of the Russian Orthodox Church were given media attention.[242] Independent organizations appeared, most supportive of Gorbachev, although the largest, Pamyat, was ultra-nationalist and antisemitic in nature.[243] Gorbachev also announced that Soviet Jews wishing to migrate to Israel would be allowed to do so, something previously prohibited.[244]\n In August 1987, Gorbachev holidayed in Nizhnyaya Oreanda in Oreanda, Crimea, there writing Perestroika: New Thinking for Our Country and Our World[245] at the suggestion of US publishers.[246] For the 70th anniversary of the October Revolution of 1917\u2014which brought Lenin and the Communist Party to power\u2014Gorbachev produced a speech on \"October and Perestroika: The Revolution Continues\". Delivered to a ceremonial joint session of the Central Committee and the Supreme Soviet in the Kremlin Palace of Congresses, it praised Lenin but criticized Stalin for overseeing mass human rights abuses.[247] Party hardliners thought the speech went too far; liberalisers thought it did not go far enough.[248]\n In March 1988, the magazine Sovetskaya Rossiya published an open letter by the teacher Nina Andreyeva. It criticized elements of Gorbachev's reforms, attacking what she regarded as the denigration of the Stalinist era and arguing that a reformer clique\u2014whom she implied were mostly Jews and ethnic minorities\u2014were to blame.[249] Over 900 Soviet newspapers reprinted it and anti-reformists rallied around it; many reformers panicked, fearing a backlash against perestroika.[250] On returning from Yugoslavia, Gorbachev called a Politburo meeting to discuss the letter, at which he confronted those hardliners supporting its sentiment. Ultimately, the Politburo arrived at a unanimous decision to express disapproval of Andreyeva's letter and publish a rebuttal in Pravda.[251] Yakovlev and Gorbachev's rebuttal claimed that those who \"look everywhere for internal enemies\" were \"not patriots\" and presented Stalin's \"guilt for massive repressions and lawlessness\" as \"enormous and unforgiveable\".[252]\n Although the next party congress was not scheduled until 1991, Gorbachev convened the 19th Party Conference in its place in June 1988. He hoped that by allowing a broader range of people to attend than at previous conferences, he would gain additional support for his reforms.[253] With sympathetic officials and academics, Gorbachev drafted plans for reforms that would shift power away from the Politburo and towards the soviets. While the soviets had become largely powerless bodies that rubber-stamped Politburo policies, he wanted them to become year-round legislatures. He proposed the formation of a new institution, the Congress of People's Deputies, whose members were to be elected in a largely free vote.[254] This congress would in turn elect a USSR Supreme Soviet, which would do most of the legislating.[255]\n These proposals reflected Gorbachev's desire for more democracy; however, in his view there was a major impediment in that the Soviet people had developed a \"slave psychology\" after centuries of Tsarist autocracy and Marxist\u2013Leninist authoritarianism.[256] Held at the Kremlin Palace of Congresses, the conference brought together 5,000 delegates and featured arguments between hardliners and liberalisers. The proceedings were televised, and for the first time since the 1920s, voting was not unanimous.[257] In the months following the conference, Gorbachev focused on redesigning and streamlining the party apparatus; the Central Committee staff\u2014which then numbered around 3,000\u2014was halved, while various Central Committee departments were merged to cut down the overall number from twenty to nine.[258]\n In March and April 1989, elections to the new Congress were held.[259] Of the 2,250 legislators to be elected, one hundred\u2014termed the \"Red Hundred\" by the press\u2014were directly chosen by the Communist Party, with Gorbachev ensuring many were reformists.[260] Although over 85% of elected deputies were party members,[261] many of those elected\u2014including Sakharov and Yeltsin\u2014were liberalisers.[262] Gorbachev was happy with the result, describing it as \"an enormous political victory under extraordinarily difficult circumstances\".[263] The new Congress convened in May 1989.[264] Gorbachev was then elected its chair\u2014the new de facto head of state\u2014with 2,123 votes in favor to 87 against.[265] Its sessions were televised live,[265] and its members elected the new Supreme Soviet.[266] At the Congress, Sakharov spoke repeatedly, exasperating Gorbachev with his calls for greater liberalization and the introduction of private property.[267] When Sakharov died shortly after, Yeltsin became the figurehead of the liberal opposition.[268]\n Gorbachev tried to improve relations with the UK, France, and West Germany;[269] like previous Soviet leaders, he was interested in pulling Western Europe away from US influence.[270] Calling for greater pan-European co-operation, he publicly spoke of a \"Common European Home\" and of a Europe \"from the Atlantic to the Urals\".[271] In March 1987, Thatcher visited Gorbachev in Moscow; despite their ideological differences, they liked one another.[272] In April 1989 he visited London, lunching with Elizabeth\u00a0II.[273] In May 1987, Gorbachev again visited France, and in November 1988 Mitterrand visited him in Moscow.[274] The West German chancellor, Helmut Kohl, had initially offended Gorbachev by comparing him to Nazi propagandist Joseph Goebbels, although he later informally apologized and in October 1988 visited Moscow.[275] In June 1989 Gorbachev then visited Kohl in West Germany.[276] In November 1989 he also visited Italy, meeting with Pope John Paul II.[277] Gorbachev's relationships with these West European leaders were typically far warmer than those he had with their Eastern Bloc counterparts.[278]\n Gorbachev continued to pursue good relations with China to heal the Sino-Soviet Split. In May 1989 he visited Beijing and there met its leader Deng Xiaoping; Deng shared Gorbachev's belief in economic reform but rejected calls for democratization.[279] Pro-democracy students had massed in Tiananmen Square during Gorbachev's visit but after he left were massacred by troops. Gorbachev did not condemn the massacre publicly but it reinforced his commitment not to use violent force in dealing with pro-democracy protests in the Eastern Bloc.[280]\n Following the failures of earlier talks with the US, in February 1987, Gorbachev held a conference in Moscow, titled \"For a World without Nuclear Weapons, for Mankind's Survival\", which was attended by various international celebrities and politicians.[281] By publicly pushing for nuclear disarmament, Gorbachev sought to give the Soviet Union the moral high ground and weaken the West's self-perception of moral superiority.[282] Aware that Reagan would not budge on SDI, Gorbachev focused on reducing \"Intermediate-Range Nuclear Forces\", to which Reagan was receptive.[283] In April 1987, Gorbachev discussed the issue with US secretary of state George P. Shultz in Moscow; he agreed to eliminate the Soviets' SS-23 rockets and allow US inspectors to visit Soviet military facilities to ensure compliance.[284] There was hostility to such compromises from the Soviet military, but following the May 1987 Mathias Rust incident\u2014in which a West German teenager was able to fly undetected from Finland and land in Red Square\u2014Gorbachev fired many senior military figures for incompetence.[285] In December 1987, Gorbachev visited Washington, DC, where he and Reagan signed the Intermediate-Range Nuclear Forces Treaty.[286] Taubman called it \"one of the highest points of Gorbachev's career\".[287]\n A second US\u2013Soviet summit occurred in Moscow in May\u2013June 1988, which Gorbachev expected to be largely symbolic.[288] Again, he and Reagan criticized each other's countries\u2014Reagan raising Soviet restrictions on religious freedom; Gorbachev highlighting poverty and racial discrimination in the US, but Gorbachev related that they spoke \"on friendly terms\".[289] They reached an agreement on notifying each other before conducting ballistic missile tests and made agreements on transport, fishing, and radio navigation.[290] At the summit, Reagan told reporters that he no longer considered the Soviet Union an \"evil empire\" and the two revealed that they considered themselves friends.[291]\n The third summit was held in New York City in December.[292] Arriving there, Gorbachev gave a speech to the United Nations General Assembly where he announced a unilateral reduction in the Soviet armed forces by 500,000; he also announced that 50,000 troops would be withdrawn from Central and Eastern Europe.[293] He then met with Reagan and President-elect George H. W. Bush, following which he rushed home, skipping a planned visit to Cuba, to deal with the Armenian earthquake.[294] On becoming US president, Bush appeared interested in continuing talks with Gorbachev but wanted to appear tougher on the Soviets than Reagan, and had to allay criticism from the right wing of his Republican Party.[295] In December 1989, Gorbachev and Bush met at the Malta Summit.[296] Bush offered to assist the Soviet economy by suspending the Jackson\u2013Vanik amendment and repealing the Stevenson and Baird Amendments.[297] There, they agreed to a joint press conference, the first time that a US and Soviet leader had done so.[298] Gorbachev also urged Bush to normalize relations with Cuba and meet its president, Fidel Castro, although Bush refused to do so.[299]\n On taking power, Gorbachev found some unrest among different national groups within the Soviet Union. In December 1986, riots broke out in several Kazakh cities after a Russian was appointed head of the region.[300] In 1987, Crimean Tatars protested in Moscow to demand resettlement in Crimea, the area from which they had been deported on Stalin's orders in 1944. Gorbachev ordered a commission, headed by Gromyko, to examine their situation. Gromyko's report opposed calls for assisting Tatar resettlement in Crimea.[301] By 1988, the Soviet \"nationality question\" was increasingly pressing.[302] In February, the administration of the Nagorno-Karabakh Autonomous Oblast officially requested that it be transferred from the Azerbaijan Soviet Socialist Republic to the Armenian Soviet Socialist Republic; the majority of the region's population were ethnically Armenian and wanted unification with other majority Armenian areas.[303] As rival Armenian and Azerbaijani demonstrations took place in Nagorno-Karabakh, Gorbachev called an emergency meeting of the Politburo.[304] Gorbachev promised greater autonomy for Nagorno-Karabakh but refused the transfer, fearing that it would set off similar ethnic tensions and demands throughout the Soviet Union.[305] In the end however, greater autonomy was never given, and instead Gorbachev ordered the further violent ethnic cleansing of Armenians in parts of Nagorno-Karabakh and the adjacent Armenian-populated Shahumyan region, in what was named Operation Ring.[306]\n That month, in the Azerbaijani city of Sumgait, Azerbaijani gangs began killing members of the Armenian minority. Local troops tried to quell the unrest but were attacked by mobs.[307] The Politburo ordered additional troops into the city, but in contrast to those like Ligachev who wanted a massive display of force, Gorbachev urged restraint. He believed that the situation could be resolved through a political solution, urging talks between the Armenian and Azerbaijani Communist Parties.[308] Further anti-Armenian violence broke out in Baku in January 1990, followed by the Soviet Army killing about 150 Azeris.[309] Problems also emerged in the Georgian Soviet Socialist Republic; in April 1989, Soviet troops crushed Georgian pro-independence demonstrations in Tbilisi, resulting in various deaths.[310] Independence sentiment was also rising in the Baltic states; the Supreme Soviets of the Estonian, Lithuanian, and Latvian Soviet Socialist Republics declared their economic \"autonomy\" from the Soviet central government and introduced measures to restrict Russian immigration.[311] In August 1989, protesters formed the Baltic Way, a human chain across the three countries to symbolize their wish to restore independence.[312] That month, the Lithuanian Supreme Soviet ruled the 1940 Soviet annexation of their country to be illegal;[313] in January 1990, Gorbachev visited the republic to encourage it to remain part of the Soviet Union.[314]\n Gorbachev rejected the Brezhnev Doctrine, the idea that the Soviet Union had the right to intervene militarily in other Marxist\u2013Leninist countries if their governments were threatened.[315] In December 1987 he announced the withdrawal of 500,000 Soviet troops from Central and Eastern Europe.[316] While pursuing domestic reforms, he did not publicly support reformers elsewhere in the Eastern Bloc.[317] Hoping instead to lead by example, he later related that he did not want to interfere in their internal affairs, but he may have feared that pushing reform in Central and Eastern Europe would have angered his own hardliners too much.[318] Some Eastern Bloc leaders, like Hungary's J\u00e1nos K\u00e1d\u00e1r and Poland's Wojciech Jaruzelski, were sympathetic to reform; others, like Romania's Nicolae Ceau\u0219escu, were hostile to it.[319] In May 1987 Gorbachev visited Romania, where he was appalled by the state of the country, later telling the Politburo that there \"human dignity has absolutely no value\".[320] He and Ceau\u0219escu disliked each other, and argued over Gorbachev's reforms.[321]\n In August 1989, the Pan-European Picnic, which Otto von Habsburg planned as a test of Gorbachev, resulted in a large mass exodus of East German refugees. According to the \"Sinatra Doctrine\", the Soviet Union did not interfere and the media-informed Eastern European population realized that on the one hand their rulers were increasingly losing power and on the other hand the Iron Curtain was falling apart as a bracket for the Eastern Bloc.[322][323][324]\n In the Revolutions of 1989, most of the Marxist\u2013Leninist states of Central and Eastern Europe held multi-party elections resulting in regime change.[325] In most countries, like Poland and Hungary, this was achieved peacefully, but in Romania, the revolution turned violent, and led to Ceau\u0219escu's overthrow and execution.[325] Gorbachev was too preoccupied with domestic problems to pay much attention to these events.[326] He believed that democratic elections would not lead Eastern European countries into abandoning their commitment to socialism.[327] In 1989, he visited East Germany for the fortieth anniversary of its founding;[328] shortly after, in November, the East German government allowed its citizens to cross the Berlin Wall, a decision Gorbachev praised. Over the following years, much of the wall was demolished.[329] Neither Gorbachev nor Thatcher or Mitterrand wanted a swift reunification of Germany, aware that it would likely become the dominant European power. Gorbachev wanted a gradual process of German integration but Kohl began calling for rapid reunification.[330] With German reunification in 1990, many observers declared the Cold War over.[331]\n In February 1990, both liberalisers and Marxist\u2013Leninist hardliners intensified their attacks on Gorbachev.[332] A liberalizer march took place in Moscow criticizing Communist Party rule,[333] while at a Central Committee meeting, the hardliner Vladimir Brovikov accused Gorbachev of reducing the country to \"anarchy\" and \"ruin\" and of pursuing Western approval at the expense of the Soviet Union and the Marxist\u2013Leninist cause.[334] Gorbachev was aware that the Central Committee could still oust him as general secretary, and so decided to reformulate the role of head of government to a presidency from which he could not be removed.[335] He decided that the presidential election should be held by the Congress of People's Deputies. He chose this over a public vote because he thought the latter would escalate tensions and feared that he might lose it;[336] a spring 1990 poll nevertheless still showed him as the most popular politician in the country.[337]\n In March, the Congress of People's Deputies held the first (and only) Soviet presidential election, in which Gorbachev was the only candidate. He secured 1,329 in favor to 495 against; 313 votes were invalid or absent. He therefore became the first (and only) executive President of the Soviet Union.[338] A new 18-member Presidential Council de facto replaced the Politburo.[339] At the same Congress meeting, he presented the idea of repealing Article 6 of the Soviet constitution, which had ratified the Communist Party as the \"ruling party\" of the Soviet Union. The Congress passed the reform, undermining the de jure nature of the one-party state.[340]\n In the 1990 elections for the Russian Supreme Soviet, the Communist Party faced challengers from an alliance of liberalisers known as \"Democratic Russia\"; the latter did particularly well in urban centers.[341] Yeltsin was elected the parliament's chair, something Gorbachev was unhappy about.[342] That year, opinion polls showed Yeltsin overtaking Gorbachev as the most popular politician in the Soviet Union.[337] Gorbachev struggled to understand Yeltsin's growing popularity, commenting: \"he drinks like a fish\u00a0... he's inarticulate, he comes up with the devil knows what, he's like a worn-out record\".[343] The Russian Supreme Soviet was now out of Gorbachev's control;[343] in June 1990, it declared that in the Russian Republic, its laws took precedence over those of the Soviet central government.[344] Amid a growth in Russian nationalist sentiment, Gorbachev had reluctantly allowed the formation of a Communist Party of the Russian Soviet Federative Socialist Republic as a branch of the larger Soviet Communist Party. Gorbachev attended its first congress in June, but soon found it dominated by hardliners who opposed his reformist stance.[345]\n In January 1990, Gorbachev privately agreed to permit East German reunification with West Germany, but rejected the idea that a unified Germany could retain West Germany's NATO membership.[346] His compromise that Germany might retain both NATO and Warsaw Pact memberships did not attract support.[347] On 9 February 1990 in a phone conversation with James Baker, then the US secretary of state, he set out his position that \"a broadening of the NATO zone is not acceptable\" to which Baker agreed. Scholars are puzzled why Gorbachev never pursued a written pledge.[348] In May 1990, he visited the US for talks with President Bush;[349] there, he agreed that an independent Germany would have the right to choose its international alliances.[347] Ultimately he acquiesced to the reunification on the condition that NATO troops not be posted to the territory of Eastern Germany.[350] There remains some confusion over whether US secretary of state James Baker led Gorbachev to believe that NATO would not expand into other countries in Eastern Europe as well. There was no oral or written US promise that explicitly said so. Gorbachev himself has stated that he was only made such a promise regarding East Germany and that it was kept.[351][352] In July, Kohl visited Moscow and Gorbachev informed him that the Soviets would not oppose a reunified Germany being part of NATO.[353] Domestically, Gorbachev's critics accused him of betraying the national interest;[354] more broadly, they were angry that Gorbachev had allowed the Eastern Bloc to move away from direct Soviet influence.[355]\n In August 1990, Saddam Hussein's Iraqi government invaded Kuwait; Gorbachev endorsed President Bush's condemnation of it.[356] This brought criticism from many in the Soviet state apparatus, who saw Hussein as a key ally in the Persian Gulf and feared for the safety of the 9,000 Soviet citizens in Iraq, although Gorbachev argued that the Iraqis were the clear aggressors in the situation.[357] In November the Soviets endorsed a UN Resolution permitting force to be used in expelling the Iraqi Army from Kuwait.[358] Gorbachev later called it a \"watershed\" in world politics, \"the first time the superpowers acted together in a regional crisis\".[359] However, when the US announced plans for a ground invasion, Gorbachev opposed it, urging instead a peaceful solution.[360] In October 1990, Gorbachev was awarded the Nobel Peace Prize; he was flattered but acknowledged \"mixed feelings\" about the accolade.[361] Polls indicated that 90% of Soviet citizens disapproved of the award, which was widely seen as a Western and anti-Soviet accolade.[362]\n With the Soviet budget deficit climbing and no domestic money markets to provide the state with loans, Gorbachev looked elsewhere.[363] Throughout 1991, Gorbachev requested sizable loans from Western countries and Japan, hoping to keep the Soviet economy afloat and ensure the success of perestroika.[364] Although the Soviet Union had been excluded from the G7, Gorbachev secured an invitation to its London summit in July 1991.[365] There, he continued to call for financial assistance; Mitterrand and Kohl backed him,[366] while Thatcher\u2014no longer in office\u2014also urged Western leaders to agree.[367] Most G7 members were reluctant, instead offering technical assistance and proposing the Soviets receive \"special associate\" status\u2014rather than full membership\u2014of the World Bank and International Monetary Fund.[368] Gorbachev was frustrated that the US would spend $100\u00a0billion on the Gulf War but would not offer his country loans.[369] Other countries were more forthcoming; West Germany had given the Soviets DM60\u00a0billion by mid-1991.[370] Bush visited Moscow in late July, when he and Gorbachev concluded ten years of negotiations by signing the START I treaty, a bilateral agreement on the reduction and limitation of strategic offensive arms.[371]\n At the 28th Communist Party Congress in July 1990, hardliners criticized the reformists, but Gorbachev was re-elected party leader with the support of three-quarters of delegates, and his choice of deputy general secretary, Vladimir Ivashko, was also elected.[372] Seeking compromise with the liberalizers, Gorbachev assembled a team of both his own and Yeltsin's advisers to come up with an economic reform package: the result was the \"500 Days\" programme. This called for further decentralization and some privatization.[373] Gorbachev described the plan as \"modern socialism\" rather than a return to capitalism but had many doubts about it.[374] In September, Yeltsin presented the plan to the Russian Supreme Soviet, which backed it.[375] Many in the Communist Party and state apparatus warned against it, arguing that it would create marketplace chaos, rampant inflation, and unprecedented levels of unemployment.[376] The 500 Days plan was abandoned.[377] At this, Yeltsin railed against Gorbachev in an October speech, claiming that Russia would no longer accept a subordinate position to the Soviet government.[378]\n By mid-November 1990, much of the press was calling for Gorbachev to resign and predicting civil war.[379] Hardliners were urging Gorbachev to disband the presidential council and arrest vocal liberals in the media.[380] In November, he addressed the Supreme Soviet where he announced an eight-point program, which included governmental reforms, among them the abolition of the presidential council.[381] By this point, Gorbachev was isolated from many of his former close allies and aides.[382] Yakovlev had moved out of his inner circle and Shevardnadze had resigned.[383] His support among the intelligentsia was declining,[384] and by the end of 1990 his approval ratings had plummeted.[385]\n Amid growing dissent in the Baltics, especially Lithuania, in January 1991 Gorbachev demanded that the Lithuanian Supreme Council rescind its pro-independence reforms.[386] Soviet troops occupied several Vilnius buildings and attacked protesters,[387] 15 of whom were killed.[388] Gorbachev was widely blamed by liberalizers, with Yeltsin calling for his resignation.[389] Gorbachev denied sanctioning the military operation, although some in the military claimed that he had; the truth of the matter was never clearly established.[390] Fearing more civil disturbances, that month Gorbachev banned demonstrations and ordered troops to patrol Soviet cities alongside the police. This further alienated the liberalizers but was not enough to win over hardliners.[391] Wanting to preserve the Union, in April Gorbachev and the leaders of nine Soviet republics jointly pledged to prepare a treaty that would renew the federation under a new constitution; but six of the republics\u2014Estonia, Latvia, Lithuania, Moldova, Georgia, and Armenia\u2014did not endorse this.[392] A referendum on the issue brought 76.4% in favor of continued federation but the six rebellious republics had not taken part.[393] Negotiations took place to decide what form the new constitution would take, again bringing together Gorbachev and Yeltsin in discussion; it was planned to be formally signed in August.[394]\n In August, Gorbachev and his family holidayed at their dacha, \"Zarya\" ('Dawn') in Foros, Crimea.[395] Two weeks into his holiday, a group of senior Communist Party figures\u2014the \"Gang of Eight\"\u2014calling themselves the State Committee on the State of Emergency launched a coup d'\u00e9tat to seize control of the Soviet Union.[396] The phone lines to his dacha were cut and a group arrived, including Boldin, Shenin, Baklanov, and General Varennikov, informing him of the take-over.[397] The coup leaders demanded that Gorbachev formally declare a state of emergency in the country, but he refused.[398] Gorbachev and his family were kept under house arrest in their dacha.[399] The coup plotters publicly announced that Gorbachev was ill and thus Vice President Yanayev would take charge of the country.[400]\n Yeltsin, now President of the Russian Soviet Federative Socialist Republic, went inside the Moscow White House. Tens of thousands of protesters massed outside it to prevent troops storming the building to arrest him.[401] Outside of the White House, Yeltsin, atop a tank, gave a memorable speech condemning the coup.[402] Gorbachev feared that the coup plotters would order him killed, so had his guards barricade his dacha.[403] However, the coup's leaders realized that they lacked sufficient support and ended their efforts. On 21 August, Vladimir Kryuchkov, Dmitry Yazov, Oleg Baklanov, Anatoly Lukyanov, and Vladimir Ivashko arrived at Gorbachev's dacha to inform him that they were doing so.[403]\n That evening, Gorbachev returned to Moscow, where he thanked Yeltsin and the protesters for helping to undermine the coup.[404] At a subsequent press conference, he pledged to reform the Soviet Communist Party.[405] Two days later, he resigned as its general secretary and called on the Central Committee to dissolve.[406][407] Several members of the coup committed suicide; others were fired.[408] Gorbachev attended a session of the Russian Supreme Soviet on 23 August, where Yeltsin aggressively criticized him for having appointed and promoted many of the coup members to start with.[409]\n After the coup, the Supreme Soviet indefinitely suspended all Communist Party activity, effectively ending communist rule in the Soviet Union.[410][411]\n On 30 October, Gorbachev attended a conference in Madrid trying to revive the Israeli\u2013Palestinian peace process. The event was co-sponsored by the US and Soviet Union, one of the first examples of such cooperation between the two countries. There, he again met with Bush.[412] En route home, he traveled to France where he stayed with Mitterrand at the latter's home near Bayonne.[413]\n To keep unity within the country, Gorbachev continued to pursue plans for a new union treaty but found increasing opposition to the idea of a continued federal state as the leaders of various Soviet republics bowed to growing nationalist pressure.[414] Yeltsin stated that he would veto any idea of a unified state, instead favoring a confederation with little central authority.[415] Only the leaders of Kazakhstan and Kirghizia supported Gorbachev's approach.[416] The referendum in Ukraine on 1 December with a 90% turnout for secession from the Union was a fatal blow; Gorbachev had expected Ukrainians to reject independence.[417]\n Without Gorbachev's knowledge, Yeltsin met with Ukrainian president Leonid Kravchuk and Belarusian president Stanislav Shushkevich in Belovezha Forest, near Brest, Belarus, on 8 December and signed the Belavezha Accords, which declared the Soviet Union had ceased to exist and formed the Commonwealth of Independent States (CIS) as its successor.[418] Gorbachev only learned of this development when Shushkevich phoned him; Gorbachev was furious.[419] He desperately looked for an opportunity to preserve the Soviet Union, hoping in vain that the media and intelligentsia might rally against the idea of its dissolution.[420] Ukrainian, Belarusian, and Russian Supreme Soviets then ratified the establishment of the CIS.[421] On 9 December, Gorbachev issued a statement calling the CIS agreement \"illegal and dangerous\".[422][423] On 20 December, the leaders of 11 of the 12 remaining republics\u2014all except Georgia\u2014met in Kazakhstan and signed the Alma-Ata Protocol, agreeing to dismantle the Soviet Union and formally establish the CIS. They also provisionally accepted Gorbachev's resignation as president of what remained of the Soviet Union. Accepting the fait accompli of the Soviet Union's dissolution, Gorbachev revealed that he would resign as soon as he saw that the CIS was a reality.[424][425]\n Gorbachev reached a deal with Yeltsin that called for Gorbachev to formally announce his resignation as Soviet president and Commander-in-Chief on 25 December, before vacating the Kremlin by 29 December.[426] Yakovlev, Chernyaev and Shevardnadze joined Gorbachev to help him write a resignation speech.[424] Gorbachev then gave his speech in the Kremlin in front of television cameras, allowing for international broadcast.[427] In it, he announced, \"I hereby discontinue my activities at the post of President of the Union of Soviet Socialist Republics.\" He expressed regret for the breakup of the Soviet Union but cited what he saw as the achievements of his administration: political and religious freedom, the end of totalitarianism, the introduction of democracy and a market economy, and an end to the arms race and Cold War.[428] Gorbachev was the third out of eight Soviet leaders, after Malenkov and Khrushchev, not to die in office.[429][430] The following day, 26 December, the Soviet of the Republics, the upper house of the Supreme Soviet of the Soviet Union, formally voted the country out of existence.[431] As of 31 December 1991, all Soviet institutions that had not been taken over by Russia ceased to function.[432][433]\n Out of office, Gorbachev had more time to spend with his wife and family.[434] He and Raisa initially lived in their dilapidated dacha on Rublevskoe Shosse, and were also allowed to privatize their smaller apartment on Kosygin Street.[434] He focused on establishing his International Foundation for Socio-Economic and Political Studies, or \"Gorbachev Foundation\", launched in March 1992;[435] Yakovlev and Revenko were its first vice presidents.[436] Its initial tasks were in analyzing and publishing material on the history of perestroika, as well as defending the policy from what it called \"slander and falsifications\". The foundation also tasked itself with monitoring and critiquing life in post-Soviet Russia, presenting alternative development forms to those pursued by Yeltsin.[436]\n To finance his foundation, Gorbachev began lecturing internationally, charging large fees to do so.[436] On a visit to Japan, he was well received and given multiple honorary degrees.[437] In 1992, he toured the US in a Forbes private jet to raise money for his foundation. During the trip he met up with the Reagans for a social visit.[437] From there he went to Spain, where he attended the Expo '92 world fair in Seville and met with Prime Minister Felipe Gonz\u00e1lez, who had become a friend of his.[438] He further visited Israel and Germany, where he was received warmly by many politicians who praised his role in facilitating German reunification.[439] To supplement his lecture fees and book sales, Gorbachev appeared in commercials such as a television advertisement for Pizza Hut, another for the \u00d6BB[440] and photograph advertisements for Apple Computer[441] and Louis Vuitton, enabling him to keep the foundation afloat.[442][443] With his wife's assistance, Gorbachev worked on his memoirs, which were published in Russian in 1995 and in English the following year.[444] He also began writing a monthly syndicated column for The New York Times.[445]\n In 1993, Gorbachev launched Green Cross International, which focused on encouraging sustainable futures, and then the World Political Forum.[446] In 1995, he initiated the World Summit of Nobel Peace Laureates.[447]\n Gorbachev had promised to refrain from criticizing Yeltsin while the latter pursued democratic reforms, but soon the two men were publicly criticizing each other again.[448] After Yeltsin's decision to lift price caps generated massive inflation and plunged many Russians into poverty, Gorbachev openly criticized him, comparing the reform to Stalin's policy of forced collectivization.[448] After pro-Yeltsin parties did poorly in the 1993 legislative election, Gorbachev called on him to resign.[449] In 1995, his foundation held a conference on \"The Intelligentsia and Perestroika\". It was there that Gorbachev proposed to the Duma a law that would reduce many of the presidential powers established by Yeltsin's 1993 constitution.[450] Gorbachev continued to defend perestroika but acknowledged that he had made tactical errors as Soviet leader.[446] While he still believed that Russia was undergoing a process of democratization, he concluded that it would take decades rather than years, as he had previously thought.[451]\n In contrast to her husband's political activities, Raisa had focused on campaigning for children's charities.[452] In 1997, she founded a sub-division of the Gorbachev Foundation known as Raisa Maksimovna's Club to focus on improving women's welfare in Russia.[453] The Foundation had initially been housed in the former Social Science Institute building, but Yeltsin introduced limits to the number of rooms it could use there;[454] the American philanthropist Ted Turner then donated over $1\u00a0million to enable the foundation to build new premises on the Leningradsky Prospekt.[455] In 1999, Gorbachev made his first visit to Australia, where he gave a speech to the country's parliament.[456] Shortly after, in July, Raisa was diagnosed with leukemia. With the assistance of German chancellor Gerhard Schr\u00f6der, she was transferred to a cancer center in M\u00fcnster, Germany, and there underwent chemotherapy.[457] In September she fell into a coma and died.[211] After Raisa's passing, Gorbachev's daughter Irina and his two granddaughters moved into his Moscow home to live with him.[458] When questioned by journalists, he said that he would never remarry.[445]\n The Russian presidential elections were scheduled for June 1996, and although his wife and most of his friends urged him not to run, Gorbachev decided to do so.[459] He hated the idea that the election would result in a run-off between Yeltsin and Gennady Zyuganov, the Communist Party of the Russian Federation candidate whom Yeltsin saw as a Stalinist hardliner. He never expected to win outright but thought a centrist bloc could be formed around either himself or one of the other candidates with similar views, such as Grigory Yavlinsky, Svyatoslav Fyodorov, or Alexander Lebed.[460] After securing the necessary one million signatures of nomination, he announced his candidacy in March.[461] Launching his campaign, he traveled across Russia giving rallies in twenty cities.[461] He repeatedly faced anti-Gorbachev protesters, while some pro-Yeltsin local officials tried to hamper his campaign by banning local media from covering it or by refusing him access to venues.[462] In the election, Gorbachev came seventh with approximately 386,000 votes, or around 0.5% of the total.[463] Yeltsin and Zyuganov went through to the second round, where the former was victorious.[463]\n In December 1999, Yeltsin resigned and was succeeded by his deputy, Vladimir Putin, who then won the March 2000 presidential election.[464] Gorbachev attended Putin's inauguration ceremony in May, the first time he had entered the Kremlin since 1991.[465] Gorbachev initially welcomed Putin's rise, seeing him as an anti-Yeltsin figure.[446] Although he spoke out against some of the Putin government's actions, Gorbachev also had praise for the new government; in 2002, he said: \"I've been in the same skin. That's what allows me to say that what [Putin] has done is in the interest of the majority.\"[466] At the time, he believed Putin to be a committed democrat who nevertheless had to use \"a certain dose of authoritarianism\" to stabilize the economy and rebuild the state after the Yeltsin era.[465] At Putin's request, Gorbachev became co-chair of the \"Petersburg Dialogue\" project between high-ranking Russians and Germans.[464]\n In 2000, Gorbachev helped form the Russian United Social Democratic Party.[467] In June 2002, he participated in a meeting with Putin, who praised the venture, suggesting that a center-left party could be good for Russia and that he would be open to working with it.[466] In 2003, Gorbachev's party merged with the Social Democratic Party to form the Social Democratic Party of Russia[467]\u2014which, however, faced much internal division and failed to gain traction with voters.[467] Gorbachev resigned as party leader in May 2004 following a disagreement with the party's chairman over the direction taken in the 2003 election campaign. The party was later banned in 2007 by the Supreme Court of the Russian Federation due to its failure to establish local offices with at least 500 members in the majority of Russian regions, which is required by Russian law for a political organization to be listed as a party.[468] Later that year, Gorbachev founded a new movement, the Union of Social Democrats. Stating that it would not contest the forthcoming elections, Gorbachev declared: \"We are fighting for power, but only for power over people's minds\".[469]\n Gorbachev was critical of US hostility to Putin, arguing that the US government \"doesn't want Russia to rise\" again as a global power and wants \"to continue as the sole superpower in charge of the world\".[470] More broadly, Gorbachev was critical of US policy following the Cold War, arguing that the West had attempted to \"turn [Russia] into some kind of backwater\".[471] He rejected the idea\u2014expressed by Bush\u2014that the US had \"won\" the Cold War, arguing that both sides had cooperated to end the conflict.[471] He declared that since the fall of the Soviet Union, the US, rather than cooperating with Russia, had conspired to build a \"new empire headed by themselves\".[472] He was critical of how the US had expanded NATO right up to Russia's borders despite their initial assurances that they would not do so, citing this as evidence that the US government could not be trusted.[471][473] He spoke out against the 1999 NATO bombing of Yugoslavia because it lacked UN backing, as well as the 2003 invasion of Iraq led by the US.[471] In June 2004, Gorbachev nevertheless attended Reagan's state funeral,[474] and in 2007 visited New Orleans to see the damage caused by Hurricane Katrina.[475]\n Barred by the constitution from serving more than two consecutive terms as president, Putin stood down in 2008 and was succeeded by his chosen successor, Dmitry Medvedev, who reached out to Gorbachev in ways that Putin had not.[470] In September 2008, Gorbachev and business oligarch Alexander Lebedev announced they would form the Independent Democratic Party of Russia,[476] and in May 2009 Gorbachev announced that the launch was imminent.[477] After the outbreak of the Russo-Georgian War between Russia and South Ossetian separatists on one side and Georgia on the other, Gorbachev spoke out against US support for Georgian president Mikheil Saakashvili and for moving to bring the Caucasus into the sphere of its national interest.[478][479] Gorbachev nevertheless remained critical of Russia's government and criticized the 2011 parliamentary elections as being rigged in favor of the governing party, United Russia, and called for them to be re-held.[480] After protests broke out in Moscow over the election, Gorbachev praised the protesters.[480]\n In 2009, Gorbachev released Songs for Raisa, an album of Russian romantic ballads, sung by him and accompanied by musician Andrey Makarevich, to raise money for a charity devoted to his late wife.[481] That year, he also met with US president Barack Obama in efforts to \"reset\" strained US\u2013Russian relations,[482] and attended an event in Berlin commemorating the twentieth anniversary of the fall of the Berlin Wall.[483] In 2011, an eightieth birthday gala for him was held at London's Royal Albert Hall, featuring tributes from Shimon Peres, Lech Wa\u0142\u0119sa, Michel Rocard, and Arnold Schwarzenegger. The rock band Scorpions were invited and performed their song \"Wind of Change\", inspired by the fall of the Iron Curtain.[484] Proceeds from the event went to the Raisa Gorbachev Foundation.[485] That year, Medvedev awarded him the Order of St Andrew the Apostle the First-Called.[480]\n After Putin announced his intention to run for president in the 2012 election, Gorbachev was opposed to the idea.[486][487][488] He complained that Putin's new measures had \"tightened the screws\" on Russia and that the president was trying to \"completely subordinate society\", adding that United Russia now \"embodied the worst bureaucratic features of the Soviet Communist party\".[486]\n In 2015, Gorbachev ceased his frequent international traveling.[489] He continued to speak out on issues affecting Russia and the world. In 2014, he defended the Crimean status referendum and Russia's annexation of Crimea that began the Russo-Ukrainian War.[471] In his judgment, while Crimea was transferred from Russia to Ukraine in 1954, when both were part of the Soviet Union, the Crimean people had not been asked at the time, whereas in the 2014 referendum they had.[490] After sanctions were placed on Russia as a result of the annexation, Gorbachev spoke out against them.[491] His comments led to Ukraine banning him from entering the country for five years.[492]\n Russia can succeed only through democracy. Russia is ready for political competition, a real multiparty system, fair elections and regular rotation of government. This should define the role and responsibility of the president.\n At a November 2014 event marking 25 years since the fall of the Berlin Wall, Gorbachev warned that the ongoing war in Donbas had brought the world to the brink of a new Cold War, and he accused Western powers, particularly the US, of adopting an attitude of \"triumphalism\" towards Russia.[494][495] In December 2014, he said that both sides in the war in Donbas \"have been violating the terms of the ceasefire; both sides are guilty of using dangerous types of weapons and violating human rights\",[496] adding that Minsk agreements \"form the basis for the settlement\" of the conflict.[497] In 2016, he said that \"Politicians who think that problems and disputes can be solved by using military force\u2014even as a last resort\u2014should be rejected by society, they should clear the political stage.\"[498] In July 2016, Gorbachev criticized NATO for deploying more troops to Eastern Europe amid escalating tensions between the military alliance and Russia.[499] In June 2018, he welcomed the Russia\u2013United States summit in Helsinki between Putin and US president Donald Trump,[500] although in October criticized Trump's threat to withdraw from the 1987 Intermediate-Range Nuclear Forces Treaty, saying the move \"is not the work of a great mind\". He added: \"all agreements aimed at nuclear disarmament and the limitation of nuclear weapons must be preserved, for the sake of life on Earth\".[501]\n Following the death of former president George H. W. Bush in 2018, a critical partner and friend of his time in office, Gorbachev stated that the work they had both accomplished led directly to the end of the Cold War and the nuclear arms race, and that he \"deeply appreciated the attention, kindness and simplicity typical of George, Barbara and their large, friendly family\".[502]\n After the January 6 United States Capitol attack, Gorbachev declared, \"The storming of the capitol was clearly planned in advance, and it's obvious by whom.\" He did not clarify to whom he was referring. Gorbachev also stated that the attack \"called into question the future fate of the United States as a nation\".[503]\n In an interview with Russian news agency TASS on 20 January 2021, Gorbachev said that relations between the United States and Russia are of \"great concern\", and called on US president Joe Biden to begin talks with the Kremlin to make the two countries' \"intentions and actions clearer\" and \"in order to normalize relations\".[504] On 24 December 2021, Gorbachev said that the United States \"grew arrogant and self-confident\" after the collapse of the Soviet Union, resulting in \"a new empire. Hence the idea of NATO expansion\". He also endorsed the upcoming security talks between the United States and Russia, saying, \"I hope there will be a result.\"[505]\n Gorbachev made no personal comment publicly on the 2022 Russian invasion of Ukraine, although his Gorbachev Foundation stated on 26 February that \"[they] affirm the need for an early cessation of hostilities and immediate start of peace negotiations. There is nothing more precious in the world than human lives.\"[506] At the end of July 2022, Gorbachev's close friend, journalist Alexei Venediktov, said that Gorbachev was very upset when he found out that Putin had launched a full-scale invasion of Ukraine. According to Venediktov, Gorbachev believed that Putin \"destroyed his life's work\".[507] Gorbachev's interpreter, Pavel Palazhchenko, also stated that Gorbachev was psychologically traumatized by the invasion in the months preceding his death.[508][509]\n Even before he left office, Gorbachev had become a kind of social democrat\u2014believing in, as he later put it, equality of opportunity, publicly supported education and medical care, a guaranteed minimum of social welfare, and a \"socially oriented market economy\"\u2014all within a democratic political framework. Exactly when this transformation occurred is hard to say, but surely by 1989 or 1990 it had taken place.\n According to his university friend Zden\u011bk Mlyn\u00e1\u0159, in the early 1950s \"Gorbachev, like everyone else at the time, was a Stalinist\".[510] Mlyn\u00e1\u0159 noted, however, that unlike most other Soviet students, Gorbachev did not view Marxism simply as \"a collection of axioms to be committed to memory\".[511] Biographers Doder and Branson related that after Stalin's death, Gorbachev's \"ideology would never be doctrinal again\",[512] but noted that he remained \"a true believer\" in the Soviet system.[513] Doder and Branson noted that at the Twenty-Seventh Party Congress in 1986, Gorbachev was seen to be an orthodox Marxist\u2013Leninist;[514] that year, the biographer Zhores Medvedev stated that \"Gorbachev is neither a liberal nor a bold reformist\".[515]\n By the mid-1980s, when Gorbachev took power, many analysts were arguing that the Soviet Union was declining to the status of a Third World country.[516] In this context, Gorbachev argued that the Communist Party had to adapt and engage in creative thinking much as Lenin had creatively interpreted and adapted the writings of Karl Marx and Friedrich Engels to the situation of early 20th century Russia.[517] For instance, he thought that rhetoric about global revolution and overthrowing the bourgeoisie\u2014which had been integral to Leninist politics\u2014had become too dangerous in an era where nuclear warfare could obliterate humanity.[518] He began to move away from the Marxist\u2013Leninist belief in class struggle as the engine of political change, instead viewing politics as a way of coordinating the interests of all classes.[519] However, as Gooding noted, the changes that Gorbachev proposed were \"expressed wholly within the terms of Marxist-Leninist ideology\".[520]\n According to Doder and Branson, Gorbachev also wanted to \"dismantle the hierarchical military society at home and abandon the grand-style, costly, imperialism abroad\".[521] However, Jonathan Steele argued that Gorbachev failed to appreciate why the Baltic nations wanted independence and \"at heart he was, and remains, a Russian imperialist\".[522] Gooding thought that Gorbachev was \"committed to democracy\", something marking him out as different from his predecessors.[523] Gooding also suggested that when in power, Gorbachev came to see socialism not as a place on the path to communism, but a destination in itself.[524]\n Gorbachev's political outlook was shaped by the 23 years he served as a party official in Stavropol.[525] Doder and Branson thought that throughout most of his political career prior to becoming general secretary, \"his publicly expressed views almost certainly reflected a politician's understanding of what should be said, rather than his personal philosophy. Otherwise he could not have survived politically.\"[526] Like many Russians, Gorbachev sometimes thought of the Soviet Union as being largely synonymous with Russia and in various speeches described it as \"Russia\"; in one incident he had to correct himself after calling the USSR \"Russia\" while giving a speech in Kiev.[525]\n McCauley noted that perestroika was \"an elusive concept\", one which \"evolved and eventually meant something radically different over time\".[527] McCauley stated that the concept originally referred to \"radical reform of the economic and political system\" as part of Gorbachev's attempt to motivate the labor force and make management more effective.[528] It was only after initial measures to achieve this proved unsuccessful that Gorbachev began to consider market mechanisms and co-operatives, albeit with the state sector remaining dominant.[528] The political scientist John Gooding suggested that had the perestroika reforms succeeded, the Soviet Union would have \"exchanged totalitarian controls for milder authoritarian ones\" although not become \"democratic in the Western sense\".[523] With perestroika, Gorbachev had wanted to improve the existing Marxist\u2013Leninist system but ultimately ended up destroying it.[529] In this, he brought an end to state socialism in the Soviet Union and paved the way for a transition to liberal democracy.[530]\n Taubman nevertheless thought Gorbachev remained a socialist.[531] He described Gorbachev as \"a true believer\u2014not in the Soviet system as it functioned (or didn't) in 1985 but in its potential to live up to what he deemed its original ideals\".[531] He added that \"until the end, Gorbachev reiterated his belief in socialism, insisting that it wasn't worthy of the name unless it was truly democratic\".[532]\nAs Soviet leader, Gorbachev believed in incremental reform rather than a radical transformation;[533] he later referred to this as a \"revolution by evolutionary means\".[533] Doder and Branson noted that over the course of the 1980s, his thought underwent a \"radical evolution\".[534] Taubman noted that by 1989 or 1990, Gorbachev had transformed into a social democrat.[467] McCauley suggested that by at least June 1991 Gorbachev was a \"post-Leninist\", having \"liberated himself\" from Marxism\u2013Leninism.[535] After the fall of the Soviet Union, the newly formed Communist Party of the Russian Federation would have nothing to do with him.[536] However, in 2006, he expressed his continued belief in Lenin's ideas: \"I trusted him then and I still do\".[531] He claimed that \"the essence of Lenin\" was a desire to develop \"the living creative activity of the masses\".[531] Taubman believed that Gorbachev identified with Lenin on a psychological level.[537]\n By 1955, Gorbachev's hair was thinning,[539] and by the late 1960s he was bald,[540] revealing a distinctive port-wine stain on the top of his head.[541] Gorbachev reached an adult height of 5\u00a0foot 9\u00a0inches (1.75\u00a0m).[542] Throughout the 1960s, he struggled against obesity and dieted to control the problem;[87] Doder and Branson characterized him as \"stocky but not fat\".[542] He spoke in a southern Russian accent,[543] and was known to sing both folk and pop songs.[544]\n Throughout his life, he tried to dress fashionably.[545] Having an aversion to hard liquor,[546] he drank sparingly and did not smoke.[547] He was protective of his private life and avoided inviting people to his home.[115] Gorbachev cherished his wife,[548] who in turn was protective of him.[106] He was an involved parent and grandparent.[549] He sent his daughter, his only child, to a local school in Stavropol rather than to a school set aside for the children of party elites.[550] Unlike many of his contemporaries in the Soviet administration, he was not a womanizer and was known for treating women respectfully.[82]\n Gorbachev was baptized Russian Orthodox and when he was growing up, his grandparents had been practicing Christians.[551] In 2008, there was some press speculation that he was a practicing Christian after he visited the tomb of St Francis of Assisi, to which he publicly clarified that he was an atheist.[552] Since studying at university, Gorbachev considered himself an intellectual;[35] Doder and Branson thought that \"his intellectualism was slightly self-conscious\",[553] noting that unlike most Russian intelligentsia, Gorbachev was not closely connected \"to the world of science, culture, the arts, or education\".[554] When living in Stavropol, he and his wife collected hundreds of books.[555] Among his favorite authors were Arthur Miller, Dostoevsky, and Chinghiz Aitmatov, while he also enjoyed reading detective fiction.[556] He enjoyed going for walks,[557] having a love of natural environments,[558] and was also a fan of association football.[559] He favored small gatherings where the assembled discussed topics like art and philosophy rather than the large, alcohol-fueled parties common among Soviet officials.[560]\n Gorbachev's university friend, Mlyn\u00e1\u0159, described him as \"loyal and personally honest\".[561] He was self-confident,[562] polite,[547] and tactful;[547] he had a happy and optimistic temperament.[563] He used self-deprecating humor,[564] and sometimes profanities,[564] and often referred to himself in the third person.[565] He was a skilled manager,[82] and had a good memory.[566] A hard worker or workaholic,[567] as general secretary, he would rise at 7:00 or 8:00 in the morning and not go to bed until 1:00 or 2:00.[568] He commuted from the western suburbs between 9 and 10 in the morning and returned home around 8 in the evening.[569] Taubman called him \"a remarkably decent man\";[548] he thought Gorbachev to have \"high moral standards\".[570]\n Zhores Medvedev thought he was a talented orator, in 1986 stating that \"Gorbachev is probably the best speaker there has been in the top Party echelons\" since Leon Trotsky.[571] Medvedev also considered Gorbachev \"a charismatic leader\", something Brezhnev, Andropov, and Chernenko had not been.[572] Doder and Branson called him \"a charmer capable of intellectually seducing doubters, always trying to co-opt them, or at least blunt the edge of their criticism\".[573] McCauley thought Gorbachev displayed \"great tactical skill\" in maneuvering successfully between hardline Marxist\u2013Leninists and liberalisers for most of his time as leader, adding, though, that he was \"much more skilled at tactical, short-term policy than strategic, long-term thinking\", in part because he was \"given to making policy on the hoof\".[574]\n Doder and Branson thought Gorbachev \"a Russian to the core, intensely patriotic as only people living in the border regions can be\".[525] Taubman also noted that the former Soviet leader has a \"sense of self-importance and self-righteousness\" as well as a \"need for attention and admiration\" which grated on some of his colleagues.[570] He was sensitive to personal criticism and easily took offense.[575] Colleagues were often frustrated that he would leave tasks unfinished,[576] and sometimes also felt underappreciated and discarded by him.[577] Biographers Doder and Branson thought that Gorbachev was \"a puritan\" with \"a proclivity for order in his personal life\".[578] Taubman noted that he was \"capable of blowing up for calculated effect\".[579] He also thought that by 1990, when his domestic popularity was waning, Gorbachev had become \"psychologically dependent on being lionized abroad\", a trait for which he was criticized in the Soviet Union.[580] McCauley was of the view that \"one of his weaknesses was an inability to foresee the consequences of his actions\".[581]\n Gorbachev died at the Central Clinical Hospital in Moscow on 30 August 2022,[582] at the age of 91.[583] He died after a \"severe and prolonged illness,\" according to the hospital.[584][585][586]\n For a number of years before his death, Gorbachev suffered from severe diabetes and underwent several surgeries and hospital stays.[587] In April 2011, Gorbachev underwent complex spinal surgery in Germany, at the Munich clinic Sch\u00f6n Klinik M\u00fcnchen Harlaching.[588] On 11 June 2013, it was reported that Gorbachev was hospitalized for a routine examination. Two months earlier, he had not come to the funeral of Margaret Thatcher for health reasons.[587] On 22 October 2013, it became known that Gorbachev was undergoing another examination in a German clinic.[589] He was also hospitalized in the Central Clinical Hospital on 9 October 2014.[590] Also in 2014, Gorbachev underwent oral surgery.[480] Gorbachev was briefly hospitalized in May 2015 as well.[591] In November 2016, Gorbachev had a pacemaker installed at the Moscow Central Clinical Hospital.[592] Also in 2016, he underwent surgery to replace his lenses due to cataracts.[593]\n The length of his hospital visits increased in 2019, with Gorbachev hospitalized in December with pneumonia.[594][595] At the beginning of 2020, Gorbachev was placed under the continuous supervision of doctors.[585][586] Gorbachev's condition deteriorated even further in July 2022 as he developed kidney problems, which led to him being transferred for hemodialysis.[596] Shortly before his death, Gorbachev underwent four more operations, lost 40 kilograms of weight, and could no longer walk.[597] In interviews given shortly before his death, Gorbachev had complained about health and appetite problems.[598] Gorbachev was receiving palliative care, but was allowed to leave the hospital for short periods of time. On 29 August 2022, Gorbachev arrived at the Central Clinical Hospital for another hemodialysis, where he died on 30 August at approximately 10:00\u00a0p.m. Moscow time.[599][600]\n Russian president Vladimir Putin bid an official farewell to Gorbachev on 1 September 2022 during a visit to the Central Clinical Hospital, where he laid flowers at his coffin.[601][602] His press secretary Dmitry Peskov said that the \"tight schedule of the president\" would not allow him to be present at the funeral, as he was scheduled to visit Kaliningrad.[601][603]\n A funeral for Gorbachev was held on 3 September 2022 from 10 a.m. to 12 noon in the Column Hall of the House of Unions. The ceremony included an honor guard, but was not an official state funeral.[604] The service included rites administered by a Russian Orthodox priest.[605][606]\n Gorbachev was buried at Moscow's Novodevichy Cemetery, in the same grave as his wife Raisa, as requested by his will.[411]\n Russian president Vladimir Putin expressed his condolences on the death of Gorbachev,[607] and paid tribute to him at the Moscow hospital where the ex-president had died but, according to spokesman Dmitry Peskov, had no time to attend his funeral due to a busy work schedule. Putin also sent a telegram to Gorbachev's family, calling him \"a politician and statesman who had a huge impact on the course of world history\".[608] Russian prime minister Mikhail Mishustin called Gorbachev an \"outstanding statesman\".[609] Other reactions were less positive, with the leader of Russia's Communist Party, Gennady Zyuganov, stating that Gorbachev was a leader whose rule brought \"absolute sadness, misfortune and problems\" for \"all the peoples of our country\".[610] Naina Yeltsina, widow of former Russian president Boris Yeltsin, said that Gorbachev \"sincerely wanted to change the Soviet system\" and transform the USSR into a \"free and peaceful state\".[611]\n European Commission president Ursula von der Leyen paid tribute to him on Twitter, as did the UK's prime minister Boris Johnson, former US secretary of state Condoleezza Rice and Ireland's Taoiseach Miche\u00e1l Martin.[612]\n United Nations secretary-general Ant\u00f3nio Guterres said Gorbachev was a \"one-of-a-kind statesman who changed the course of history and a towering global leader, committed multilateralist, and tireless advocate for peace\", as former US secretary of state James Baker III stated that \"history will remember Mikhail Gorbachev as a giant who steered his great nation towards democracy\" in the context of the Cold War's conclusion. Queen Elizabeth\u00a0II, in her condolence and in one of her last public messages,[h] stated that \"through his courage and vision, he gained the admiration, affection and respect of the British people\".[614] Canadian prime minister Justin Trudeau said \"He helped bring an end to the Cold War, embraced reforms in the Soviet Union, and reduced the threat of nuclear weapons. He leaves behind an important legacy\",[615] while former Canadian prime minister Brian Mulroney said that \"he was a very pleasant man to deal with\" and \"history will remember him as a transformational leader\".[616] French president Emmanuel Macron called Gorbachev \"a man of peace whose choices opened up a path of liberty for Russians\". US president Joe Biden called Gorbachev \"a man of remarkable vision\".[617] Polish foreign minister Zbigniew Rau stated that Gorbachev had \"increased the scope of freedom of the enslaved peoples of the Soviet Union in an unprecedented way, giving them hope for a more dignified life\".[618] Lithuanian foreign minister Gabrielius Landsbergis said that Lithuanians would not glorify Gorbachev or forget about the 1991 January Events.[387][182][i]\n The 14th Dalai Lama wrote to the Gorbachev Foundation to express his \"condolences to his daughter, Irina Virganskaya and members of his family, his friends and supporters\".[620] Japan's prime minister Fumio Kishida said Gorbachev had \"left behind great [accomplishments] as a world leader supporting the abolishment of nuclear weapons\".[621] Germany's former chancellor Angela Merkel, who grew up in East Germany, said he completely changed her life and the world while current German chancellor Olaf Scholz hailed Gorbachev's role in reuniting Germany.[622]\n Opinions on Gorbachev are deeply divided.[565] According to a 2017 survey carried out by the independent institute Levada Center, 46% of Russian citizens have a negative opinion towards Gorbachev, 30% are indifferent, while only 15% have a positive opinion.[623] Many, particularly in Western countries, see him as the greatest statesman of the second half of the 20th century.[624] US press referred to the presence of \"Gorbymania\" in Western countries during the late 1980s and early 1990s, as represented by large crowds that turned out to greet his visits,[625] with Time naming him its \"Man of the Decade\" in the 1980s.[626] In the Soviet Union itself, opinion polls indicated that Gorbachev was the most popular politician from 1985 through to late 1989.[627] For his domestic supporters, Gorbachev was seen as a reformer trying to modernise the Soviet Union,[628] and to build a form of democratic socialism.[629] Taubman characterized Gorbachev as \"a visionary who changed his country and the world\u2014though neither as much as he wished\".[630] Taubman regarded Gorbachev as being \"exceptional\u00a0... as a Russian ruler and a world statesman\", highlighting that he avoided the \"traditional, authoritarian, anti-Western norm\" of both predecessors like Brezhnev and successors like Putin.[631] McCauley thought that in allowing the Soviet Union to move away from Marxism\u2013Leninism, Gorbachev gave the Soviet people \"something precious, the right to think and manage their lives for themselves\", with all the uncertainty and risk that that entailed.[632]\n Gorbachev succeeded in destroying what was left of totalitarianism in the Soviet Union; he brought freedom of speech, of assembly, and of conscience to people who had never known it, except perhaps for a few chaotic months in 1917. By introducing free elections and creating parliamentary institutions, he laid the groundwork for democracy. It is more the fault of the raw material he worked with than of his own real shortcomings and mistakes that Russian democracy will take much longer to build than he thought.\n Gorbachev's negotiations with the US helped bring an end to the Cold War and reduced the threat of nuclear conflict.[630] His decision to allow the Eastern Bloc to break apart prevented significant bloodshed in Central and Eastern Europe; as Taubman noted, this meant that the \"Soviet Empire\" ended in a far more peaceful manner than the British Empire several decades before.[630] Similarly, under Gorbachev, the Soviet Union broke apart without falling into civil war, as happened during the breakup of Yugoslavia at the same time.[633] McCauley noted that in facilitating the merger of East and West Germany, Gorbachev was \"a co-father of German unification\", assuring him long-term popularity among the German people.[634] However, he remains a controversial figure in former Soviet-occupied and administered countries such as the Baltic States, Ukraine, Georgia, Kazakhstan, Azerbaijan and Poland, after violent repressions against the local populations who sought independence. Locals have stated that they consider western veneration of the man an injustice and have said they do not understand his positive legacy in the west, with a group of Lithuanians having pursued legal action against him.[635]\n He also faced domestic criticism during his rule. During his career, Gorbachev attracted the admiration of some colleagues, but others came to hate him.[570] Across society more broadly, his inability to reverse the decline in the Soviet economy brought discontent.[636] Liberals thought he lacked the radicalism to really break from Marxism\u2013Leninism and establish a free market liberal democracy.[637] Conversely, many of his Communist Party critics thought his reforms were reckless and threatened the survival of Soviet socialism;[638] some believed he should have followed the example of China's Communist Party and restricted himself to economic rather than governmental reforms.[639] Many Russians saw his emphasis on persuasion rather than force as a sign of weakness.[532]\n For much of the Communist Party nomenklatura, the Soviet Union's dissolution was disastrous as it resulted in their loss of power.[640] In Russia, he is widely despised for his role in the collapse of the Soviet Union and the ensuing economic collapse in the 1990s.[565] General Varennikov, one of those who orchestrated the 1991 coup attempt against Gorbachev, for instance called him \"a renegade and traitor to your own people\".[450] Many of his critics attacked him for allowing the Marxist\u2013Leninist governments across Eastern Europe to fall,[641] and for allowing a reunited Germany to join NATO, something they deem to be contrary to Russia's national interest.[642]\n The historian Mark Galeotti stressed the connection between Gorbachev and his predecessor, Andropov. In Galeotti's view, Andropov was \"the godfather of the Gorbachev revolution\", because\u2014as a former head of the KGB\u2014he was able to put forward the case for reform without having his loyalty to the Soviet cause questioned, an approach that Gorbachev was able to build on and follow through with.[643] According to McCauley, Gorbachev \"set reforms in motion without understanding where they could lead. Never in his worst nightmare could he have imagined that perestroika would lead to the destruction of the Soviet Union\".[644]\n According to The New York Times, \"Few leaders in the 20th century, indeed in any century, have had such a profound effect on their time. In little more than six tumultuous years, Mr. Gorbachev lifted the Iron Curtain, decisively altering the political climate of the world.\"[645]\n In 1988, India awarded Gorbachev the Indira Gandhi Prize for Peace, Disarmament and Development;[646] in 1990, he was given the Nobel Peace Prize for \"his leading role in the peace process which today characterizes important parts of the international community\".[647] Out of office he continued to receive honors. In 1992, he was the first recipient of the Ronald Reagan Freedom Award,[648] and in 1994 was given the Grawemeyer Award by the University of Louisville, Kentucky.[649] In 1995, he was awarded the Grand-Cross of the Order of Liberty by Portuguese president M\u00e1rio Soares,[650] and in 1998 the Freedom Award from the National Civil Rights Museum in Memphis, Tennessee.[651] In 2000, he was presented with the Golden Plate Award of the American Academy of Achievement at an awards ceremony at Hampton Court Palace near London.[652] In 2002, Gorbachev received the Freedom of the City of Dublin from Dublin City Council.[653]\n In 2002, Gorbachev was awarded the Charles V Prize by the European Academy of Yuste Foundation.[654] Gorbachev, together with Bill Clinton and Sophia Loren, were awarded the 2004 Grammy Award for Best Spoken Word Album for Children for their recording of Sergei Prokofiev's 1936 Peter and the Wolf for Pentatone.[655] In 2005, Gorbachev was awarded the Point Alpha Prize for his role in supporting German reunification.[656]\n In 2020/2021, the Theatre of Nations in Moscow, in collaboration with Latvian director Alvis Hermanis, staged a production called Gorbachev.[657] Yevgeny Mironov and Chulpan Khamatova played the roles of Gorbachev and his wife Raisa respectively.[658]\n Gorbachev was portrayed by David Dencik in the 2019 miniseries Chernobyl,[659] by Matthew Marsh in the 2023 film Tetris[660] and by Aleksander Krupa in the 2024 biographical drama Reagan.[661]\n Gorbachev appears in Call of Duty: Black Ops Cold War during a mission where the playable character infiltrates the KGB headquarter. Gorbachev is portrayed by David Agranov.[662]\n"
    },
    {
        "title": "Ramadan Revolution",
        "url": "https://en.wikipedia.org/wiki/Ramadan_Revolution",
        "content": "Coup successful\n The Ramadan Revolution, also referred to as the 8 February Revolution and the February 1963 coup d'\u00e9tat in Iraq, was a military coup by the Iraqi branch of the Ba'ath Party which overthrew the prime minister of Iraq, Abdul-Karim Qasim in 1963. It took place between 8 and 10 February 1963. Qasim's former deputy, Abdul Salam Arif, who was not a Ba'athist, was given the largely ceremonial title of president, while prominent Ba'athist general Ahmed Hassan al-Bakr was named prime minister. The most powerful leader of the new government was the secretary general of the Iraqi Ba'ath Party, Ali Salih al-Sa'di, who controlled the National Guard militia and organized a massacre of hundreds\u2014if not thousands\u2014of suspected communists and other dissidents following the coup.[7]\n The government lasted approximately nine months, until Arif disarmed the National Guard in the November 1963 Iraqi coup d'\u00e9tat, which was followed by a purge of Ba'ath Party members.\n Some time after the Homeland Officers' Organization, or \"Al-Ahrar\" (\"The Free\") succeeded in toppling the monarchy and transforming the Iraqi government into a republic in 1958, signs of differences between political parties and forces and the Homeland Officers' Organization began when Pan-Arab nationalist forces led by Abdul Salam Arif and the Ba'ath Party called for immediate unification with the United Arab Republic (UAR). In an attempt to create a state of political equilibrium, the Iraqi Communist Party (ICP), which opposed unity, tried to discount cooperation with the UAR in economics, culture, and science rather than political and military agreements.\n Gradually, Abd al-Karim Qasim's relations with some of his fellow members of Al-Ahrar worsened, and his relationship with the unionist and nationalist currents, which had played an active role in supporting the 1958 movement, became strained. As for conflicting currents in the ICP, they were aspiring for a coalition with General Qasim and had long been extending their relationship with him. Qasim thought that some of his allies in the Communist party were coming close to leapfrogging the proposition, especially after the increasing influence of the Communist party in the use of the slogan, proclaimed by many Communists and government supporters during marches: \"Long live leader Abd al-Karim and the Communist Party in governing great demand!\"[8] Qasim began to minimize the Communist movement. He ordered the party to be disarmed and most of the party leaders to be arrested. However, the party retained Air Commander Jalal al-Awqati and Lt. Col. Fadhil Abbas Mahdawi, Qasim's cousin.\n Qasim's removal took place on 8 February 1963, the fourteenth day of Ramadan, and so the coup was called the 14 Ramadan Coup. It had been in its planning stages since 1962, and several attempts had been planned, only to be abandoned for fear of discovery.[9] The coup had been initially planned for January 18, but was moved to 25 January and then 8 February after Qasim gained knowledge of the proposed attempt and arrested some of the plotters.\n The coup began in the early morning of 8 February 1963, when the communist air force chief, Jalal al-Awqati, was assassinated, and tank units occupied the Abu Ghraib radio station. A bitter two-day struggle unfolded with heavy fighting between the Ba'athist conspirators and pro-Qasim forces. Qasim took refuge in the Ministry of Defence, where fighting became particularly heavy. Communist sympathisers took to the streets to resist the coup, which added to the high casualties: \"An estimated eighty Ba'thists and between 300 and 5,000 communist sympathizers were killed in the two days of fighting to control Baghdad's streets,\" as recounted by Ariel Ira Ahram.[5]\n On 9 February, Qasim eventually offered his surrender in return for safe passage out of the country. His request was refused, and in the afternoon, he was executed on the orders of the newly formed National Council of the Revolutionary Command (NCRC).[10] Qasim was given a mock trial over Baghdad radio and then killed. Many of his Shi'ite supporters believed that he had merely gone into hiding and would appear like the Mahdi to lead a rebellion against the new government. To counter that sentiment and to terrorize his supporters, Qasim's dead body was displayed on television in a five-minute propaganda video, The End of the Criminals, which included close-up views of his bullet wounds amid disrespectful treatment of his corpse, which is spat on in the final scene.[11][12]\n Qasim's former deputy, Abdul Salam Arif, who was not a Ba'athist, was given the largely ceremonial title of president, and the prominent Ba'athist general Ahmed Hassan al-Bakr was named prime minister. However, the secretary general of the Ba'ath Party, Ali Salih al-Sa'di, used his control of the National Guard militia, commanded by Mundhir al-Wanadawi, to establish himself as the de facto new leader of Iraq and had more authority in reality than al-Bakr or Arif. The nine-month rule of al-Sa'di and his civilian branch of the Ba'ath Party has been described as \"a reign of terror\" as the National Guard, under orders from the Revolutionary Command Council (RCC) \"to annihilate anyone who disturbs the peace,\" detained, tortured, or executed thousands of suspected Qasim loyalists. Furthermore, the National Guard, which developed from a core group of perhaps 5,000 civilian Ba'athist partisans but increased to 34,000 members by August 1963, who were identified by their green armbands, was poorly disciplined, as militiamen engaged in extensive infighting and created a widespread perception of chaos and disorder.[5][7]\n While it's still early, the Iraqi revolution seems to have succeeded. It is almost certainly a net gain for our side.\u00a0... We will make informal friendly noises as soon as we can find out whom to talk with, and ought to recognize as soon as we're sure these guys are firmly in the saddle. CIA had excellent reports on the plotting, but I doubt either they or UK should claim much credit for it.\n It has long been suspected that the Ba'ath Party collaborated with the CIA in planning and carrying out the coup.[14] Pertinent contemporary documents relating to the CIA's operations in Iraq have remained classified[15][16] and as of 2021, \"[s]cholars are only beginning to uncover the extent to which the United States was involved in organizing the coup,\"[17] but are \"divided in their interpretations of American foreign policy.\"[18][19][20] Bryan R. Gibson, writes that although \"[i]t is accepted among scholars that the CIA ... assisted the Ba\u2019th Party in its overthrow of [Qasim's] regime,\" that \"barring the release of new information, the preponderance of evidence substantiates the conclusion that the CIA was not behind the February 1963 Ba'thist coup.\"[21] Peter Hahn argues that \"[d]eclassified U.S. government documents offer no evidence to support\" suggestions of direct U.S. involvement.[22] On the other hand, Brandon Wolfe-Hunnicutt writes that \"CIA involvement in the 1963 coup ... has been an open secret for decades,\" citing \"compelling evidence of an American role,\" and publicly declassified documents that \"largely substantiate the plausibility\" of the CIA's involvement.[3][18][23] Eric Jacobsen, citing the testimony of contemporary prominent Ba'athists and U.S. government officials, states that \"[t]here is ample evidence that the CIA not only had contacts with the Iraqi Ba'th in the early sixties, but also assisted in the planning of the coup.\"[24] Nathan J. Citino writes that \"Washington backed the movement by military officers linked to the pan-Arab Ba\u2018th Party that overthrew Qasim,\" but that \"the extent of U.S. responsibility cannot be fully established on the basis of available documents,\" and that \"[a]lthough the United States did not initiate the 14 Ramadan coup, at best it condoned and at worst it contributed to the violence that followed.\"[25]\n \nBa'athist leaders maintained supportive relationships with U.S. officials before, during, and after the coup.[2][26] A March 1964 State Department memorandum stated that U.S. \"officers assiduously cultivated\" a \"Baathi student organization, which triggered the revolution of February 8, 1963 by sponsoring a successful student strike at the University of Baghdad.\"[27] According to Wolfe-Hunnicutt, declassified documents suggest that the Kennedy administration viewed two prominent Ba'athist officials\u2014Ba'ath Party Army Bureau head, Lt. Col. Salih Mahdi Ammash, whose arrest on February 4 served as the coup's catalyst, and Hazim Jawad, \"responsible for [the Ba'ath Party's] clandestine printing and propaganda distribution operations\"\u2014as \"assets.\"[26] Ammash was described as \"Western-oriented, anti-British, and anti-Communist,\" and known to be \"friendly to the service attaches of the US Embassy in Baghdad,\" while future U.S. ambassador to Iraq, Robert C. Strong, would refer to Jawad as \"one of our boys.\"[26][28] Jamal al-Atassi\u2014a cabinet member of the Ba'athist regime that took power in Syria that same year\u2014would tell Malik Mufti that the Iraqi Ba'athists, in conversations with their Syrian counterparts, argued \"that their cooperation with the CIA and the US to overthrow Abd al-Karim Qasim and take over power\" was comparable \"to how Lenin arrived in a German train to carry out his revolution, saying they had arrived in an American train.\"[29] Similarly, then secretary general of the Iraqi Ba'ath Party, Ali Salih al-Sa'di, is quoted as saying that the Iraqi Ba'athists \"came to power on a CIA train.\"[24] Former U.S. ambassador to Saudi Arabia, James E. Akins, who worked in the Baghdad Embassy's political section from 1961 to 1964, would state that he personally witnessed contacts between Ba'ath Party members and CIA officials,[24] and that: The [1963 Ba'athist] revolution was of course supported by the U.S. in money and equipment as well. I don't think the equipment was terribly important, but the money was to the Ba'ath Party leaders who took over the revolution. It wasn't talked about openly\u2014that we were behind it\u2014but an awful lot of people knew.[30][31] Conversely, according to Gibson, the CIA official working to instigate a military coup against Qasim, and who later became the head of the CIA's operations in Iraq and Syria, has \"denied any involvement in the Ba'ath Party's actions,\" stating instead that the CIA's efforts against Qasim were still in the planning stages at the time: \"I was still engaged in contacting people who could play a role in a coup attempt against [him].\"[32]\n U.S. officials were undoubtedly pleased with the coup's outcome, ultimately approving a $55 million arms deal with Iraq and urging America's Arab allies to oppose a Soviet-sponsored diplomatic offensive accusing Iraq of genocide against its Kurdish minority at the United Nations (UN) General Assembly.[33] In its ascension to power, the Ba'athists \"methodically hunted down Communists\" thanks to \"mimeographed lists [...] complete with home addresses and auto license plate numbers.\"[26][34] While it is unlikely that the Ba'athists would've needed assistance in identifying Iraqi communists,[35][36] it is widely believed that the CIA provided the National Guard with lists of communists and other leftists, who were then arrested or killed under al-Wanadawi's and al-Sa'di's direction.[37] This claim first originated in a September 27, 1963 Al-Ahram interview with King Hussein of Jordan, who declared:\n You tell me that American Intelligence was behind the 1957 events in Jordan. Permit me to tell you that I know for a certainty that what happened in Iraq on 8 February had the support of American Intelligence. Some of those who now rule in Baghdad do not know of this thing but I am aware of the truth. Numerous meetings were held between the Ba'ath party and American Intelligence, the more important in Kuwait. Do you know that\u00a0... on 8 February a secret radio beamed to Iraq was supplying the men who pulled the coup with the names and addresses of the Communists there so that they could be arrested and executed?\u00a0... Yet I am the one accused of being an agent of America and imperialism![29][35][38] Similarly, Qasim's former foreign minister, Hashim Jawad, would state that \"the Iraqi Foreign Ministry had information of complicity between the Ba'ath and the CIA. In many cases the CIA supplied the Ba'ath with the names of individual communists, some of whom were taken from their homes and murdered.\"[29] Gibson emphasizes that the Ba'athists compiled their own lists, citing Bureau of Intelligence and Research reports stating that \"[Communist] party members [are being] rounded up on the basis of lists prepared by the now-dominant Ba'th Party\" and that the ICP had \"exposed virtually all its assets\" whom the Ba'athists had \"carefully spotted and listed.\"[6] On the other hand, Wolfe-Hunnicutt, citing contemporary U.S. counterinsurgency doctrine, notes that assertions of CIA involvement in the Ba'athist purge campaign \"would be consistent with American special warfare doctrine\" regarding U.S. covert support to anti-communist \"Hunter-Killer\" teams \"seeking the violent overthrow of a communist dominated and supported government,\"[39] and \"speaks to a larger pattern in American foreign policy,\" drawing parallels to other instances where the CIA compiled lists of suspected communists targeted for execution, such as Guatemala in 1954 and Indonesia in 1965-66.[40] Also, Citino and Wolfe-Hunnicutt note that two officials in the U.S. embassy in Baghdad\u2014William Lakeland and James E. Akins\u2014\"used coverage of the July 1962 Moscow Conference for Disarmament and Peace in Iraq's leftist press to compile lists of Iraqi communists and their supporters\u00a0... Those listed included merchants, students, members of professional societies, and journalists, although university professors constituted the largest single group.\"[26][41] Wolfe-Hunnicutt comments that \"it\u2019s not unreasonable to suspect [such a] list \u2013 or ones like it \u2013 would have been shared with the Ba\u2018ath.\"[36] Lakeland, a former SCI participant, \"personally maintained contact following the coup with a National Guard interrogator,\" and may have been influenced by his prior interaction with then-Major Hasan Mustafa al-Naqib, the Iraqi military attach\u00e9 in the U.S. who defected to the Ba'ath Party after Qasim \"upheld Mahdawi's death sentences\" against nationalists involved in the 1959 Mosul uprising.[42] Furthermore, \"Weldon C. Mathews has meticulously established that National Guard leaders who participated in human rights abuses had been trained in the United States as part of a police program run by the International Cooperation Administration and Agency for International Development.\"[43]\n The attacks on the people's freedoms carried out by the\u00a0... bloodthirsty members of the National Guard, their violation of things sacred, their disregard of the law, the injuries they have done to the state and the people, and finally their armed rebellion on November 13, 1963, has led to an intolerable situation which is fraught with grave dangers to the future of this people which is an integral part of the Arab nation. We have endured all we could.\u00a0... The army has answered the call of the people to rid them from this terror.\n  The U.S. provided $120,000 in \"police assistance\" to Iraq during 1963\u20131965, considerably less than the $832,000 in assistance that it provided to Iran during those years.[45]\n The Kennedy administration officially advocated a diplomatic settlement to the First Iraqi\u2013Kurdish War, but its provision of military aid to the Ba'athist government emboldened Iraqi hardliners to resume hostilities against Kurdish rebels on June 10, after which Iraq requested additional emergency U.S. assistance including napalm weapons. President Kennedy approved the arms sale in part on the recommendation of senior adviser Robert Komer and the weapons were provided, but an offer by Iraqi general Hasan Sabri al-Bayati to reciprocate this gesture by sending a Soviet T-54 tank in Iraq's possession to the U.S. embassy in Baghdad for inspection became something of a \"scandal\" as Bayati's offer had not been approved by al-Bakr, Foreign Minister Talib El-Shibib, or other senior Iraqi officials. Ultimately, the Ba'ath Party leadership reneged on that part of the agreement, fearing that handing over the tank to the U.S. would irrevocably harm Iraq's reputation. Shibib subsequently recounted that the incident damaged Iraq's relations with both the U.S. and the Soviet Union: \"On the one side Iraq would lose the Soviets as a source of intelligence. On the other the United States would see us as a bunch of kid swindlers.\"[46]\n Throughout 1963, the Soviet Union actively worked to undermine the Ba'athist government, supporting Kurdish rebels under the leadership of Mustafa Barzani with propaganda and a \"small monthly stipend for Barzani,\" suspending military shipments to Iraq in May, convincing its ally Mongolia to make charges of genocide against Iraq at the UN General Assembly from July to September, and sponsoring a failed communist coup attempt on July 3.[47]\n The same year, the party's military committee in Syria succeeded in persuading Nasserist and independent officers to make common cause with it and successfully carried out a military coup on 8 March. A National Revolutionary Command Council took control, assigned itself legislative power, and appointed Salah al-Din al-Bitar as head of a \"national front\" government. The Ba'ath participated in the government, along with the Arab Nationalist Movement, the United Arab Front, and the Socialist Unity Movement.\n As Hanna Batatu notes, that took place without the fundamental disagreement over immediate or \"considered\" reunification having been resolved. The Ba'ath moved to consolidate its power within the new government by purging Nasserist officers in April. Subsequent disturbances led to the fall of the al-Bitar government, and in the aftermath of Jasim Alwan\u2019s failed Nasserist coup in July, the Ba'ath monopolized power.\n The attacks on the people's freedoms carried out by the\u00a0... bloodthirsty members of the National Guard, their violation of things sacred, their disregard of the law, the injuries they have done to the state and the people, and finally their armed rebellion on November 13, 1963, has led to an intolerable situation which is fraught with grave dangers to the future of this people which is an integral part of the Arab nation. We have endured all we could.\u00a0... The army has answered the call of the people to rid them from this terror.\n The Ba'athist government collapsed in November 1963 over the question of unification with Syria and the extremist and uncontrollable behavior of al-Sa'di's National Guard. President Arif, with the overwhelming support of the Iraqi military, purged Ba'athists from the government and ordered the National Guard to stand down and disarm. Although al-Bakr had conspired with Arif to remove al-Sa'di, on 5 January 1964, Arif removed al-Bakr from his new position as vice president for fear of allowing the Ba'ath Party to retain a foothold inside his government.[5][48]\n After the November coup, mounting evidence of Ba'athist atrocities emerged, which Lakeland predicted \"will have a more or less permanent effect on the political developments in the country\u2014particularly on the prospects of a Ba'athi revival.\"[49] Marion Farouk-Sluglett and Peter Sluglett describe the Ba'athists as having cultivated a \"profoundly unsavory image\" by \"acts of wanton brutality\" on a scale without prior precedent in Iraq, including \"some of the most terrible scenes of violence hitherto experienced in the postwar Middle East.\" \"As almost every family in Baghdad was affected\u2014and both men and women were equally maltreated\u2014the Ba'athists' activities aroused a degree of intense loathing for them that has persisted to this day among many Iraqis of that generation.\" More broadly, the Slugletts state, \"Qasim's failings, serious as they were, can scarcely be discussed in the same terms as the venality, savagery and wanton brutality characteristic of the regimes which followed his own.\"[50] Batatu recounts: \n In the cellars of al-Nihayyah Palace, which the [National Guard's] Bureau [of Special Investigation] used as its headquarters, were found all sorts of loathsome instruments of torture, including electric wires with pincers, pointed iron stakes on which prisoners were made to sit, and a machine which still bore traces of chopped-off fingers. Small heaps of blooded clothing were scattered about, and there were pools on the floor and stains over the walls.[44]\n"
    },
    {
        "title": "Freedom of speech",
        "url": "https://en.wikipedia.org/wiki/Freedom_of_speech",
        "content": "\n Freedom of speech is a principle that supports the freedom of an individual or a community to articulate their opinions and ideas without fear of retaliation, censorship, or legal sanction. The right to freedom of expression has been recognised as a human right in the Universal Declaration of Human Rights and international human rights law. Many countries have constitutional law that protects free speech. Terms like free speech, freedom of speech, and freedom of expression are used interchangeably in political discourse. However, in a legal sense, the freedom of expression includes any activity of seeking, receiving, and imparting information or ideas, regardless of the medium used.\n Article 19 of the UDHR states that \"everyone shall have the right to hold opinions without interference\" and \"everyone shall have the right to freedom of expression; this right shall include freedom to seek, receive, and impart information and ideas of all kinds, regardless of frontiers, either orally, in writing or print, in the form of art, or through any other media of his choice\". The version of Article 19 in the ICCPR later amends this by stating that the exercise of these rights carries \"special duties and responsibilities\" and may \"therefore be subject to certain restrictions\" when necessary \"[f]or respect of the rights or reputation of others\" or \"[f]or the protection of national security or public order (ordre public), or of public health or morals\".[2]\n Freedom of speech and expression, therefore, may not be recognized as being absolute, and common limitations or boundaries to freedom of speech relate to libel, slander, obscenity, pornography, sedition, incitement, fighting words, hate speech, classified information, copyright violation, trade secrets, food labeling, non-disclosure agreements, the right to privacy, dignity, the right to be forgotten, public security, blasphemy and perjury. Justifications for such include the harm principle, proposed by John Stuart Mill in On Liberty, which suggests that \"the only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others\".[3]\n The idea of the \"offense principle\" is also used to justify speech limitations, describing the restriction on forms of expression deemed offensive to society, considering factors such as extent, duration, motives of the speaker, and ease with which it could be avoided.[3] With the evolution of the digital age, application of freedom of speech becomes more controversial as new means of communication and restrictions arise, for example, the Golden Shield Project, an initiative by Chinese government's Ministry of Public Security that filters potentially unfavourable data from foreign countries. Facebook routinely and automatically eliminates what it perceives as hate speech, even if such words are used ironically or poetically with no intent to insult others.\n Freedom of speech and expression has a long history that predates modern international human rights instruments.[4] It is thought that the ancient Athenian democratic principle of free speech may have emerged in the late 6th or early 5th century BC.[5]\n Freedom of speech was vindicated by Erasmus and Milton.[4] Edward Coke claimed freedom of speech as \"an ancient custom of Parliament\" in the 1590s, and it was affirmed in the Protestation of 1621.[6] Restating what is written in the English Declaration of Right, 1689, England's Bill of Rights 1689 legally established the constitutional right of freedom of speech in Parliament, which is still in effect.[7][8] This so-called parliamentary privilege includes no possible defamation claims meaning Parliamentarians are free to speak up in the House without fear of legal action.[9] This protection extends to written proceedings: for example, written and oral questions, motions and amendments tabled to bills and motions.[9]\n \nOne of the world's first freedom of the press acts was introduced in Sweden in 1766 (Swedish Freedom of the Press Act), mainly due to the classical liberal member of parliament and Ostrobothnian priest Anders Chydenius.[10][11][12][13]  In a report published in 1776, he wrote:[14] No evidence should be needed that a certain freedom of writing and printing is one of the strongest bulwarks of a free organization of the state, as, without it, the estates would not have sufficient information for the drafting of good laws, and those dispensing justice would not be monitored, nor would the subjects know the requirements of the law, the limits of the rights of government, and their responsibilities. Education and ethical conduct would be crushed; coarseness in thought, speech, and manners would prevail, and dimness would darken the entire sky of our freedom in a few years. Under the leadership of Anders Chydenius, the Caps at the Swedish Riksdag in G\u00e4vle on December 2, 1766, passed the adoption of a freedom of the press regulation that stopped censorship and introduced the principle of public access to official records in Sweden. Excluded were defamation of the king's majesty and the Swedish Church.\n The Declaration of the Rights of Man and of the Citizen, adopted during the French Revolution in 1789, specifically affirmed freedom of speech as an inalienable right.[4] Adopted in 1791, freedom of speech is a feature of the First Amendment to the United States Constitution.[15] The French Declaration provides for freedom of expression in Article 11, which states that:\n The free communication of ideas and opinions is one of the most precious of the rights of man. Every citizen may, accordingly, speak, write, and print with freedom, but shall be responsible for such abuses of this freedom as shall be defined by law.[16] Article 19 of the Universal Declaration of Human Rights, adopted in 1948, states that:\n Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers.[17] Today, freedom of speech, or the freedom of expression, is recognised in international and regional human rights law. The right is enshrined in Article 19 of the International Covenant on Civil and Political Rights, Article 10 of the European Convention on Human Rights, Article 13 of the American Convention on Human Rights and Article 9 of the African Charter on Human and Peoples' Rights.[18] Based on John Milton's arguments, freedom of speech is understood as a multi-faceted right that includes not only the right to express, or disseminate, information and ideas but three further distinct aspects:\n International, regional and national standards also recognise that freedom of speech, as the freedom of expression, includes any medium, whether orally, in writing, in print, through the internet or art forms. This means that the protection of freedom of speech as a right includes the content and the means of expression.[18]\n The right to freedom of speech and expression is closely related to other rights. It may be limited when conflicting with other rights (see limitations on freedom of speech).[18] The right to freedom of expression is also related to the right to a fair trial and court proceeding which may limit access to the search for information, or determine the opportunity and means in which freedom of expression is manifested within court proceedings.[19] As a general principle freedom of expression may not limit the right to privacy, as well as the honor and reputation of others. However, greater latitude is given when criticism of public figures is involved.[19]\n The right to freedom of expression is particularly important for media, which play a special role as the bearer of the general right to freedom of expression for all.[18] However, freedom of the press does not necessarily enable freedom of speech. Judith Lichtenberg has outlined conditions in which freedom of the press may constrain freedom of speech. For example, if all the people who control the various mediums of publication suppress information or stifle the diversity of voices inherent in freedom of speech. This limitation was famously summarised as \"Freedom of the press is guaranteed only to those who own one\".[20] Lichtenberg argues that freedom of the press is simply a form of property right summed up by the principle \"no money, no voice\".[21]\n Freedom of speech is usually seen as a negative right.[22] This means that the government is legally obliged to take no action against the speaker based on the speaker's views, but that no one is obliged to help any speakers publish their views, and no one is required to listen to, agree with, or acknowledge the speaker or the speaker's views. These concepts correspond to earlier traditions of natural law and common law rights.[23]\n Freedom of speech is understood to be fundamental in a democracy. The norms on limiting freedom of expression mean that public debate may not be completely suppressed even in times of emergency.[19] One of the most notable proponents of the link between freedom of speech and democracy is Alexander Meiklejohn. He has argued that the concept of democracy is that of self-government by the people. For such a system to work, an informed electorate is necessary. In order to be appropriately knowledgeable, there must be no constraints on the free flow of information and ideas. According to Meiklejohn, democracy will not be true to its essential ideal if those in power can manipulate the electorate by withholding information and stifling criticism. Meiklejohn acknowledges that the desire to manipulate opinion can stem from the motive of seeking to benefit society. However, he argues, choosing manipulation negates, in its means, the democratic ideal.[24]\n Eric Barendt has called this defence of free speech on the grounds of democracy \"probably the most attractive and certainly the most fashionable free speech theory in modern Western democracies\".[25] Thomas I. Emerson expanded on this defence when he argued that freedom of speech helps to provide a balance between stability and change. Freedom of speech acts as a \"safety valve\" to let off steam when people might otherwise be bent on revolution. He argues that \"The principle of open discussion is a method of achieving a more adaptable and at the same time more stable community, of maintaining the precarious balance between healthy cleavage and necessary consensus\". Emerson furthermore maintains that \"Opposition serves a vital social function in offsetting or ameliorating (the) normal process of bureaucratic decay\".[26]\n Research undertaken by the Worldwide Governance Indicators project at the World Bank, indicates that freedom of speech, and the process of accountability that follows it, have a significant impact on the quality of governance of a country. \"Voice and Accountability\" within a country, defined as \"the extent to which a country's citizens are able to participate in selecting their government, as well as freedom of expression, freedom of association, and free media\" is one of the six dimensions of governance that the Worldwide Governance Indicators measure for more than 200 countries.[27] Against this backdrop it is important that development agencies create grounds for effective support for a free press in developing countries.[28]\n Richard Moon has developed the argument that the value of freedom of speech and freedom of expression lies with social interactions. Moon writes that \"by communicating an individual forms relationships and associations with others \u2013 family, friends, co-workers, church congregation, and countrymen. By entering into discussion with others an individual participates in the development of knowledge and in the direction of the community\".[29]\n The Human Rights Measurement Initiative[30] measures the right to opinion and expression for countries around the world, using a survey of in-country human rights experts.[31]\n Freedom of speech is not regarded as absolute by some, with most legal systems generally setting limits on the freedom of speech, particularly when freedom of speech conflicts with other rights and protections, such as in the cases of libel, slander, pornography, obscenity, fighting words, and intellectual property.\n Some limitations to freedom of speech may occur through legal sanction, and others may occur through social disapprobation.[33] In Saudi Arabia, journalists are forbidden to write with disrespect or disapproval of the royal family, religion, or the government. Journalists are also not given any legal protection for their writing in Saudi Arabia. Journalist Jamal Khashoggi was a critic of the Saudi Arabian government. He was killed in 2018 by Saudi Arabian officials for his writing.\n Some views are illegal to express because they are perceived by some to be harmful to others. This category often includes speech that is both false and potentially dangerous, such as falsely shouting \"Fire!\" in a theatre and causing a panic. Justifications for limitations to freedom of speech often reference the \"harm principle\" or the \"offence principle\".\n In On Liberty (1859), John Stuart Mill argued that \"...there ought to exist the fullest liberty of professing and discussing, as a matter of ethical conviction, any doctrine, however immoral it may be considered\".[33] Mill argues that the fullest liberty of expression is required to push arguments to their logical limits, rather than the limits of social embarrassment.[34][35][36][37]\n In 1985, Joel Feinberg introduced what is known as the \"offence principle\". Feinberg wrote, \"It is always a good reason in support of a proposed criminal prohibition that it would probably be an effective way of preventing serious offence (as opposed to injury or harm) to persons other than the actor, and that it is probably a necessary means to that end\".[38] Hence Feinberg argues that the harm principle sets the bar too high and that some forms of expression can be legitimately prohibited by law because they are very offensive. Nevertheless, as offending someone is less serious than harming someone, the penalties imposed should be higher for causing harm.[38] In contrast, Mill does not support legal penalties unless they are based on the harm principle.[33] Because the degree to which people may take offence varies, or may be the result of unjustified prejudice, Feinberg suggests that several factors need to be taken into account when applying the offence principle, including: the extent, duration and social value of the speech, the ease with which it can be avoided, the motives of the speaker, the number of people offended, the intensity of the offence, and the general interest of the community at large.[33]\n Jasper Doomen argued that harm should be defined from the point of view of the individual citizen, not limiting harm to physical harm since nonphysical harm may also be involved; Feinberg's distinction between harm and offence is criticized as largely trivial.[39]\n In 1999, Bernard Harcourt wrote of the collapse of the harm principle: \"Today the debate is characterized by a cacophony of competing harm arguments without any way to resolve them. There is no longer an argument within the structure of the debate to resolve the competing claims of harm. The original harm principle was never equipped to determine the relative importance of harms\".[40]\n Interpretations of both the harm and offense limitations to freedom of speech are culturally and politically relative. For instance, in Russia, the harm and offense principles have been used to justify the Russian LGBT propaganda law restricting speech (and action) concerning LGBT issues. Many European countries outlaw speech that might be interpreted as Holocaust denial. These include Austria, Belgium, Canada, the Czech Republic, France, Germany, Hungary, Israel, Liechtenstein, Lithuania, Luxembourg, the Netherlands, Poland, Portugal, Russia, Slovakia, Switzerland and Romania.[41] Armenian genocide denial is also illegal in some countries.\n Apostasy has been instrumentalized to restrict freedom of speech in some countries.[43] In some countries, blasphemy is a crime. For example, in Austria, defaming Muhammad, the prophet of Islam, is not protected as free speech.[44][45][46] In contrast, in France, blasphemy and disparagement of Muhammad are protected under free speech law.\n Certain public institutions may also enact policies restricting the freedom of speech, for example, speech codes at state-operated schools.\n \nIn the U.S., the standing landmark opinion on political speech is Brandenburg v. Ohio (1969),[47] expressly overruling Whitney v. California.[48] In Brandenburg, the U.S. Supreme Court referred to the right even to speak openly of violent action and revolution in broad terms: [Our] decisions have fashioned the principle that the constitutional guarantees of free speech and free press do not allow a State to forbid or proscribe advocacy of the use of force or law violation except where such advocacy is directed to inciting or producing imminent lawless action and is likely to incite or cause such action.[49]  The opinion in Brandenburg discarded the previous test of \"clear and present danger\" and made the right to freedom of (political) speech protections in the United States almost absolute.[50][51] Hate speech is also protected by the First Amendment in the United States, as decided in R.A.V. v. City of St. Paul, (1992) in which the Supreme Court ruled that hate speech is permissible, except in the case of imminent violence.[52] See the First Amendment to the United States Constitution for more detailed information on this decision and its historical background.\n Limitations based on time, place, and manner apply to all speech, regardless of the view expressed.[53] They are generally restrictions that are intended to balance other rights or a legitimate government interest. For example, a time, place, and manner restriction might prohibit a noisy political demonstration at a politician's home during the middle of the night, as that impinges upon the rights of the politician's neighbors to quiet enjoyment of their own homes. An otherwise identical activity might be permitted if it happened at a different time (e.g., during the day), at a different place (e.g., at a government building or in another public forum), or in a different manner (e.g., a silent protest). Funeral Protests are a complex issue in the United States. It is a right to Americans to be able to hold a peaceful protest against various policies they deem unreasonable. It is a question of whether or not it is appropriate through the time, place and manner outlook to protest funeral proceedings. Because of recent flare ups of this occurring, legislation has been put to action to limit this. Now, funeral protests are governed and prohibited by law on a state-to-state basis inside the United States.\n Jo Glanville, editor of the Index on Censorship, states that \"the Internet has been a revolution for censorship as much as for free speech\".[55] International, national and regional standards recognise that freedom of speech, as one form of freedom of expression, applies to any medium, including the Internet.[18] The Communications Decency Act (CDA) of 1996 was the first major attempt by the United States Congress to regulate pornographic material on the Internet. In 1997, in the landmark cyberlaw case of Reno v. ACLU, the US Supreme Court partially overturned the law.[56] Judge Stewart R. Dalzell, one of the three federal judges who in June 1996 declared parts of the CDA unconstitutional, in his opinion stated the following:[57]\n The Internet is a far more speech-enhancing medium than print, the village green, or the mails. Because it would necessarily affect the Internet itself, the CDA would necessarily reduce the speech available for adults on the medium. This is a constitutionally intolerable result. Some of the dialogue on the Internet surely tests the limits of conventional discourse. Speech on the Internet can be unfiltered, unpolished, and unconventional, even emotionally charged, sexually explicit, and vulgar \u2013 in a word, \"indecent\" in many communities. But we should expect such speech to occur in a medium in which citizens from all walks of life have a voice. We should also protect the autonomy that such a medium confers to ordinary people as well as media magnates.[...] My analysis does not deprive the Government of all means of protecting children from the dangers of Internet communication. The Government can continue to protect children from pornography on the Internet through vigorous enforcement of existing laws criminalising obscenity and child pornography. [...] As we learned at the hearing, there is also a compelling need for public educations about the benefits and dangers of this new medium, and the Government can fill that role as well. In my view, our action today should only mean that Government's permissible supervision of Internet contents stops at the traditional line of unprotected speech. [...] The absence of governmental regulation of Internet content has unquestionably produced a kind of chaos, but as one of the plaintiff's experts put it with such resonance at the hearing: \"What achieved success was the very chaos that the Internet is. The strength of the Internet is chaos.\" Just as the strength of the Internet is chaos, so that strength of our liberty depends upon the chaos and cacophony of the unfettered speech the First Amendment protects.[57] The World Summit on the Information Society (WSIS) Declaration of Principles adopted in 2003 makes specific reference to the importance of the right to freedom of expression for the \"Information Society\" in stating:\n We reaffirm, as an essential foundation of the Information society, and as outlined in Article 19 of the Universal Declaration of Human Rights, that everyone has the right to freedom of opinion and expression; that this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers. Communication is a fundamental social process, a basic human need and the foundation of all social organisation. It is central to the Information Society. Everyone, everywhere should have the opportunity to participate and no one should be excluded from the benefits of the Information Society offers.[58] According to Bernt Hugenholtz and Lucie Guibault, the public domain is under pressure from the \"commodification of information\" as information with previously little or no economic value has acquired independent economic value in the information age. This includes factual data, personal data, genetic information and pure ideas. The commodification of information is taking place through intellectual property law, contract law, as well as broadcasting and telecommunications law.[59]\n Freedom of information is an extension of freedom of speech where the medium of expression is the Internet. Freedom of information may also refer to the right to privacy in the context of the Internet and information technology. As with the right to freedom of expression, the right to privacy is a recognised human right and freedom of information acts as an extension to this right.[60] Freedom of information may also concern censorship in an information technology context, i.e., the ability to access Web content, without censorship or restrictions.[61]\n Freedom of information is also explicitly protected by acts such as the Freedom of Information and Protection of Privacy Act of Ontario, in Canada. The Access to Information Act gives Canadian citizens, permanent residents, and any person or corporation present in Canada a right to access records of government institutions that are subject to the Act.[62]\n The concept of freedom of information has emerged in response to state sponsored censorship, monitoring and surveillance of the internet. Internet censorship includes the control or suppression of the publishing or accessing of information on the Internet.[63] The Global Internet Freedom Consortium claims to remove blocks to the \"free flow of information\" for what they term \"closed societies\".[64] According to the Reporters without Borders (RWB) \"internet enemy list\" the following states engage in pervasive internet censorship: Mainland China, Cuba, Iran, Myanmar/Burma, North Korea, Saudi Arabia, Syria, Turkmenistan, Uzbekistan, and Vietnam.[65]\n A widely publicized example of internet censorship is the \"Great Firewall of China\" (in reference both to its role as a network firewall and the ancient Great Wall of China). The system blocks content by preventing IP addresses from being routed through and consists of standard firewall and proxy servers at the internet gateways. The system also selectively engages in DNS poisoning when particular sites are requested. The government does not appear to be systematically examining Internet content, as this appears to be technically impractical.[66] Internet censorship in the People's Republic of China is conducted under a wide variety of laws and administrative regulations, including more than sixty regulations directed at the Internet. Censorship systems are vigorously implemented by provincial branches of state-owned ISPs, business companies, and organizations.[67][68]\n Saudi Arabia's government had been intensifying the scrutiny of social media accounts, under which they were detaining several activists, critics and even normal social media users over few critical tweets. A law professor, Awad Al-Qarni became a victim of Saudi's internet censorship and was facing death sentence. Saudi-controlled media portrayed him as a dangerous preacher due to his Twitter and WhatsApp posts, but dissidents considered him as an important intellectual who maintained strong social media influence.[69]\n Some legal scholars (such as Tim Wu of Columbia University) have argued that the traditional issues of free speech\u2014that \"the main threat to free speech\" is the censorship of \"suppressive states\", and that \"ill-informed or malevolent speech\" can and should be overcome by \"more and better speech\" rather than censorship\u2014assumes scarcity of information. This scarcity prevailed during the 20th century, but with the arrival of the internet, information became plentiful, \"but the attention of listeners\" scarce. Furthermore, in the words of Wu, this \"cheap speech\" made possible by the internet \" ... may be used to attack, harass, and silence as much as it is used to illuminate or debate\".[70][71] The Electronic Frontier Foundation (EFF) has argued that \"censorship cannot be the only answer to disinformation online\" and that tech companies \"have a history of overcorrecting and censoring accurate, useful speech\u2014or, even worse, reinforcing misinformation with their policies.\"[72]\n According to Wu, in the 21st century, the danger is not \"suppressive states\" that target \"speakers directly\", but that:\n ...targets listeners or it undermines speakers indirectly. More precisely, emerging techniques of speech control depend on (1) a range of new punishments, like unleashing \"troll armies\" to abuse the press and other critics, and (2) \"flooding\" tactics (sometimes called \"reverse censorship\") that distort or drown out disfavored speech through the creation and dissemination of fake news, the payment of fake commentators, and the deployment of propaganda robots.[73] As journalist Peter Pomerantsev writes, these techniques employ \"information ... in weaponized terms, as a tool to confuse, blackmail, demoralize, subvert and paralyze.\"[70][74] Before the invention of the printing press, a written work, once created, could only be physically multiplied by highly laborious and error-prone manual copying. No elaborate system of censorship and control over scribes existed, who until the 14th century were restricted to religious institutions, and their works rarely caused wider controversy. In response to the printing press, and the theological heresies it allowed to spread, the Roman Catholic Church moved to impose censorship.[75] Printing allowed for multiple exact copies of a work, leading to a more rapid and widespread circulation of ideas and information (see print culture).[76] The origins of copyright law in most European countries lie in efforts by the Roman Catholic Church and governments to regulate and control the output of printers.[76]\n In 1501, Pope Alexander VI issued a Bill against the unlicensed printing of books. In 1559, Pope Paul IV promulgated the Index Expurgatorius, or List of Prohibited Books.[75] The Index Expurgatorius is the most famous and long-lasting example of \"bad books\" catalogues issued by the Roman Catholic Church, which presumed to be in authority over private thoughts and opinions, and suppressed views that went against its doctrines. The Index Expurgatorius was administered by the Roman Inquisition, but enforced by local government authorities, and went through 300 editions. Amongst others, it banned or censored books written by Ren\u00e9 Descartes, Giordano Bruno, Galileo Galilei, David Hume, John Locke, Daniel Defoe, Jean-Jacques Rousseau and Voltaire.[78] While governments and church encouraged printing in many ways because it allowed for the dissemination of Bibles and government information, works of dissent and criticism could also circulate rapidly. Consequently, governments established controls over printers across Europe, requiring them to have official licenses to trade and produce books.[76]\n The notion that the expression of dissent or subversive views should be tolerated, not censured or punished by law, developed alongside the rise of printing and the press. Areopagitica, published in 1644, was John Milton's response to the Parliament of England's re-introduction of government licensing of printers, hence publishers.[79] Church authorities had previously ensured that Milton's essay on the right to divorce was refused a license for publication. In Areopagitica, published without a license,[80] Milton made an impassioned plea for freedom of expression and toleration of falsehood,[79] stating:\n Give me the liberty to know, to utter, and to argue freely according to conscience, above all liberties.[79] Milton's defense of freedom of expression was grounded in a Protestant worldview. He thought that the English people had the mission to work out the truth of the Reformation, which would lead to the enlightenment of all people. Nevertheless, Milton also articulated the main strands of future discussions about freedom of expression. By defining the scope of freedom of expression and \"harmful\" speech, Milton argued against the principle of pre-censorship and in favor of tolerance for a wide range of views.[79] Freedom of the press ceased being regulated in England in 1695 when the Licensing Order of 1643 was allowed to expire after the introduction of the Bill of Rights 1689 shortly after the Glorious Revolution.[83][84] The emergence of publications like the Tatler (1709) and the Spectator (1711) are credited for creating a 'bourgeois public sphere' in England that allowed for a free exchange of ideas and information.\n More governments attempted to centralize control as the \"menace\" of printing spread.[85] The French crown repressed printing and the printer Etienne Dolet was burned at the stake in 1546. In 1557 the British Crown thought to stem the flow of seditious and heretical books by chartering the Stationers' Company. The right to print was limited to the members of that guild. Thirty years later, the Star Chamber was chartered to curtail the \"greate enormities and abuses\" of \"dyvers contentyous and disorderlye persons professinge the arte or mystere of pryntinge or selling of books\". The right to print was restricted to two universities and the 21 existing printers in the city of London, which had 53 printing presses. As the British crown took control of type founding in 1637, printers fled to the Netherlands. Confrontation with authority made printers radical and rebellious, with 800 authors, printers, and book dealers being incarcerated in the Bastille in Paris before it was stormed in 1789.[85]\n A succession of English thinkers was at the forefront of early discussion on a right to freedom of expression, among them John Milton (1608\u201374) and John Locke (1632\u20131704). Locke established the individual as the unit of value and the bearer of rights to life, liberty, property and the pursuit of happiness. However, Locke's ideas evolved primarily around the concept of the right to seek salvation for one's soul. He was thus primarily concerned with theological matters. Locke neither supported a universal toleration of peoples nor freedom of speech; according to his ideas, some groups, such as atheists, should not be allowed.[86]\n By the second half of the 17th century philosophers on the European continent like Baruch Spinoza and Pierre Bayle developed ideas encompassing a more universal aspect freedom of speech and toleration than the early English philosophers.[86] By the 18th century the idea of freedom of speech was being discussed by thinkers all over the Western world, especially by French philosophes like Denis Diderot, Baron d'Holbach and Claude Adrien Helv\u00e9tius.[88] The idea began to be incorporated in political theory both in theory as well as practice; the first state edict in history proclaiming complete freedom of speech was the one issued 4 December 1770 in Denmark-Norway during the regency of Johann Friedrich Struensee.[89] However Struensee himself imposed some minor limitations to this edict on 7 October 1771, and it was even further limited after the fall of Struensee with legislation introduced in 1773, although censorship was not reintroduced.[90]\n John Stuart Mill (1806\u20131873) argued that without human freedom, there could be no progress in science, law, or politics, which according to Mill, required free discussion of opinion. Mill's On Liberty, published in 1859, became a classic defence of the right to freedom of expression.[79] Mill argued that truth drives out falsity, therefore the free expression of ideas, true or false, should not be feared. Truth is not stable or fixed but evolves with time. Mill argued that much of what we once considered true has turned out false. Therefore, views should not be prohibited for their apparent falsity. Mill also argued that free discussion is necessary to prevent the \"deep slumber of a decided opinion\". Discussion would drive the march of truth, and by considering false views, the basis of true views could be re-affirmed.[91] Furthermore, Mill argued that an opinion only carries intrinsic value to the owner of that opinion, thus silencing the expression of that opinion is an injustice to a basic human right. It is generally held that for Mill, the only instance in which speech can be justifiably suppressed is to prevent harm from a clear and direct threat.[92] Neither economic or moral implications nor the speaker's own well-being would justify suppression of speech.[93] However Mill in On Liberty suggests the speech of pimps \u2014 instigating clients and sex workers to have sex \u2014 should be restricted. This suggests he may be willing to restrict some speech that, while not harming others, undermines their decisional autonomy.[94]\n In her 1906 biography of Voltaire, Evelyn Beatrice Hall coined the following sentence to illustrate Voltaire's beliefs: \"I disapprove of what you say, but I will defend to the death your right to say it\".[95] Hall's quote is frequently cited to describe the principle of freedom of speech.[95] Noam Chomsky stated, \"If you believe in freedom of speech, you believe in freedom of speech for views you don't like. Dictators such as Stalin and Hitler, were in favor of freedom of speech for views they liked only. If you're in favor of freedom of speech, that means you're in favor of freedom of speech precisely for views you despise\".[96] Lee Bollinger argues that \"the free speech principle involves a special act of carving out one area of social interaction for extraordinary self-restraint, the purpose of which is to develop and demonstrate a social capacity to control feelings evoked by a host of social encounters\". Bollinger argues that tolerance is a desirable value, if not essential. However, critics argue that society should be concerned by those who directly deny or advocate, for example, genocide (see limitations above).[97]\n As chairman of the London-based PEN International, a club which defends freedom of expression and a free press, English author H. G. Wells met with Stalin in 1934 and was hopeful of reform in the Soviet Union. However, during their meeting in Moscow, Wells said, \"the free expression of opinion\u2014even of opposition opinion, I do not know if you are prepared yet for that much freedom here\".[98]\n The 1928 novel Lady Chatterley's Lover by D. H. Lawrence was banned for obscenity in several countries, including the United Kingdom, the United States, Australia, Canada, and India. In the late 1950s and early 1960s, it was the subject of landmark court rulings that saw the ban for obscenity overturned. Dominic Sandbrook of The Telegraph in the UK wrote, \"Now that public obscenity has become commonplace, it is hard to recapture the atmosphere of a society that saw fit to ban books such as Lady Chatterley's Lover because it was likely to 'deprave and corrupt' its readers\".[99] Fred Kaplan of The New York Times stated the overturning of the obscenity laws \"set off an explosion of free speech\" in the U.S.[100] The 1960s also saw the Free Speech Movement, a massive long-lasting student protest on the campus of the University of California, Berkeley, during the 1964\u201365 academic year.[101]\n In contrast to Anglophone nations, France was a haven for literary freedom.[102] The innate French regard for the mind meant that France was disinclined to punish literary figures for their writing, and prosecutions were rare.[102] While it was prohibited everywhere else, James Joyce's Ulysses was published in Paris in 1922. Henry Miller's 1934 novel Tropic of Cancer (banned in the U.S. until 1963) and Lawrence's Lady Chatterley's Lover were published in France decades before they were available in the respective authors' home countries.[102]\n In 1964 comedian Lenny Bruce was arrested in the U.S. due to complaints again about his use of various obscenities. A three-judge panel presided over his widely publicized six-month trial. He was found guilty of obscenity in November 1964. He was sentenced on 21 December 1964, to four months in a workhouse.[103] He was set free on bail during the appeals process and died before the appeal was decided. On 23 December 2003, thirty-seven years after Bruce's death, New York Governor George Pataki granted him a posthumous pardon for his obscenity conviction.[104]\n In the United States, the right to freedom of expression has been interpreted to include the right to take and publish photographs of strangers in public areas without their permission or knowledge.[105][106] This is not the case worldwide.\n In some countries, people are not allowed to talk about certain things such as L\u00e8se-majest\u00e9 which is an offence against the dignity of a reigning sovereign or against a state. Doing so constitutes an offence. For example, Saudi Arabia is responsible for executing journalist Jamal Khashoggi in 2018. As he entered the Saudi embassy in Turkey, a team of Saudi assassins killed him.[107] Another Saudi writer, Raif Badawi, was arrested in 2012 and lashed.[108]\n On 4 March 2022, Russian President Vladimir Putin signed into law a bill introducing prison sentences of up to 15 years for spreading \"fake news\" about Russia's military operation in Ukraine.[109] As of December 2022, more than 4,000 Russians were prosecuted under \"fake news\" laws.[110] The 1993 Russian Constitution expressly prohibits censorship in Article 29 of Chapter 2, Rights and Liberties of Man and Citizen.[111][112]\n"
    },
    {
        "title": "1971 Sudanese coup d'\u00e9tat",
        "url": "https://en.wikipedia.org/wiki/1971_Sudanese_coup_d%27%C3%A9tat",
        "content": "\n Coup attempt fails\n  Democratic Republic of the Sudan\n Revolutionary Council\n Maj. Hashem al Atta\u00a0Col. Babikir al-Nur OsmanMaj. Farouk HamadallahCol. Abdel Moneim Mohamed Ahmed\u00a0Lt. Col. Osman HusseinCpt. Muawaiya Abdul Hay\n The 1971 Sudanese coup d'\u00e9tat was a short-lived communist-backed coup, led by Major Hashem al Atta, one of the founding members of the free officers organization that carried out a coup two years prior, against the government of President Gaafar Nimeiry. The coup took place on 19 July 1971, toppling the government of the Democratic Republic of the Sudan, but failed to garner support either domestically or internationally. After several days Nimeiry loyalists launched a counter-coup, freeing Nimeiry and toppling Atta's government.\n Following the coup Nimeiry, pushed by Defense Minister Khalid Hassan Abbas, made moves to strengthen his rule, and by the end of the year ultimate authority had transferred from the multi-member Revolutionary Command Council to the Presidency, held by Nimeiry. Over the next several years, the remaining former members of the RCC would see their authority diminished, and by 1975 all but Abu al-Gasim Mohammed Ibrahim had been forced out of government.\n After neutralizing the conservative opposition from the Ansar movement, the government of the Revolutionary Command Council (RCC), who had seized power in 1969, concentrated on consolidating its political organization to phase out communist participation in the government. This strategy prompted an internal debate within the Sudanese Communist Party (SCP). The orthodox wing, led by party secretary general Abd al Khaliq Mahjub, demanded a popular front government with communists participating as equal partners. The National Communist wing, on the other hand, supported cooperation with the government.\n Soon after the army had crushed the Ansar at Aba Island, Nimeiri moved against the SCP. He ordered the deportation of Abd al Khaliq Mahjub. Then, when the SCP secretary general returned to Sudan illegally after several months abroad, Nimeiri placed him under house arrest. In March 1971, Nimeiri indicated that trade unions, a traditional communist stronghold, would be placed under government control. The RCC also banned communist affiliated student, women's, and professional organizations. Additionally, Nimeiri announced the planned formation of a national political movement called the Sudan Socialist Union (SSU), which would assume control of all political parties, including the SCP. After this speech, the government arrested the SCP's central committee and other leading communists.\n The SCP, however, retained a covert organization that was not damaged in the sweep. Before further action could be taken against the party, the SCP launched a coup against Nimeiri. The coup occurred on 19 July 1971, when one of the plotters, Major Hisham al Atta, surprised Nimeiri and the RCC meeting in the presidential palace and seized them along with a number of pro-Nimeiri officers. Atta named a seven-member revolutionary council, in which communists ranked prominently, to serve as the national government. Three days after the coup, however, loyal army units stormed the palace, rescued Nimeiri, and arrested Atta and his confederates. Nimeiri, who blamed the SCP for the coup, ordered the arrest of hundreds of communists and dissident military officers. The government subsequently executed some of these individuals and imprisoned many others.\n The coup began in the mid afternoon of 19 July, when Khartoum was relatively quiet due to many Sudanese retiring from the scorching mid afternoon sun to take a siesta. With Khartoum relatively quiet Atta moved tanks into positions around government buildings, capturing the Presidential Palace and taking Nimeiry and several dozen of his followers prisoner.[1]\n At this point Atta declared himself, Babiker Al Nour, and Farouk Osman Hamdallah in charge of the government, proclaiming a new Revolutionary Council. The three men were rumoured communists, and whilst they denied this, their new government's first act was to lift Nimeiry's ban on the Sudanese Communist Party and its various affiliated organisations. Speaking over the radio, Atta announced the new government would work in closer collaboration with communist and socialist countries, and stated that Sudanese communists would be brought into a new coalition government.[1]\n The response to the coup was initially limited, with Atta's forces receiving no opposition from either the Sudanese Armed Forces or from the wider population. Khartoum did however see communists stage pro-coup demonstrations. Whilst Atta was in Khartoum, his fellow coup leaders, Al Nour and Hamdallah, were in London, with Hamdallah having accompanied Al Nour on a trip for medical treatment. Upon being informed of the success of the coup the two wound up their affairs and prepared to fly back to Khartoum, with Al Nour being slated to serve as Chief of Staff in the new government.[1]\n Despite the ease at which Atta had seized Khartoum the coup had little widespread support. While the Sudanese Communist Party was the largest Communist Party in the Arab World, its support base was limited to a small section of Sudan's population. In contrast to this there was a widespread opposition in Sudan's religious and conservative population to communism, which was viewed as having dangerous links to atheism.[1]\n Adding to this was that Sudan's neighbors were also opposed to the new communist government. None of Sudan's neighbours wished to have either a communist or communist-sympathising government for a neighbor,[1] and Anwar Sadat in Egypt ordered first a fact-finding mission to Khartoum, and later ordered Egyptian forces stationed south of Khartoum to resist the coup.[2]\n Gaddafi's Libya also supported Nimeiry. Gaddafi, like Nimeiry, had come to power two years prior. Gaddafi was also at this point virulently anti-communist. Gaddafi's response was far more extreme than Sadat's, and he dispatched two Libyan fighter jets to force down the British Airlines jetliner which was ferrying Al Nour and Hamdallah from London back to Khartoum. The plane was forced down in Libya, and the two were taken off the plane and arrested.[2]\n Saudi Arabia was also worried about the prospect of a new communist government across the Red Sea, although Saudi Arabia refrained from engaging in any clear action against the new Sudanese government. Ba'athist Iraq did however respond favorably to the new government, publicly supporting the coup, and was actually the only Arab government to do so. Baghdad dispatched an airliner to Khartoum carrying an Iraqi delegation to congratulate Atta and his new government, although this crashed under mysterious circumstances whilst crossing Saudi Arabia.[2]\n Unaware that Al Nour and Hamdallah's plane had been forced down in Libya, Atta travelled to Khartoum International Airport on the morning of the 22 July expecting to welcome the two back to Sudan. Atta had realized by this point that the coup may prove more difficult than it originally appeared, and had dispatched orders to prevent any attempted countercoups; the army had been ordered to immobilise its tanks in the Khartoum area, most armored brigades and paratroopers had been put on leave, and the arms and ammunition of units whose loyalty was in doubt had been removed and locked away. According to rumours, Atta had not undertaken these courses of action alone, but had instead done so with the advice and support of Soviet military personnel.[2]\n Upon learning of the status of the plane, Atta travelled to the center of Khartoum to address a rally that he had called to welcome back Al Nour and Hamdullah. Speaking to the crowd, Atta tried desperately to muster support for his coup, but the crowd was thin, Atta was heckled, and there were calls for the return of Nimeiry.[2]\n Within hours, military units loyal to Nimeiry moved into Khartoum, engaging with units loyal to Atta and freeing Nimeiry after a short battle. Al Nour and Hamdullah were returned to Khartoum by Gaddafi, and were executed alongside Atta and a half dozen other coup leaders.[2][3]\n Having survived the SCP-inspired coup, Nimeiri reaffirmed his commitment to establishing a socialist state. A provisional constitution, published in August 1971, described Sudan as a \"socialist democracy\" and provided for a presidential form of government to replace the RCC. A plebiscite the following month elected Nimeiri to a six-year term as president.\n The coup brought major changes in Sudan's foreign and domestic policies. In its aftermath, leading members of the Sudanese Communist Party were executed, and several communist-dominated trade unions were banned. In his foreign policy, Nimeiry expelled East German security advisers and denounced the Soviet Union and most of its European allies for their attitude to the attempted coup.[4][5]\n"
    },
    {
        "title": "2022 SCO summit",
        "url": "https://en.wikipedia.org/wiki/2022_SCO_summit",
        "content": "\u00a0Russia\n\u00a0China\n\u00a0India\n\u00a0Pakistan\n\u00a0Kazakhstan\n\u00a0Kyrgyzstan\n\u00a0Tajikistan\n\u00a0Belarus\n\u00a0Iran\n\u00a0Turkey\n\u00a0Azerbaijan\n\u00a0Turkmenistan\n The 2022 SCO summit was the 22nd annual summit of heads of state of the Shanghai Cooperation Organisation held between 15 and 16 September 2022 in Samarkand, Uzbekistan.[1]\n As part of the meetings, general secretary of the Chinese Communist Party and Chinese president Xi Jinping met with Russian president Vladimir Putin. During the meeting, Putin acknowledged that the Chinese side had \"concerns and questions\" over Russia's invasion of Ukraine.[2]\n Putin also met with Indian prime minister Narendra Modi. During the meeting, Modi said that \"today's era is not of war\".[3]\n The leaders of Kyrgyzstan and Tajikistan met to discuss the 2022 Kyrgyzstan\u2013Tajikistan clashes which escalated during the summit.[4]\n Iran formally submitted its application to join the SCO as a full member state, which is expected to become effective within a year.[5]\n Turkey also announced its intention to join in the future.[6]\n"
    },
    {
        "title": "Improved sanitation",
        "url": "https://en.wikipedia.org/wiki/Improved_sanitation",
        "content": "Improved sanitation (related to but distinct from a \"safely managed sanitation service\") is a term used to categorize types of sanitation for monitoring purposes. It refers to the management of human feces at the household level. The term was coined by the Joint Monitoring Program (JMP) for Water Supply and Sanitation of UNICEF and WHO in 2002 to help monitor the progress towards Goal Number 7 of the Millennium Development Goals (MDGs). The opposite of \"improved sanitation\" has been termed \"unimproved sanitation\" in the JMP definitions. The same terms are used to monitor progress towards Sustainable Development Goal 6 (Target 6.2, Indicator 6.2.1) from 2015 onwards.[2] Here, they are a component of the definition for \"safely managed sanitation service\".  \n The Joint Monitoring Program (JMP) for Water Supply and Sanitation has been publishing updates on the global sanitation situation on an annual basis. For example, in 2015 it was reported that 68% of the world's population had access to improved sanitation.[3]\n In 2015 this goal was replaced by Sustainable Development Goal 6, in which Target 6.2 states: \"By 2030, achieve access to adequate and equitable sanitation and hygiene for all and end open defecation, paying special attention to the needs of women and girls and those in vulnerable situations.\" Indicator 6.2.1 is the \"Proportion of population using (a) safely managed sanitation services and (b) a handwashing facility with soap and water\".[4]\n In 2017, the JMP defined a new term: \"basic sanitation service\". This is defined as the use of improved sanitation facilities that are not shared with other households. A lower level of service is now called \"limited sanitation service\" which refers to the use of improved sanitation facilities that are shared between two or more households. A higher level of service is called \"safely managed sanitation\". This is basic sanitation service where excreta is safely disposed of in situ or transported and treated offsite.[2]\n The definition of improved sanitation facilities is: Those facilities designed to hygienically separate excreta from human contact.[2]:\u200a8\u200a\n The ladder of sanitation services includes (from lowest to highest): open defecation, unimproved, limited, basic, safely managed.[2]:\u200a8\u200a\n An improved sanitation facility is defined as one that hygienically separates human excreta from human contact.[5] It is not necessarily identical with sustainable sanitation. The opposite of \"improved sanitation\" has been termed \"unimproved sanitation\" in the JMP definitions.\n To allow for international comparability of estimates for monitoring the Millennium Development Goals (MDGs), the Joint Monitoring Program (JMP) for Water Supply and Sanitation defines \"improved\" sanitation as the following kind of toilets:[5]\n Sanitation facilities that are not considered as \"improved\" (also called \"unimproved\") are:\n Whilst \"shared\" toilets are not counted as improved sanitation, data about usage of shared toilets is nevertheless reported in the annual progress reports of the JMP.[6]\n"
    },
    {
        "title": "Out to Innovate",
        "url": "https://en.wikipedia.org/wiki/National_Organization_of_Gay_and_Lesbian_Scientists_and_Technical_Professionals",
        "content": "Out to Innovate, previously known as the National Organization of Gay and Lesbian Scientists and Technical Professionals (NOGLSTP), is a professional society for professionals in science, technology, mathematics, and engineering.[1][2] Each year, Out to Innovate gives the Walt Westman Award to members who helped make significant contributions to the association's mission.\n The organization was organized along the lines of earlier organizations of gay scientists in Los Angeles and the Research Triangle area of North Carolina, and arose out of a session at the 1980 American Association for the Advancement of Science (AAAS) meeting. It was formally organized in 1983 and incorporated in California in 1991.  The foundation of the organization was in response to issues such as gay scientists not being able to get visas to immigrate to the United States or security clearances to work in government laboratories, the lack of research on LGBT health issues, and loss of productivity due to the stress of stigmatization.  Much of the organization's early work related to increasing the visibility of LGBT scientists and opposing homophobia.  In the 1990s, it focused on encouraging corporations to adopt nondiscrimination policies and assisted in a 1995 Government Accounting Office report that recommended that LGBT status should not be considered a vulnerability to blackmail in security clearance investigations. In the 2000s and 2010s, awards for LGBT scientists, engineers, and STEM educators were established.\n Out to Innovate supports regional groups and caucuses who choose to affiliate with Out to Innovate. Out to Innovate affiliates and partners with other national STEM organizations, including AAAS. Out to Innovate also organizes a mentoring network, a scholarship program for students, and a biannual career summit.[1]\n In July 2019, Out to Innovate partnered with the Out Astronaut Project, a nonprofit initiative aimed at sending the first out LGBTQIA+ astronaut into space.[3] The goal of the partnership, according to a press release from OAP, is to \"provide opportunities for LGBTQ persons to become actively involved in space-related research.\"[4]  The goals of OAP, beyond sending the first LGBTQIA+ astronaut into space includes providing a robust presence in STEM fields for LGBTQIA+ individuals \"by highlighting the contributions of LGBTQ members currently working in science and space while providing grants to promising LGBTQ students.\"[4] On September 24, 2019, the OAP announced via Facebook that they had found the winner of the first phase of their project.[5]\n Out to Innovate has a number of other partnerships and affiliations.[6] They include: The American Association for the Advancement of Science, the National Postdoctoral Association, and the American Chemical Society.\n Out to Innovate recognizes an LGBTQ+ Scientist, Engineer, and Educator each year \"who has made outstanding contributions to their field\".[7] In addition, they give the Walt Westman Award to recognize Out to Innovate members who have significantly advanced Out to Innovate's mission.\n Awardees are:\n"
    },
    {
        "title": "Sweet spot (sports)",
        "url": "https://en.wikipedia.org/wiki/Sweet_spot_(sports)#Baseball",
        "content": "The sweet spot is a place where a combination of factors results in a maximum response for a given amount of effort. In tennis, squash, racquetball, baseball, cricket or golf a given swing will result in a more powerful hit if the ball strikes the racket, bat or club on the latter's sweet spot. \n The sweet spot is the location at which the object being struck, usually a ball, absorbs the maximum amount of the available forward momentum and rebounds away from the racket, bat, club, etc. with a greater velocity than if struck at any other point on the racket, bat or club.  \n In endurance sports such as cycling, sweet spot training aims to maximise training benefit \u2014 generally for performance at or near functional threshold power (FTP) \u2014 by optimally balancing training effect, physiological strain and maximum duration.[1]\n A batted ball with a launch angle between 8 and 32 degrees is quantified as having been hit off the sweet spot of the bat.[2] Balls hit in the sweet spot are not necessarily hit hard with a high exit velocity.[3]\n The sweet spot of a cricket bat is roughly 150-160mm above the toe.[4] Scientific research conducted at the University of Cambridge discovered that bamboo bats are stronger with a better sweet spot compared to those made of willow.[5]\n"
    },
    {
        "title": "Male contraceptive",
        "url": "https://en.wikipedia.org/wiki/Male_contraceptive",
        "content": "\nMale contraceptives, also known as male birth control, are methods of preventing pregnancy by interrupting the function of sperm.[1] The main forms of male contraception available today are condoms, vasectomy, and withdrawal, which together represented 20% of global contraceptive use in 2019.[2][3][4][5]  New forms of male contraception are in clinical and preclinical stages of research and development, but as of 2024, none have reached regulatory approval for widespread use.[6][7][8][9]\n These new methods include topical creams, daily pills, injections, long-acting implants, and external devices, and these products have both hormonal and non-hormonal mechanisms of action.[6][10][11][12][13][14][15]  Some of these new contraceptives could even be unisex, or usable by any person, because they could theoretically incapacitate mature sperm in the man's body before ejaculation, or incapacitate sperm in the body of a woman after insemination.[16][17]\n In the 21st century, surveys indicated that around half of men in countries across the world have been interested in using a variety of novel contraceptive methods,[18][19][20][21] and men in clinical trials for male contraceptives have reported high levels of satisfaction with the products.[12][22]  Women worldwide have also shown a high level of interest in new male contraceptives, and though both male and female partners could use their own contraceptives simultaneously, women in long-term relationships have indicated a high degree of trust in their male partner's ability to successfully manage contraceptive use.[18][23][24]\n A modelling study from 2018 suggested that even partial adoption of new male contraceptives would significantly reduce unintended pregnancy rates around the globe,[25] which remain at nearly 50%, even in developed countries where women have access to modern contraceptives.[26][27][28] Unintended pregnancies are associated with negative socioeconomic, educational, and health outcomes for women, men, and the resulting children (especially in historically marginalized communities),[27][29][30][31][32][33][34] and 60% of unintended pregnancies end in abortions,[35][36] many of which are unsafe and can lead to women's harm or death.[37][38][39][40]  Therefore, the development of new male contraceptives has the potential to improve racial, economic, and gender equality across the world, advance reproductive justice and reproductive autonomy for all people, and save lives.\n Vasectomy is surgical procedure for permanent male sterilization usually performed in a physician's office in an outpatient procedure.[41] During the procedure, the vasa deferentia of a patient are severed, and then tied or sealed to prevent sperm from being released during ejaculation.[42] Vasectomy is an effective procedure, with less than 0.15% of partners becoming pregnant within the first 12 months after the procedure.[43] Vasectomy is also a widely reliable and safe method of contraception, and complications are both rare and minor.[44][45]  Vasectomies can be reversed, though rates of successful reversal decline as the time since vasectomy increases, and the procedure is technically difficult and often costly.[42][46][41]\n A condom is a barrier device made of latex or thin plastic film that is rolled onto an erect penis before intercourse and retains ejaculated semen, thereby preventing pregnancy.[47] Condoms are less effective at preventing pregnancy than vasectomy or modern methods of female contraception, with a real-world failure rate of 13%.[43] However, condoms have the advantage of providing protection against some sexually transmitted infections such as HIV/AIDS.[48][49] Condoms may be combined with other forms of contraception (such as spermicide) for greater protection.[50]\n The withdrawal method, also known as coitus interruptus or pulling out, is a behavior that involves halting penile-vaginal intercourse to remove the penis out and away from the vagina prior to ejaculation.[51][52] Withdrawal is considered a less-effective contraceptive method, with typical-use failure rates around 20%.[41][43] However, it requires no equipment or medical procedures.[51]\n Researchers have been working to generate novel male contraceptives with diverse mechanisms of action and possible delivery methods, including long-acting reversible contraceptives (LARCs), daily transdermal gels, daily and on-demand oral pills, monthly injectables, and implants.[53][54][55] Efforts to develop male contraceptives have been ongoing for many decades, but progress has been slowed by a lack of funding and industry involvement. As of 2024, most funding for male contraceptive research is derived from government or philanthropic sources.[56][57][58][59]\n Novel male contraceptives could work by blocking various steps of the sperm development process, blocking sperm release, or interfering with any of the sperm functions necessary to reach and fertilize an egg in the female reproductive tract.[60]  Advantages and disadvantages of each of these approaches will be discussed below, along with relevant examples of products in development.\n These methods work by preventing the testes from producing sperm, or interfering with sperm production in a way that leads to the production of nonfunctional sperm.[61]  This approach can be accomplished by either hormonal or nonhormonal small-molecule drugs, or potentially by thermal methods.  The effectiveness of contraceptives in this group can be easily assessed microscopically, by measuring sperm count or abnormalities in sperm shape, but because spermatogenesis takes approximately 70 days to complete,[62] these methods are likely to require approximately three months of use before they become effective, and approaches that halt sperm production at an early stage of the process may result in reduced testicular size.[63] Methods have been suggested in the 1980s.[64]\n Hormonal contraceptives for men work similarly to hormonal female methods, using steroids to interrupt the hypothalamic-pituitary-gonadal axis and thereby block sperm production. Administering external androgens and progestogens suppresses secretion of the gonadotropins LH and FSH, which impairs testosterone production and sperm generation in the testes, leading to reduced sperm counts in ejaculates within 4\u201312 weeks of use.[65] However, since the contraceptives contain testosterone or related androgens, the levels of androgens in the blood remain relatively constant, thereby limiting side effects and maintaining masculine secondary sex characteristics like muscle mass and hair growth.[65]\n Multiple methods of male hormonal contraception have been tested in clinical trials since the 1990s, and although one trial was halted early, leading to a large amount of press attention,[66][67][68][69][70] most hormonal male contraceptives have been found to be effective, reversible, and well-tolerated.[71][72][73][74][75][76]\n As of 2024[update], the following hormonal male contraceptive products are in clinical trials:\n Some anabolic steroids may exhibit suppressive effects on spermatogenesis, but none are being investigated for use as a male contraceptive.[84]\n Non-hormonal contraceptives for men are a diverse group of molecules that act by inhibiting any of the many proteins involved in sperm production, release, or function.  Because sperm cells are highly specialized, they express many proteins that are rare in the rest of the human body.[85][86][87]  This suggests the possibility that non-hormonal contraceptives that specifically block these sperm proteins could have fewer side effects than hormonal contraceptives, since sex steroid receptors are found in tissues throughout the body.[88]  Non-hormonal contraceptives can work by blocking spermatogenesis, sperm release, or mature sperm function, resulting in products with a wide variety of usage patterns, from slow onset to on-demand usage.[89]  Contraceptives targeting mature sperm functions could even be taken by both sperm-producing and egg producing people.[17][16]  Challenges of non-hormonal contraceptive development include bioavailability and delivery past the blood-testis barrier.[90]\n As of 2024[update], the following non-hormonal male contraceptive product is in clinical trials:\n As of 2024[update], the following non-hormonal male contraceptive products are in preclinical development:\n Prolonged testicular heating had been shown to reduce sperm counts in 1941,[115] considered as a method of birth control after 1968 and in the 1980s[116][117] No modern clinical trials have demonstrated the safety, contraceptive effectiveness, or reversibility of this approach. Various devices are in early preclinical stages of development, and as of 2017 some approaches have been used by men through self-experimentation.[14][118]  As of 2015, the mechanism by which heating disrupts spermatogenesis was still not fully understood.[119] There have been theoretical  concerns that prolonged heating could increase the risk of testicular cancer since the inborn birth defect of cryptorchidism carries a risk of testicular cancer[120] or that heating could damage sperm DNA, resulting in harm to potential offspring.[121]\n These approaches work by either physically or chemically preventing the emission of sperm during ejaculation, and are likely to be effective on-demand.\n \u03b11-adrenoceptor antagonists and P2X1 antagonists have been shown to inhibit smooth muscle contractions in the vas deferens during ejaculation, and therefore prevent the release of semen and sperm while maintaining the sensation of orgasm.[122][123][124] Various molecules in these categories are under consideration as possible on-demand male contraceptives.\n Vas-occlusive contraception is a form of male contraception that blocks sperm transport in the vas deferens, the tubes that carry sperm from the epididymis to the ejaculatory ducts.\n Vas-occlusive contraception provides a contraceptive effect through physical blockage of the vas deferens, the duct connecting the epididymis to the urethra. While a vasectomy excises, or removes, a piece of each vas deferens and occludes the remaining open ends of the duct, vas-occlusive methods aim to block the duct while leaving it intact.  Vas-occlusive methods generally aim to create long-acting reversible options, through a second procedure that removes the blockage.[138]  However, full reversibility remains questionable, since animal and human studies have shown sperm abnormalities, incomplete recovery of sperm parameters, and the development of fertility-impairing antibodies against one's own sperm after blockage removal.[111][139][140][141][142][143]\n As of 2024[update], the following vas-occlusive male contraceptive products are in clinical trials:\n As of 2024[update], the following vas-occlusive male contraceptive products are in preclinical development:\n Research into new, more acceptable designs of condoms is ongoing.[164][165]\n These approaches work by blocking functions that mature sperm need in order to reach and fertilize an egg in the female reproductive tract, such as motility, capacitation, semen liquification, or fertilization.  Drugs or devices that target mature sperm are likely to be effective on-demand (taken just before intercourse), and could even be delivered either in sperm-producing or egg-producing bodies, leading to unisex contraceptives.[17][16]\n As of 2024[update], the following non-hormonal male contraceptive approaches are in preclinical or early development:\n Although some people question whether men would be interested in managing their own contraceptives[188] or whether women would trust their male partners to do so successfully,[189] studies consistently show that men around the world have significant levels of interest in novel forms of male contraception[18][23][190][19][191][192] and that women in committed relationships would generally trust their male partners to manage the contraceptive burden in the relationship.[23]  Additionally, males participating in various contraceptive clinical trials have reported high satisfaction with the products they were using.[81][12][22]\n Studies on potential uptake indicate that in most countries, more than half of men surveyed would be willing to use a new method of male contraception.[18][20][190][193][194][195]  Interestingly, some of the highest rates were reported in low-income countries like Nigeria and Bangladesh where 76% of men surveyed indicated that they would be willing to use a new method within the first 12 months that it is available.[18] This is particularly compelling, since it has been estimated that a mere 10% uptake of new male contraceptive methods could avert nearly 40% of unintended pregnancies in Nigeria.[25]  Across the world, many young and middle-aged men especially want the ability to control their own fertility, and are not well-served by existing family planning programs.[196]\n Although a phase II trial for an injectable male contraceptive was halted in 2011 by an independent data safety monitoring board due likely to rare adverse effects experienced by some participants,[197][67] leading many popular articles to suggest men could not tolerate side effects similar to those that many women endure on hormonal birth control,[70][198] in reality more than 80% of the study's male participants stated at the end of the trial that they were satisfied with the contraceptive injection, and would be willing to use the method if it were available.[199]  Subsequent hormonal male contraceptive clinical trials have progressed successfully, showing high levels of efficacy and acceptability among the participants.[12][22][81][200]\n It is sometimes assumed that women won\u2019t trust men to take contraceptives, since women would bear the consequences of a male partner's missed dose or misuse.[189]  Of course, male contraceptive options would not have to replace female contraceptives, and in casual sexual encounters both partners may prefer to independently control their own contraceptive methods.  On the other hand, some long-term couples might want only one partner to bear the contraceptive burden. Indeed, there is evidence that a large proportion of women in relationships in many countries around the world would trust their partners to take a potential male method,[24][18] and many women want more male partner involvement in their own reproductive health services.[201]  Further, current contraceptive use data show that more than a quarter of women worldwide already rely on male-controlled methods for contraception (such as condoms and vasectomy),[202] and this figure could grow as more male contraceptive methods become available.\n Despite the fact that modern female pharmaceutical contraception has been on the market since the 1960s,[203] 40-50% of pregnancies are still unintended worldwide, leading to an approximate total of 121 million unintended pregnancies annually.[204][205][206]  Importantly, most studies on unintended pregnancies only measure women's intentions about the pregnancy, and so pregnancies that were unintended by men are understudied and may be under-reported.[207]  Unintended pregnancies have been shown to be linked with a wide variety of negative outcomes on mental and physical health, as well as educational and socioeconomic attainment in both parents and the children born of unintended pregnancies.[27][29][30][32][33][34]\n Surprisingly, although the rate of unintended pregnancies (per 1000 women of childbearing age) is higher in developing countries,[205][208][209] the percentage of pregnancies that are unintended is actually higher in developed countries, since a lower proportion of women in developed countries are intending to conceive at any given time.[205]  Research indicates that unmet need for modern contraception is the cause of 84% of unintended pregnancies in developing countries.[210]  In the United States, which has a higher unintended pregnancy rate than many other developed nations,[211] one important reason that women cite for nonuse of contraceptives is concerns about the side effects of existing products.[212]  Taken together, these statistics suggests that the current suite of contraceptives is insufficient to meet the fertility planning needs of people across the world, and therefore the introduction of new male contraceptives is likely to decrease the stubbornly high global rates of unintended pregnancy.[25]\n International market research indicates that 49% of men in the United states and 76% of men in Nigeria would try a novel male contraceptive within the first year of its existence.[18]  Independent modelling predicts that even if real-world usage is only 10% as high as the market research suggests, the introduction of a male contraceptive would avert roughly 200,000 unintended pregnancies per year in the USA and Nigeria each.[25]\n Fathers with unintended births report lower proportions of happiness than in fathers with intentional births[213] and unintended fatherhood for men in their early 30's is associated with a significant increase in depressive symptoms.[214]  In addition, men in insecure financial situations are more likely to report a recent unintended pregnancy,[207] and supporting and raising a child brings significant costs that can exacerbate financial insecurity.[215][216]  More broadly, access to effective and reliable contraception would advance men's ability to \"maintain personal bodily autonomy, have children, not have children, and parent the children we have in safe and sustainable communities\" in accordance with the principles of Reproductive Justice.[217]\n Family planning has been found to be associated with overall well-being and is one of the most efficient tools for women's empowerment.[218][219][220] Positive outcomes of effective birth control include improvements in women's health, self-agency, education, labor force participation, financial stability, as well as decreases in pregnancy-related deaths,[221][222][223] and these positive social and health impacts may be further realized by the addition of novel male and unisex methods.[25][18] New male contraceptive options would not come at the expense of women\u2019s reproductive autonomy, since women would still be able to take advantage of all of the contraceptive methods available to them, choose to have both partners use their own contraceptive methods at the same time, or rely solely on their male partners\u2019 form of contraception.\n Interventions encouraging male engagement in couples' reproductive health and decisionmaking have shown positive outcomes related to promoting more equitable gender norms in the context of family planning,[224] and increased joint decision making in couples. It is reasonable to assume from these data that increasing male involvement as contraceptive users will further improve gender equity.[225]\n While this article has used the term \"male\" contraception for clarity, these contraceptives are most accurately described as \"sperm-targeting\" contraceptives, since they would work effectively in any body that produces sperm, regardless of that person's gender identity or external genitalia.[17]  Importantly, contraceptives that block functions of mature sperm could be delivered in a unisex manner, incapacitating sperm before ejaculation in sperm-producing people, or after sperm arrives in the body of egg-producing people.[16][17]\n Transgender, nonbinary, and intersex people are underserved by current contraceptive options.  For example, many trans men can become pregnant (both intentionally and unintentionally),[226]\u00a0but may prefer not to use estrogen- or progestin-containing hormonal birth control (both because of the social classification of these hormones as \"female sex hormones\" and because of a fear they will interfere with masculinizing hormone therapy, although the American College of Obstetricians and Gynecologists states that these hormonal contraceptives have little effect on masculinization.)[227][228][229] Trans women who have not had gender-affirming genital surgery may have similar unmet contraceptive needs as those of cisgender men, since gender-affirming hormonal therapy is not effective contraception.[229]  Nonbinary and intersex people may be less likely to use current methods of birth control, since they are popularly categorized by the labels \"male\" and \"female\", which may not match an individual's gender identity or may invoke feelings of gender dysphoria.[228]  This dynamic may contribute to the higher rates of unintended pregnancies seen in the LGBTQ+ community as compared to heterosexual peers,[230][231][232] which could in theory be ameliorated by the introduction of unisex contraceptives.\n Novel male contraceptive options are predicted to reduce the incidence unintended pregnancies,[25][18] and being the product of an intended rather than unintended pregnancy has been shown to correlate with improved health and wellbeing outcomes in children.[31][222][233][234]  Additionally, reduced family size correlates with improved educational outcomes,[235] and children born after the introduction of family planning programs in the USA experienced a reduction in poverty rates, both in childhood and adulthood.[236]\n Unintended pregnancies rates increase as income decreases, both between countries[36] and between socioeconomic and racial groups within a given country.[211][237] \u00a0Women of color, especially Black women, in the United States and other developed countries have dramatically higher rates of death during and after birth and worse maternal health outcomes, due in part to systemic discrimination.[238][222] \u00a0Since unintended pregnancies can have negative effects on an individual's physical and mental health, educational attainment, and economic prospects, these higher unintended pregnancy rates likely contribute to the persistent socioeconomic gaps within and between societies.[27][29][30][31][32][33][34]  \u00a0 It\u2019s therefore possible that the introduction of new male contraceptives would not only mitigate gender inequities, as discussed above, but racial and income inequities as well, by providing more ways for individuals to avoid unintended pregnancies.[25]\n In addition to the personal financial savings of avoiding unintended pregnancy mentioned above, on a societal level, contraceptives are a public health intervention with a high return on investment: for every dollar the United States government spends on family planning programs, it saves $7.09, for a total of over $13 billion per year.[239]  Unintended pregnancies in the United States are estimated to cause $4.5 billion in direct medical costs.[34][32]  New male contraceptives are likely to prevent some unintended pregnancies[25] and therefore reduce these costs.\n 61% of unintended pregnancies end in abortion,[35] whereas only 20% of all pregnancies end in abortion.[240] Interestingly, unintended pregnancy rates are higher in countries where abortion is illegal than those where abortion is legal, yet the incidence of abortion is similar between these groups of countries.[35][241]  Illegal abortions are more likely to be unsafe, and there are an estimated 25 million unsafe abortions globally each year, leading to 50,000 - 70,000 yearly deaths and 5 million people with long-term health consequences.[37][38][39][40]   Importantly, increases in the prevalence and uptake of modern contraceptives have been shown to decrease unintended pregnancy and abortion rates when fertility rates are constant.[242][243][244]  This suggests that the introduction of new forms of male contraception could prevent a significant number of abortions, save lives, and avoid unnecessary suffering.\n A variety of plant extracts have been used throughout history in attempts to prevent pregnancy, though most were used by women, and the efficacy and safety of these methods is questionable.[245][246][247]\n Condoms made of animal organs or fabric have been in documented use since at least the 16th century,[248] and various types of penile coverings have been depicted and referenced in materials from cultures around the world as early as 3000 BCE, though it is not always clear that these coverings were used for birth control or protection from sexually transmitted infections.[249]  The 1800's saw the development of thick reusable rubber condoms,[248][249] and thinner disposable latex rubber condoms entered production in the 1920s.[250][251][252]\n Vasectomy was first performed in humans in the late 1800s, but not initially as a method of voluntary birth control.  Instead, it was first used as an attempted treatment (later proved to be ineffective) for enlarged prostates, and within a few years, one-sided vasectomy became popular as a supposed method of sexual rejuvenation in older men.[253][254]  Although this rejuvenation treatment was ineffective pseudoscience and any perceived effects were likely due only to the placebo effect, many prominent men, such as Sigmund Freud and W.B. Yeats, sought out the procedure.[255]  In the early 1900s, the use of vasectomy took a darker turn, and it became widely promoted and practiced as a means of eugenic involuntary sterilization.[254][253]  It was not until the 1950s that vasectomy became widely used as a method for voluntary sterilization and family planning.[254][253]  Since then, vasectomy has undergone extensive technical improvements and innovations, such that it is no longer a single procedure, but a family of related procedures.[253][256]\n In the 1990s, and into the early 2000s, major pharmaceutical companies Organon, Wyeth, and Schering were pursuing preclinical and clinical development of various male contraceptive products, but in 2006, all three companies ceased development of these products within a short time of each other, for reasons that have not been publicly released.[257][110]\n In 2013, the Male Contraceptive Initiative was founded with the goal of funding and supporting the development of new male contraceptives.[258][259]\n In 2020, Dr. Polina Lishko was awarded the MacArthur \"Genius\" Fellowship for her contributions to the understanding of sperm physiology, with the award specifically noting her work on \"opening up new avenues in ... the development of male-specific or unisex contraceptives.\"[260]\n Many researchers have attempted to develop male contraceptive products over the last hundred years.  A selection of these efforts (that are no longer in development as of 2024) are listed below.\n"
    },
    {
        "title": "Agrarianism",
        "url": "https://en.wikipedia.org/wiki/Agrarianism",
        "content": "\n Defunct\n Agrarianism is a social and political philosophy that advocates for rural development and a rural agricultural lifestyle, family farming, widespread property ownership, and political decentralization.[1][2] Those who adhere to agrarianism tend to value traditional forms of local community over urban modernity.[3] Agrarian political parties sometimes aim to support the rights and sustainability of small farmers and poor peasants against the wealthy in society.[4]\n Some scholars suggest that agrarianism espouses the superiority of rural society to urban society and the independent farmer as superior to the paid worker, and sees farming as a way of life that can shape the ideal social values.[5] It stresses the superiority of a simpler rural life in comparison  to the complexity of urban life. For example, M. Thomas Inge defines agrarianism by the following basic tenets:[6]\n The philosophical roots of agrarianism include European and Chinese philosophers. The Chinese school of Agriculturalism (\u519c\u5bb6/\u8fb2\u5bb6) was a philosophy that advocated peasant utopian communalism and egalitarianism. In societies influenced by Confucianism that had as its foundation that humans are innately good, the farmer was considered an esteemed productive member of society, but merchants who made money were looked down upon.[7] That influenced European intellectuals like Fran\u00e7ois Quesnay, an avid Confucianist and advocate of China's agrarian policies, in forming the French agrarian philosophy of physiocracy.[8] The physiocrats, along with the ideas of John Locke and the Romantic Era, formed the basis of modern European and American agrarianism.\n Physiocracy (French: physiocratie; from the Greek for \"government of nature\") is an economic theory developed by a group of 18th-century Age of Enlightenment French economists who believed that the wealth of nations derived solely from the value of \"land agriculture\" or \"land development\" and that agricultural products should be highly priced.[9] Their theories originated in France and were most popular during the second half of the 18th century. Physiocracy became one of the first well-developed theories of economics.[10]\n Fran\u00e7ois Quesnay (1694\u20131774), the marquis de Mirabeau (1715\u20131789) and Anne-Robert-Jacques Turgot (1727\u20131781) dominated the movement,[11] which immediately preceded the first modern school, classical economics, which began with the publication of Adam Smith's The Wealth of Nations in 1776.\n The physiocrats made a significant contribution in their emphasis on productive work as the source of national wealth. This contrasted with earlier schools, in particular mercantilism, which often focused on the ruler's wealth, accumulation of gold, or the balance of trade. Whereas the mercantilist school of economics held that value in the products of society was created at the point of sale,[12] by the seller exchanging his products for more money than the products had \"previously\" been worth, the physiocratic school of economics was the first to see labor as the sole source of value.  However, for the physiocrats, only agricultural labor created this value in the products of society.[12]  All \"industrial\" and non-agricultural labors were \"unproductive appendages\" to agricultural labor.[12]\n Quesnay was likely influenced by his medical training, particularly by the work of William Harvey who explained  how blood flow and the circulatory system is vital to the human body; Quesnay held that the circulation of wealth was vital to the economy. Societies at the time were also overwhelmingly agrarian. This may be why they viewed agriculture as the primary source of a nation's wealth. This is an idea which Quesnay purported to demonstrate with data, comparing a workshop to a farm. He analyzed \"how money flowed between the three classes of farmers, proprietors, and artisans, in the same mechanical way that blood flows between different organs\" and claimed only the farm produced a surplus that added to the nation's wealth.\nPhysiocrats viewed the production of goods and services as equivalent to the consumption of the agricultural surplus, since human or animal muscle provided the main source of power and all energy derived from the surplus from agricultural production.  Profit in capitalist production was really only the \"rent\" obtained by the owner of the land on which the agricultural production took place.[12]\n The United States president Thomas Jefferson was an agrarian who based his ideas about the budding American democracy around the notion that farmers are \"the most valuable citizens\" and the truest republicans.[15] Jefferson and his support base were committed to American republicanism, which they saw as being in opposition to aristocracy and corruption, and which prioritized virtue, exemplified by the \"yeoman farmer\", \"planters\", and the \"plain folk\".[16] In praising the rural farmfolk, the Jeffersonians felt that financiers, bankers and industrialists created \"cesspools of corruption\" in the cities and should thus be avoided.[17]\n The Jeffersonians sought to align the American economy more with agriculture than industry. Part of their motive to do so was Jefferson's fear that the over-industrialization of America would create a class of wage slaves who relied on their employers for income and sustenance. In turn, these workers would cease to be independent voters as their vote could be manipulated by said employers. To counter this, Jefferson introduced, as scholar Clay Jenkinson noted, \"a graduated income tax that would serve as a disincentive to vast accumulations of wealth and would make funds available for some sort of benign redistribution downward\" and tariffs on imported articles, which were mainly purchased by the wealthy.[18] In 1811, Jefferson, writing to a friend, explained: \"these revenues will be levied entirely on the rich... . the rich alone use imported articles, and on these alone the whole taxes of the general government are levied. the poor man ... pays not a farthing of tax to the general government, but on his salt.\"[19]\n There is general agreement that the substantial United States' federal policy of offering land grants (such as thousands of gifts of land to veterans) had a positive impact on economic development in the 19th century.[20]\n Agrarian socialism is a form of agrarianism that is anti-capitalist in nature and seeks to introduce socialist economic systems in their stead.\n Notable agrarian socialists include Emiliano Zapata who was a leading figure in the Mexican Revolution. As part of the Liberation Army of the South, his group of revolutionaries fought on behalf of the Mexican peasants, whom they saw as exploited by the landowning classes. Zapata published the Plan of Ayala, which called for significant land reforms and land redistribution in Mexico as part of the revolution. Zapata was killed and his forces crushed over the course of the Revolution, but his political ideas lived on in the form of Zapatismo.\n Zapatismo would form the basis for neozapatismo, the ideology of the Zapatista Army of National Liberation. Known as Ej\u00e9rcito Zapatista de Liberaci\u00f3n Nacional or EZLN in Spanish, EZLN is a far-left libertarian socialist political and militant group that emerged in the state of Chiapas in southmost Mexico in 1994. EZLN and Neozapatismo, as explicit in their name, seek to revive the agrarian socialist movement of Zapata, but fuse it with new elements such as a commitment to indigenous rights and community-level decision making.\n Subcommander Marcos, a leading member of the movement, argues that the peoples' collective ownership of the land was and is the basis for all subsequent developments the movement sought to create: \n...When the land became property of the peasants ... when the land passed into the hands of those who work it ... [This was] the starting point for advances in government, health, education, housing, nutrition, women's participation, trade, culture, communication, and information ...[it was] recovering the means of production, in this case, the land, animals, and machines that were in the hands of large property owners.\"[21] Maoism, the far-left ideology of Mao Zedong and his followers, places a heavy emphasis on the role of peasants in its goals. In contrast to other Marxist schools of thought which normally seek to acquire the support of urban workers, Maoism sees the peasantry as key. Believing that \"political power grows out of the barrel of a gun\",[22] Maoism saw the Chinese Peasantry as the prime source for a Marxist vanguard because it possessed two qualities: (i) they were poor, and (ii) they were a political blank slate; in Mao's words, \"A clean sheet of paper has no blotches, and so the newest and most beautiful words can be written on it\".[23] During the Chinese Civil War and the Second Sino-Japanese War, Mao and the Chinese Communist Party made extensive use of peasants and rural bases in their military tactics, often eschewing the cities.\n Following the eventual victory of the Communist Party in both wars, the countryside and how it should be run remained a focus for Mao. In 1958, Mao launched the Great Leap Forward, a social and economic campaign which, amongst other things, altered many aspects of rural Chinese life. It introduced mandatory collective farming and forced the peasantry to organize itself into communal living units which were known as people's communes. These communes, which consisted of 5,000 people on average, were expected to meet high production quotas while the peasants who lived on them adapted to this radically new way of life. The communes were run as co-operatives where wages and money were replaced by work points. Peasants who criticised this new system were persecuted as \"rightists\" and \"counter-revolutionaries\". Leaving the communes was forbidden and escaping from them was difficult or impossible, and those who attempted it were subjected to party-orchestrated \"public struggle sessions,\" which further jeopardized their survival.[24] These public criticism sessions were often used to intimidate the peasants into obeying local officials and they often devolved into little more than public beatings.[25]\n On the communes, experiments were conducted in order to find new methods of planting crops, efforts were made to construct new irrigation systems on a massive scale, and the communes were all encouraged to produce steel backyard furnaces as part of an effort to increase steel production. However, following the Anti-Rightist Campaign, Mao had instilled a mass distrust of intellectuals into China, and thus engineers often were not consulted with regard to the new irrigation systems and the wisdom of asking untrained peasants to produce good quality steel from scrap iron was not publicly questioned. Similarly, the experimentation with the crops did not produce results. In addition to this the Four Pests Campaign was launched, in which the peasants were called upon to destroy sparrows and other wild birds that ate crop seeds, in order to protect fields. Pest birds were shot down or scared away from landing until they dropped from exhaustion. This campaign resulted in an ecological disaster that saw an explosion of the vermin population, especially crop-eating insects, which was consequently not in danger of being killed by predators.\n None of these new systems were working, but local leaders did not dare to state this, instead, they falsified reports so as not to be punished for failing to meet the quotas. In many cases they stated that they were greatly exceeding their quotas, and in turn, the Chinese state developed a completely false sense of success with regard to the commune system.[26]\n All of this culminated in the Great Chinese Famine, which began in 1959, lasted 3 years, and saw an estimated 15 to 30 million Chinese people die.[27] A combination of bad weather and the new, failed farming techniques that were introduced by the state led to massive shortages of food. By 1962, the Great Leap Forward was declared to be at an end.\n In the late 1960s and early 1970s, Mao once again radically altered life in rural China with the launching of the Down to the Countryside Movement. As a response to the Great Chinese Famine, the Chinese President Liu Shaoqi began \"sending down\" urban youths to rural China in order to recover its population losses and alleviate overcrowding in the cities. However, Mao turned the practice into a political crusade, declaring that the sending down would strip the youth of any bourgeois tendencies by forcing them to learn from the unprivileged rural peasants. In reality, it was the Communist Party's attempt to reign in the Red Guards, who had become uncontrollable during the course of the Cultural Revolution. 10% of the 1970 urban population of China was sent out to remote rural villages, often in Inner Mongolia. The villages, which were still poorly recovering from the effects of the Great Chinese Famine, did not have the excess resources that were needed to support the newcomers. Furthermore, the so-called \"sent-down youth\" had no agricultural experience and as a result, they were unaccustomed to the harsh lifestyle that existed in the countryside, and their unskilled labor in the villages provided little benefit to the agricultural sector. As a result, many of the sent-down youth died in the countryside. The relocation of the youths was originally intended to be permanent, but by the end of the Cultural Revolution, the Communist Party relented and some of those who had the capacity to return to the cities were allowed to do so.[28]\n In imitation of Mao's policies, the Khmer Rouge of Cambodia (who were heavily funded and supported by the People's Republic of China) created their own version of the Great Leap Forward which was known as \"Maha Lout Ploh\". With the Great Leap Forward as its model, it had similarly disastrous effects, contributing to what is now known as the Cambodian genocide. As a part of the Maha Lout Ploh, the Khmer Rouge sought to create an entirely agrarian socialist society by forcibly relocating 100,000 people to move from Cambodia's cities into newly created communes. The Khmer Rouge leader, Pol Pot sought to \"purify\" the country by setting it back to \"Year Zero\", freeing it from \"corrupting influences\".[29] Besides trying to completely de-urbanize Cambodia, ethnic minorities were slaughtered along with anyone else who was suspected of being a \"reactionary\" or a member of the \"bourgeoisie\", to the point that wearing glasses was seen as grounds for execution.[30] The killings were only brought to an end when Cambodia was invaded by the neighboring socialist nation of Vietnam, whose army toppled the Khmer Rouge.[31] However, with Cambodia's entire society and economy in disarray, including its agricultural sector, the country still plunged into renewed famine due to vast food shortages. However, as international journalists began to report on the situation and send images of it out to the world, a massive international response was provoked, leading to one of the most concentrated relief efforts of its time.[32]\n Peasant parties first appeared across Eastern Europe between 1860 and 1910, when commercialized agriculture and world market forces disrupted traditional rural society, and the railway and growing literacy facilitated the work of roving organizers. Agrarian parties advocated land reforms to redistribute land on large estates among those who work it. They also wanted village cooperatives to keep the profit from crop sales in local hands and credit institutions to underwrite needed improvements. Many peasant parties were also nationalist parties because peasants often worked their land for the benefit of landlords of different ethnicity.\n Peasant parties rarely had any power before World War I but some became influential in the interwar era, especially in Bulgaria and Czechoslovakia. For a while, in the 1920s and the 1930s, there was a Green International (International Agrarian Bureau) based on the peasant parties in Bulgaria, Czechoslovakia, Poland, and Serbia. It functioned primarily as an information center that spread the ideas of agrarianism and combating socialism on the left and landlords on the right and never launched any significant activities.\n In Bulgaria, the Bulgarian Agrarian National Union (BZNS) was organized in 1899 to resist taxes and build cooperatives. BZNS came to power in 1919 and introduced many economic, social, and legal reforms. However, conservative forces crushed BZNS in a 1923 coup and assassinated its leader, Aleksandar Stamboliyski (1879\u20131923). BZNS was made into a communist puppet group until 1989, when it reorganized as a genuine party.\n In Czechoslovakia, the Republican Party of Agricultural and Smallholder People often shared power in parliament as a partner in the five-party p\u011btka coalition. The party's leader, Anton\u00edn \u0160vehla (1873\u20131933), was prime minister several times. It was consistently the strongest party, forming and dominating coalitions. It moved beyond its original agrarian base to reach middle-class voters. The party was banned by the National Front after the Second World War.[33]\n In France, the Hunting, Fishing, Nature, Tradition party is a moderate conservative, agrarian party, reaching a peak of 4.23% in the 2002 French presidential election. It would later on become affiliated to France's main conservative party, Union for a Popular Movement.  More recently, the Resistons! movement of Jean Lassalle espoused agrarianism.\n In Hungary, the first major agrarian party, the small-holders party was founded in 1908. The party became part of the government in the 1920s but lost influence in the government. A new party, the Independent Smallholders, Agrarian Workers and Civic Party was established in 1930 with a more radical program representing larger scale land redistribution initiatives. They implemented this program together with the other coalition parties after WWII. However, after 1949 the party was outlawed when a one-party system was introduced. They became part of the government again 1990\u20131994, and 1998\u20132002 after which they lost political support. The ruling Fidesz party has an agrarian faction, and promotes agrarian interest since 2010 with the emphasis now placed on supporting larger family farms versus small-holders.\n In the late 19th century, the Irish National Land League aimed to abolish landlordism in Ireland and enable tenant farmers to own the land they worked on. The \"Land War\" of 1878\u20131909 led to the Irish Land Acts, ending absentee landlords and ground rent and redistributing land among peasant farmers.\n Post-independence, the Farmers' Party operated in the Irish Free State from 1922, folding into the National Centre Party in 1932. It was mostly supported by wealthy farmers in the east of Ireland.\n Clann na Talmhan (Family of the Land; also called the National Agricultural Party) was founded in 1938. They focused more on the poor smallholders of the west, supporting land reclamation, afforestation, social democracy and rates reform. They formed part of the governing coalition of the Government of the 13th D\u00e1il and Government of the 15th D\u00e1il. Economic improvement in the 1960s saw farmers vote for other parties and Clann na Talmhan disbanded in 1965.\n In Kazakhstan, the Peasants' Union, originally a communist organization, was formed as one of first agrarian parties in independent Kazakhstan and would win four seats in the 1994 legislative election.[34][35] The Agrarian Party of Kazakhstan, led by Romin Madinov, was founded in 1999, which favored the privatization of agricultural land, developments towards rural infrastructure, as well as changes in the tax system in agrarian economy.[36] The party would go on to win three M\u00e4jilis seats in the 1999 legislative election and eventually unite with the Civic Party of Kazakhstan to form the pro-government Agrarian-Industrial Union of Workers (AIST) bloc that would be chaired by Madinov for the 2004 legislative election, with the AIST bloc winning 11 seats in the M\u00e4jilis.[37][38] From there, the bloc remained short-lived as it would merge with the ruling Nur Otan party in 2006.[39]\n Several other parties in Kazakhstan over the years have embraced agrarian policies in their programs in an effort to appeal towards a large rural Kazakh demographic base, which included Amanat, ADAL, and Respublica.[40][41][42]\n Since late 2000s, the \"Auyl\" People's Democratic Patriotic Party remains the largest and most influential agrarian-oriented party in Kazakhstan, as its presidential candidate Jiguli Dairabaev had become the second-place frontrunner in the 2022 presidential election after sweeping 3.4% of the vote.[43] In the 2023 legislative election, the Auyl party for the first time was represented the parliament after winning nine seats in the lower chamber M\u00e4jilis.[44] The party raises rural issues in regard to decaying villages, rural development and the agro-industrial complex, the issues of social security of the rural population, and has consistently opposed the ongoing rural flight in Kazakhstan.[45]\n In Latvia, the Union of Greens and Farmers is supportive of traditional small farms and perceives them as more environmentally friendly than large-scale farming: Nature is threatened by development, while small farms are threatened by large industrial-scale farms.\n In Lithuania, the government led by the Lithuanian Farmers and Greens Union was in power between 2016 and 2020.\n The Nordic agrarian parties,[46] also referred to as Scandinavian agrarian parties[47][48] or agrarian liberal parties,[49][50] are agrarian political parties that belong to a political tradition particular to the Nordic countries. Positioning themselves in the centre of the political spectrum, but fulfilling roles distinctive to Nordic countries, they remain hard to classify by conventional political ideology.\n These parties are non-Socialist and typically combine a commitment to small businesses, rural issues and political decentralisation, and, at times, scepticism towards the European Union. The parties have divergent views on the free market and environmentalism. Internationally, they are most commonly aligned to the Alliance of Liberals and Democrats for Europe (ALDE) and the Liberal International.\n Defunct\n \n In Poland, the Polish People's Party (Polskie Stronnictwo Ludowe, PSL) traces its tradition to an agrarian party in Austro-Hungarian-controlled Galician Poland. After the fall of the communist regime, PSL's biggest success came in 1993 elections, where it won 132 out of 460 parliamentary seats. Since then, PSL's support has steadily declined, until 2019, when they formed Polish Coalition with an anti-establishment, direct democracy Kukiz'15 party, and managed to get 8.5% of popular vote. Moreover, PSL tends to get much better results in local elections. In 2014 elections they have managed to get 23.88% of votes.\n The right-wing Law and Justice party has also become supportive of agrarian policies in recent years and polls show that most of their support comes from rural areas.[52] AGROunia resembles the features of agrarianism.\n In Romania, older party parties from Transylvania, Moldavia, and Wallachia merged to become the National Peasants' Party (PN\u021a) in 1926. Iuliu Maniu (1873\u20131953) was a prime minister with an agrarian cabinet from 1928 to 1930 and briefly in 1932\u20131933, but the Great Depression made proposed reforms impossible. The communist administration dissolved the party in 1947 (along with other historical parties such as the National Liberal Party), but it reformed in 1989 after they fell from power.\n The reformed party, which also incorporated elements of Christian democracy in its ideology, governed Romania as part of the Romanian Democratic Convention (CDR) between 1996 and 2000.\n In Serbia, Nikola Pa\u0161i\u0107 (1845\u20131926) and his People's Radical Party dominated Serbian politics after 1903. The party also monopolized power in Yugoslavia from 1918 to 1929. During the dictatorship of the 1930s, the prime minister was from that party.\n In Ukraine, the Radical Party of Oleh Lyashko has promised to purify the country of oligarchs \"with a pitchfork\".[53] The party advocates a number of traditional left-wing positions (a progressive tax structure, a ban on agricultural land sale and eliminating the illegal land market, a tenfold increase in budget spending on health, setting up primary health centres in every village)[54] and mixes them with strong nationalist sentiments.[55]\n In land law the heyday of English, Irish (and thus Welsh) agrarianism was c.\u20091500 to 1603, led by the Tudor royal advisors, who sought to maintain a broad pool of agricultural commoners from which to draw military men, against the interests of larger landowners who sought enclosure (meaning complete private control of common land, over which by custom and common law lords of the manor always enjoyed minor rights). The heyday was eroded by hundreds of Acts of Parliament to expressly permit enclosure, chiefly from 1650 to the 1810s.  Politicians standing strongly as reactionaries to this included the Levellers, those anti-industrialists (Luddites) going beyond opposing new weaving technology and, later, radicals such as William Cobbett.\n A high level of net national or local self-sufficiency has a strong base in campaigns and movements. In the 19th century such empowered advocates included Peelites and most Conservatives. The 20th century saw the growth or start of influential non-governmental organisations, such as the National Farmers' Union of England and Wales, Campaign for Rural England, Friends of the Earth (EWNI) and of the England Wales, Scottish and Northern Irish political parties prefixed by and focussed on Green politics. The 21st century has seen decarbonisation already in electricity markets.  Following protests and charitable lobbying local food has seen growing market share, sometimes backed by wording in public policy papers and manifestos. The UK has many sustainability-prioritising businesses, green charity campaigns, events and lobby groups ranging from espousing allotment gardens (hobby community farming) through to a clear policy of local food and/or self-sustainability models.\n Historian F.K. Crowley finds that:\n Australian farmers and their spokesman have always considered that life on the land is inherently more virtuous, as well as more healthy, more important and more productive, than life in the towns and cities....The farmers complained that something was wrong with an electoral system which produced parliamentarians who spent money beautifying vampire-cities instead of developing the interior.[56] The National Party of Australia (formerly called the Country Party), from the 1920s to the 1970s, promulgated its version of agrarianism, which it called \"countrymindedness\".  The goal was to enhance the status of the graziers (operators of big sheep stations) and small farmers and justified subsidies for them.[57]\n The New Zealand Liberal Party aggressively promoted agrarianism in its heyday (1891\u20131912). The landed gentry and aristocracy ruled Britain at this time. New Zealand never had an aristocracy but its wealthy landowners largely controlled politics before 1891. The Liberal Party set out to change that by a policy it called \"populism.\" Richard Seddon had proclaimed the goal as early as 1884: \"It is the rich and the poor; it is the wealthy and the landowners against the middle and labouring classes. That, Sir, shows the real political position of New Zealand.\"[58]  The Liberal strategy was to create a large class of small landowning farmers who supported Liberal ideals. The Liberal government also established the basis of the later welfare state such as old age pensions and developed a system for settling industrial disputes, which was accepted by both employers and trade unions. In 1893, it extended voting rights to women, making New Zealand the first country in the world to do so.\n To obtain land for farmers, the Liberal government from 1891 to 1911 purchased 3,100,000 acres (1,300,000\u00a0ha) of Maori land. The government also purchased 1,300,000 acres (530,000\u00a0ha) from large estate holders for subdivision and closer settlement by small farmers. The Advances to Settlers Act (1894) provided low-interest mortgages, and the agriculture department disseminated information on the best farming methods. The Liberals proclaimed success in forging an egalitarian, anti-monopoly land policy. The policy built up support for the Liberal Party in rural North Island electorates. By 1903, the Liberals were so dominant that there was no longer an organized opposition in Parliament.[59][60]\n \nThe United States and Canada both saw a rise of Agrarian-oriented parties in the early twentieth century as economic troubles motivated farming communities to become politically active. It has been proposed that different responses to agrarian protest largely determined the course of power generated by these newly energized rural factions. According to Sociologist Barry Eidlin: \"In the United States, Democrats adopted a co-optive response to farmer and labor protest, incorporating these constituencies into the New Deal coalition. In Canada, both mainstream parties adopted a coercive response, leaving these constituencies politically excluded and available for an independent left coalition.\"[61] These reactions may have helped determine the outcome of agrarian power and political associations in the US and Canada.\n Economic desperation experienced by farmers across the state of Kansas in the nineteenth century spurred the creation of The People's Party in 1890, and soon-after would gain control of the governor's office in 1892. This party, consisting of a mix of Democrats, Socialists, Populists, and Fusionists, would find itself buckling from internal conflict regarding the unlimited coinage of silver. The Populists permanently lost power in 1898.[62]\n \nOklahoma farmers considered their political activity during the early twentieth century due to the outbreak of war, depressed crop prices, and an inhibited sense of progression towards owning their own farms. Tenancy had been reportedly as high as 55% in Oklahoma by 1910.[63] These pressures saw agrarian counties in Oklahoma supporting Socialist policies and politics, with the Socialist platform proposing a deeply agrarian-radical platform: ...the platform proposed a \"Renters and Farmer's Program\" which was strongly agrarian radical in its insistence upon various measures to put land into \"The hands of the actual tillers of the soil.\" Although it did not propose to nationalize privately owned land, it did offer numerous plans to enlarge the state's public domain, from which land would be rented at prevailing share rents to tenants until they had paid rent equal to the land's value. The tenant and his children would have the right of occupancy and use, but the 'title' would remind in the 'commonwealth', an arrangement that might be aptly termed 'Socialist fee simple'. They proposed to exempt from taxation all farm dwellings, animals, and improvements up to the value of $1,000. The State Board of Agriculture would encourage 'co-operative societies' of farmers to make plans f or the purchase of land, seed, tools, and for preparing and selling produce. In order to give farmers essential  services at cost, the Socialists called for the creation of state banks and mortgage agencies, crop insurance, elevators, and warehouses.[64] This agrarian-backed Socialist party would win numerous offices, causing a panic within the local Democratic party. This agrarian-Socialist movement would be inhibited by voter suppression laws aimed at reducing the participation of voters of color, as well as national wartime policies intended to disrupt political elements considered subversive. This party would peak in power in 1914.\n Agrarianism is similar to but not identical with the back-to-the-land movement. Agrarianism concentrates on the fundamental goods of the earth, on communities of more limited economic and political scale than in modern society, and on simple living, even when the shift involves questioning the \"progressive\" character of some recent social and economic developments. Thus, agrarianism is not industrial farming, with its specialization on products and industrial scale.[65]\n \n"
    },
    {
        "title": "Legal guardian",
        "url": "https://en.wikipedia.org/wiki/Legal_custody",
        "content": "A legal guardian is a person who has been appointed by a court or otherwise has the legal authority (and the corresponding duty) to make decisions relevant to the personal and property interests of another person who is deemed incompetent,[1] called a ward. For example, a legal guardian might be granted the authority to make decisions regarding a ward's housing or medical care or manage the ward's finances.[2] Guardianship is most appropriate when an alleged ward is functionally incapacitated, meaning they have a lagging skill critical to performing certain tasks, such as making important life decisions.[3] Guardianship intends to serve as a safeguard to protect the ward.[4]\n Anyone can petition for a guardianship hearing if they believe another individual cannot make rational decisions on their own behalf.[1] In a guardianship hearing, a judge ultimately decides whether guardianship is appropriate and, if so, will appoint a guardian.[2] Guardians are typically used in four situations: guardianship for an incapacitated elderly person (due to old age or infirmity), guardianship for a minor, and guardianship for developmentally disabled adults and for adults found to be incompetent. A family member is most commonly appointed guardian,[citation needed] though a professional guardian or public trustee may be appointed if a suitable family member is not available.\n Guardianship for an incapacitated elderly person typically arises when someone determines that an elderly person has become unable to care for their own person and/or property. In fact, most alleged wards are elderly (Ms = 76\u201382 years), many of whom resided in a care facility and had been diagnosed with a neurological impairment such as dementia.[3]  Typically, a precipitating incident prompts a professional, family member, health care worker, or clergyman to initiate guardianship proceedings. While guardianship intends to protect and support incapacitated elderly people unable to care themselves or engage in the activities of daily living without assistance, guardianship sometimes results in financial exploitation of wards.\n The process will generally start with a determination whether the alleged incapacitated person is actually incapacitated. There will often be an evidentiary hearing. A systematic review [3] of guardianship studies from the United States, Sweden, and Australia found that the most commonly used evidence in guardianship hearings was the alleged ward's medical condition; perhaps surprisingly, descriptions of the alleged ward's cognitive abilities, functional abilities and psychiatric symptoms are much less common.\n If the court determines an individual is incapacitated, the court then determines whether a guardian is necessary, the extent of the guardian's legal authority, (e.g. a guardian may be needed for the person's finances but not for the person) and, if so, who the guardian should be.[5] The determination of whether a guardianship is necessary may consider a number of factors, including whether there is a lesser restrictive alternative, such as the use of an already existing power of attorney and health care proxy.[6] In some cases, a guardianship dispute can become quite contentious and can result in litigation between a parent and adult children or between different siblings against each other in what is essentially a pre-probate dispute over a parent's wealth.\n A report published in 2010 by the U.S. Government Accountability Office looked at 20 selected closed cases in which guardians stole or otherwise improperly obtained assets from clients.  In 6 of these 20 cases, the courts failed to adequately screen guardians ahead of time and appointed individuals with criminal convictions or significant financial problems, and in 12 of 20 cases, the courts failed to oversee guardians once they had been appointed.[7][8]\n In October 2017, The New Yorker published an article looking at the situation in Nevada in which professional guardians sometimes have a number of clients, and argued toward the conclusion that in a number of cases the courts did not properly oversee these arrangements.[9] In 2018, the investigative documentary \"The Guardians\" was published, alleging \"legal kidnapping of elderly people\" in Nevada by private guardianship businesses with no familial or other preexisting relations to their wards, seeking to economically profit from seniors' savings.[10]\n A minor child's parents are the child's natural guardians.[11]\n Most jurisdictions recognise that the parents of a child are the natural guardians of the child, and that the parents may designate who shall become the child's legal guardian in the event of death, typically subject to the approval of the court. The court may appoint a guardian for a minor if their parents are disabled or deceased  or if the minor's parents cannot properly manage their child's safety and well-being.[12] If a non-parent is appointed as guardian, the court will determine how the parents' parental rights are impacted by the appointment (e.g., establishing visitation schedules).[13]\n Legal guardians may be appointed in guardianship cases for adults (see also conservatorship). For example, because parents are not automatically appointed to serve as the guardian of their mentally or physical disabled child who reaches adulthood,[2] parents may start a guardianship action to become the legal guardians when the child reaches the age of majority.\n A famous example of such an arrangement is the situation involving Britney Spears, who was placed into a conservatorship under the supervision of her father, Jamie Spears, and attorney Andrew Wallet in 2008, following a series of highly publicized personal struggles and issues with mental health.\n Courts generally have the power to appoint a guardian for an individual in need of special protection. A guardian with responsibility for both the personal well-being and the financial interests of the ward is a general guardian. A person may also be appointed as a special guardian, having limited powers over the interests of the ward. A special guardian may, for example, be given the legal right to determine the disposition of the ward's property without being given any authority over the ward's person.\n Depending on the jurisdiction, a legal guardian may be called a \"conservator\", \"tutor\", \"custodian\", or curator.  Many jurisdictions and the Uniform Probate Code distinguish between a \"guardian\" or \"guardian of the person\" who is an individual with authority over and fiduciary responsibilities for the physical person of the ward, and a \"conservator\" or \"guardian of the property\" of a ward who has authority over and fiduciary responsibilities for significant property (often an inheritance or personal injury settlement) belonging to the ward. Some jurisdictions provide for public guardianship programs serving incapacitated adults or children.[14]\n A guardian is a fiduciary and is held to a very high standard of care in exercising their powers. If the ward owns substantial property, then the guardian may be required to give a surety bond to protect the ward in case dishonesty or incompetence on their part causes financial loss to the ward.\n The Latin legal term ad litem means \"for the lawsuit\" or \"for the legal proceeding\". A guardian ad litem is thus someone appointed to represent in court the interests of a person too vulnerable to represent themselves, typically due to youth or mental incapacity.\n Guardianship is not federally regulated in the United States; therefore, states vary widely in how they address and manage guardianship cases.[15][12]\n Guardians ad litem (GsAL) are persons appointed by the court to represent \"the best interests of the child\" in court proceedings. They are not the same as \"legal guardians\" and are often appointed in under-age-children cases, many times to represent the interests of the minor children. Guardians ad litem may be called, in some U.S. states, Court Appointed Special Advocates (CASA). In New York State, they are known as attorneys-for-the-child (AFCs). They are the voice of the child and may represent the child in court, with many judges adhering to any recommendation given by a GAL. GALs may assist where a child is removed from a hostile environment and custody given to the relevant state or county family services agency, and in those cases assists in the protection of the minor child.\n Qualifications vary by state, ranging from no experience or qualification, volunteers to social workers to attorneys to others. The GAL's only job is to represent the minor children's best interest and advise the court. A guardian ad litem is an officer of the court, does not represent the parties in the suit, and often enjoys quasi-judicial immunity from any action from the parties involved in a particular case. Qualifications for becoming recognized as a GAL could differ in some states.[16][17] In, for instance, North Carolina, an applicant (volunteer) must go through a background check and complete 30 hours of training.[18] In Minnesota, the minimum qualifications to become a GAL are Bachelor's degree in psychology, social work, education, nursing, criminal justice, law or child-related discipline and some experience working with families and children or an equivalent combination of education and relevant experience. In addition, experience as a Guardian ad Litem with completion of the Guardian ad Litem pre-service orientation requirements is requested.[19][20][21]\n Although a guardian ad litem working through a CASA program volunteers their services, some guardians ad litem are paid for their services. They must submit detailed time and expense reports to the court for approval. Their fees are taxed as costs in the case. Courts may order all parties to share in the cost, or the court may order a particular party to pay the fees. Volunteer guardians ad litem and those that volunteer though a CASA program need to make sure that they do not engage in the unauthorized practice of law. Therefore, when they appear in court (even if they are an attorney) as a volunteer GAL, it is best practice to be represented by an attorney and have attorneys file motions on their behalf.\n Guardians ad litem are also appointed in cases where there has been an allegation of child abuse, child neglect, PINS, juvenile delinquency, or dependency. In these situations, the guardian ad litem is charged to represent the best interests of the minor child, which can differ from the position of the state or government agency as well as the interest of the parent or guardian. These guardians ad litem vary by jurisdiction and can be volunteer advocates or attorneys. For example, in North Carolina, trained GAL volunteers are paired with attorney advocates to advocate for the best interest of abused and neglected children. The program defines a child's best interest as a safe, permanent home.[22]\n Guardians ad litem can be appointed by the court to represent the interests of mentally ill or disabled persons. For example, the Code of Virginia requires that the court appoint a \"discreet and competent attorney-at-law\" or \"some other discreet and proper person\" to serve as guardian ad litem to protect the interests of a person under a disability.[23]\n Guardians ad litem are sometimes appointed in probate matters to represent the interests of unknown or unlocated heirs to an estate.\n When a settlement is reached in personal injury or medical malpractice cases involving claims brought on behalf of a minor or an incapacitated plaintiff, courts normally appoint a guardian ad litem to review the terms of the settlement, and to ensure it is fair and in the best interests of the claimant. The settlement guardian ad litem thoroughly investigates the case, to determine whether the settlement amount is fair and reasonable.[24]\n Because guardianship limits a ward's autonomy and ability to make certain life decisions, guardianship has the potential to damage a ward's health\u00a0and well-being.[1][2] As a result, individuals considering guardianship to support a loved one with functional incapacities might consider whether there are less restrictive alternatives that can achieve the same objectives.[1][25][15][2] Three examples of alternatives include establishing advance directives,[15][1][2] relying on supported decision-making,[15] or taking advantage of community-related services that support individuals with functional limitations.[2]\n Advance directives allow a competent individual to provide their input as to what actions should be taken should they become incompetent.[1] For example, in a healthcare setting, an advance directive would allow a patient to voice what treatment options they prefer and who they would like to make decisions on their behalf should they become incompetent.[1] The establishment of advance directives is a common practice among seniors in the United States.[26]\n Further, some individuals with limited functional capacities might maintain their autonomy by relying on family or friends who can help that individual informally or formally navigate important life decisions without formal guardianship,[2] called \"supported decision-making\".[27][15] For example, these support individuals can provide suggestions on where their loved one should live or recommend certain treatment options in medical settings.[2] This support system can also help the individual modify their environment to promote their success. For example, if a family member is concerned that their loved one with reduced functional capacity might engage in an unsafe behavior (e.g., leaving the gas stove on), this family member can reduce the opportunity for this behavior (e.g., removing the gas stove) without court involvement.[2] This technique allows individuals to support and empower loved ones who are cognitively impaired.[27]\n Finally, employing community services that will alleviate stressors of daily living may allow an alleged ward to maintain their autonomy.[2] For example, certain volunteer organizations provide services such as telephone check-ins and home visits, and many medical or mental health professionals offer in-home services.[2]\n In summary, while guardianship sometimes offers the best solution to supporting an individual who demonstrates functional incapacity, one might consider exploring alternative solutions before seeking legal guardianship.[25][2]\n The Korean Family Courts, typically, has the authority to appoint a guardian in Korea. A general adult guardian is one who is in charge of both the ward's financial interests and personal welfare. The Korean family court, or one of its branches, has authority over the ward's address and will hear the guardianship case. When the Family Court is not present in the ward's address, typically, a district court or a branch court has jurisdiction over the matter.\n Typically, after an evaluation of the ward's health by a doctor, the court proceedings begin. The court will often question the ward and hear his/her testimony regarding the guardianship. So that the ward can make the most use of his or her remaining capacity and choose a suitable guardian. The court has the power to decide the beginning of guardianship, the choice of a guardian, change of guardian, cessation of guardianship, the extent of the legal representative's authority, etc.[29]\n Guardians ad litem are employed by  Children and Family Court Advisory and Support Service (CAFCASS), a non-departmental public body, to represent the interests of children in cases where the child's wishes differ from those of either parent, known as a Section 16.4 case. The posts are filled by senior social workers with experience in family law proceedings.\n In 2006, a legal status of \"special guardianship\" was introduced (using powers delegated by the Adoption and Children Act 2002) to allow for a child to be cared for by a person with rights similar to a traditional legal guardian, but without absolute legal separation from the child's birth parents.[30] These are not to be confused with court-appointed special guardians in other jurisdictions.\n See section 13 of the Prison Act 1952.\n In section 4 of the Official Secrets Act 1989, the expression \"legal custody\" includes detention in pursuance of any enactment or any instrument made under an enactment.[31]\n See section 86 of the Children Act 1975.\n Any person required or authorized by or by virtue of the Mental Health Act 1983 to be conveyed to any place or to be kept in custody or detained in a place of safety or at any place to which he is taken under section 42(6) of that Act is, while being so conveyed, detained or kept, as the case may be, deemed to be in legal custody. In England and Wales, only an Approved Mental Health Professional has the power to detain a person under the Act.[32] For this purpose \"convey\" includes any other expression denoting removal from one place to another.[33]\n The German guardianship law with regard to adults was completely changed in 1990. Guardianship (Vormundschaft) of an adult was renamed 'curatorship' (Betreuung), although it remains Vormundschaft for minors. When a person of full age who, as a result of mental disease or physical, mental or psychological handicap or otherwise is incapable of managing his own affairs, a guardian (Betreuer) can be appointed (section 1,896, German Civil Code). An adult guardian is responsible for personal and estate matters, as well as for medical treatment. However, the ward has normally full capacity with all human rights such as those to marry, vote or make a will. The ward's legal capacity can be lost as a result of a court judgment or order (section 1903, German Civ. C.; Einwilligungsvorbehalt). Every guardian has to report annually to the guardianship court (Betreuungsgericht). Professional guardians (Berufsbetreuer) normally hold university degrees in law or social work.\n In Israel, over 50,000 adults have had legal guardians appointed for them; 85% of them have family members as their guardians, and 15% have professional guardians.  Until 2014, guardians (the term there is \"Apotropos\") were supervised by the Office of the Administrator General at the Ministry of Justice in matters of property only.  However, changes in Israel and other countries along with public pressure, appeals to the courts by social organizations, academic studies and the State Comptroller's 2004 report led to the decision to broaden the scope of supervision to include personal matters as well, to ensure that the guardians take care of all areas of life, including medical care, personal care, suitable housing, work and employment, social and recreational activities, etc., taking account of the person's wishes and acting accordingly.  The Office of the Administrator General (public guardian) at the Ministry of Justice is now implementing a system to supervise guardians in regard to personal matters in order to help identify situations in which guardians are not performing their duties adequately.[34]\n The court-appointed guardian system in the Republic of Ireland was brought into law on the proposal of the noted gay activist and member of Seanad \u00c9ireann (the Irish Senate),  David Norris. The Children Acts Advisory Board which was set up to advise the ministers of the government on policy development under the Child Care Act 1991 was then abolished in September 2011. Judges are responsible for appointing child guardians and can choose guardians from Barnardo's a children's charitable service or from among the self-employed guardians, who are mostly former social workers who have gone into private business since the legislation.[35][36]\n Saudi Arabia has edited the law, and women in Saudi Arabia are no longer required to have a male guardian (Wali) to give permission for various government and economic transactions, as well as some personal life and health decisions.\n Swedish parental law (the Parental Code) regulates legal guardianship for both children and disabled adults. Legal guardianship for unaccompanied minors is regulated by a law of its own. Except for normal parenthood, the guardianship is assigned by the district court and supervised by the Chief Guardian, a municipal authority that is mandatory in every Swedish municipality. What is included in the field of guardianship is decided by the district court. The responsibility for health care and nursing is never included in the guardianship for adults, but is always so for minors. The guardianship for adults can take two legal forms, \"conservator\" or \"administrator\". The main difference between these two is that an \"administrator\" has the sole permission to take legal actions within the field of the guardianship. A guardianship can have different legal forms for different parts of the guardianship. Such things as basic human rights is never denied the ward by this law, but some of them can be denied by other laws. A conservator is normally assigned with the approval of the ward. But if the physical conditions of the ward does not permit him to give such approval, a conservator can be assigned anyhow. Everything a conservator does for his ward have to be approved by him, or can be assumed to be approved by him. For more complex situations, like taking loans or selling of a house, he or she needs approval from the local authorities. Once a year a legally assigned guardian have to send his accounting to the Chief Guardian for review.\n Since 2017, the ward can, while she still have her mental abilities, write a special future letter of attorney (Framtidsfullmakt) which later can be used when she loses her abilities. How such a letter should be written is described in detail in the paternal law, and normally follows the principles of a will. This law was created since in Sweden, it is unclear if a normal letter of attorney is valid after the ward has lost her abilities.[37]\n"
    },
    {
        "title": "Ecological economics",
        "url": "https://en.wikipedia.org/wiki/Ecological_Economics",
        "content": "Organizations:\n Empirical methods\n Prescriptive and policy\n Ecological economics, bioeconomics, ecolonomy, eco-economics, or ecol-econ is both a transdisciplinary and an interdisciplinary field of academic research addressing the interdependence and coevolution of human economies and natural ecosystems, both intertemporally and spatially.[1] By treating the economy as a subsystem of Earth's larger ecosystem, and by emphasizing the preservation of natural capital, the field of ecological economics is differentiated from environmental economics, which is the mainstream economic analysis of the environment.[2] One survey of German economists found that ecological and environmental economics are different schools of economic thought, with ecological economists emphasizing strong sustainability and rejecting the proposition that physical (human-made) capital can substitute for natural capital (see the section on weak versus strong sustainability below).[3]\n Ecological economics was founded in the 1980s as a modern discipline on the works of and interactions between various European and American academics (see the section on History and development below). The related field of green economics is in general a more politically applied form of the subject.[4][5]\n According to ecological economist Malte Michael Faber\u00a0[de], ecological economics is defined by its focus on nature, justice, and time. Issues of intergenerational equity, irreversibility of environmental change, uncertainty of long-term outcomes, and sustainable development guide ecological economic analysis and valuation.[6] Ecological economists have questioned fundamental mainstream economic approaches such as cost-benefit analysis, and the separability of economic values from scientific research, contending that economics is unavoidably normative, i.e. prescriptive, rather than positive or descriptive.[7] Positional analysis, which attempts to incorporate time and justice issues, is proposed as an alternative.[8][9] Ecological economics shares several of its perspectives with feminist economics, including the focus on sustainability, nature, justice and care values.[10] Karl Marx also commented on relationship between capital and ecology, what is now known as ecosocialism.[11]\n The antecedents of ecological economics can be traced back to the Romantics of the 19th century as well as some Enlightenment political economists of that era. Concerns over population were expressed by Thomas Malthus, while John Stuart Mill predicted the desirability of the stationary state of an economy. Mill thereby anticipated later insights of modern ecological economists, but without having had their experience of the social and ecological costs of the Post\u2013World War II economic expansion. In 1880, Marxian economist Sergei Podolinsky attempted to theorize a labor theory of value based on embodied energy; his work was read and critiqued by Marx and Engels.[12] Otto Neurath developed an ecological approach based on a natural economy whilst employed by the Bavarian Soviet Republic in 1919. He argued that a market system failed to take into account the needs of future generations, and that a socialist economy required calculation in kind, the tracking of all the different materials, rather than synthesising them into money as a general equivalent. In this he was criticised by neo-liberal economists such as Ludwig von Mises and Freidrich Hayek in what became known as the socialist calculation debate.[13]\n The debate on energy in economic systems can also be traced back to Nobel prize-winning radiochemist Frederick Soddy (1877\u20131956). In his book Wealth, Virtual Wealth and Debt (1926), Soddy criticized the prevailing belief of the economy as a perpetual motion machine, capable of generating infinite wealth\u2014a criticism expanded upon by later ecological economists such as Nicholas Georgescu-Roegen and Herman Daly.[14]\n European predecessors of ecological economics include K. William Kapp (1950)[15] Karl Polanyi (1944),[16] and Romanian economist Nicholas Georgescu-Roegen (1971). Georgescu-Roegen, who would later mentor Herman Daly at Vanderbilt University, provided ecological economics with a modern conceptual framework based on the material and energy flows of economic production and consumption. His magnum opus, The Entropy Law and the Economic Process (1971), is credited by Daly as a fundamental text of the field, alongside Soddy's Wealth, Virtual Wealth and Debt.[17] Some key concepts of what is now ecological economics are evident in the writings of Kenneth Boulding and E. F. Schumacher, whose book Small Is Beautiful \u2013 A Study of Economics as if People Mattered (1973) was published just a few years before the first edition of Herman Daly's comprehensive and persuasive Steady-State Economics (1977).[18][19]\n The first organized meetings of ecological economists occurred in the 1980s. These began in 1982, at the instigation of Lois Banner,[20] with a meeting held in Sweden (including Robert Costanza, Herman Daly, Charles Hall, Bruce Hannon, H.T. Odum, and David Pimentel).[21] Most were ecosystem ecologists or mainstream environmental economists, with the exception of Daly. In 1987, Daly and Costanza edited an issue of Ecological Modeling to test the waters. A book entitled Ecological Economics, by Joan Martinez Alier, was published later that year.[21] Alier renewed interest in the approach developed by Otto Neurath during the interwar period.[22]  The year 1989 saw the foundation of the International Society for Ecological Economics and publication of its journal, Ecological Economics, by Elsevier. Robert Costanza was the first president of the society and first editor of the journal, which is currently edited by Richard Howarth. Other figures include ecologists C.S. Holling and H.T. Odum, biologist Gretchen Daily, and physicist Robert Ayres. In the Marxian tradition, sociologist John Bellamy Foster and CUNY geography professor David Harvey explicitly center ecological concerns in political economy.\n Articles by Inge Ropke (2004, 2005)[23] and Clive Spash (1999)[24] cover the development and modern history of ecological economics and explain its differentiation from resource and environmental economics, as well as some of the controversy between American and European schools of thought. An article by Robert Costanza, David Stern, Lining He, and Chunbo Ma[25] responded to a call by Mick Common to determine the foundational literature of ecological economics by using citation analysis to examine which books and articles have had the most influence on the development of the field. However, citations analysis has itself proven controversial and similar work has been criticized by Clive Spash for attempting to pre-determine what is regarded as influential in ecological economics through study design and data manipulation.[26] In addition, the journal Ecological Economics has itself been criticized for swamping the field with mainstream economics.[27][28]\n Various competing schools of thought exist in the field. Some are close to resource and environmental economics while others are far more heterodox in outlook. An example of the latter is the European Society for Ecological Economics.  An example of the former is the Swedish Beijer International Institute of Ecological Economics. Clive Spash has argued for the classification of the ecological economics movement, and more generally work by different economic schools on the environment, into three main categories.  These are the mainstream new resource economists, the new environmental pragmatists,[29] and the more radical social ecological economists.[30] International survey work comparing the relevance of the categories for mainstream and heterodox economists shows some clear divisions between environmental and ecological economists.[31] A growing field of radical social-ecological theory is degrowth economics.[32]Degrowth addresses both biophysical limits and global inequality while rejecting neoliberal economics. Degrowth prioritizes grassroots initiatives in progressive socio-ecological goals, adhering to ecological limits by shrinking the human ecological footprint (See Differences from Mainstream Economics Below). It involves an equitable downscale in both production and consumption of resources in order to adhere to biophysical limits. Degrowth draws from Marxian economics, citing the growth of efficient systems as the alienation of nature and man.[33] Economic movements like degrowth reject the idea of growth itself. Some degrowth theorists call for an \"exit of the economy\".[34] Critics of the degrowth movement include new resource economists, who point to the gaining momentum of sustainable development. These economists highlight the positive aspects of a green economy, which include equitable access to renewable energy and a commitment to eradicate global inequality through sustainable development (See Green Economics).[34] Examples of heterodox ecological economic experiments include the Catalan Integral Cooperative and the Solidarity Economy Networks in Italy. Both of these grassroots movements use communitarian based economies and consciously reduce their ecological footprint by limiting material growth and adapting to regenerative agriculture.[35]\n Cultural and heterodox applications of economic interaction around the world have begun to be included as ecological economic practices. E.F. Schumacher introduced examples of non-western economic ideas to mainstream thought in his book, Small is Beautiful, where he addresses neoliberal economics through the lens of natural harmony in Buddhist economics.[18] This emphasis on natural harmony is witnessed in diverse cultures across the globe. Buen Vivir is a traditional socio-economic movement in South America that rejects the western development model of economics. Meaning Good Life, Buen Vivir emphasizes harmony with nature, diverse pluralculturism, coexistence, and inseparability of nature and material. Value is not attributed to material accumulation, and it instead takes a more spiritual and communitarian approach to economic activity. Ecological Swaraj originated out of India, and is an evolving world view of human interactions within the ecosystem. This train of thought respects physical bio-limits and non-human species, pursuing equity and social justice through direct democracy and grassroots leadership. Social well-being is paired with spiritual, physical, and material well-being. These movements are unique to their region, but the values can be seen across the globe in indigenous traditions, such as the Ubuntu Philosophy in South Africa.[36]\n Ecological economics differs from mainstream economics in that it heavily reflects on the ecological footprint of human interactions in the economy. This footprint is measured by the impact of human activities on natural resources and the waste generated in the process. Ecological economists aim to minimize the ecological footprint, taking into account the scarcity of global and regional resources and their accessibility to an economy.[37] Some ecological economists prioritise adding natural capital to the typical capital asset analysis of land, labor, and financial capital. These ecological economists use tools from mathematical economics, as in mainstream economics, but may apply them more closely to the natural world. Whereas mainstream economists tend to be technological optimists, ecological economists are inclined to be technological sceptics. They reason that the natural world has a limited carrying capacity and that its resources may run out. Since destruction of important environmental resources could be practically irreversible and catastrophic, ecological economists are inclined to justify cautionary measures based on the precautionary principle.[38] As ecological economists try to minimize these potential disasters, calculating the fallout of environmental destruction becomes a humanitarian issue as well. Already, the Global South has seen trends of mass migration due to environmental changes. Climate refugees from the Global South are adversely affected by changes in the environment, and some scholars point to global wealth inequality within the current neoliberal economic system as a source of this issue.[39]\n The most cogent example of how the different theories treat similar assets is tropical rainforest ecosystems, most obviously the Yasuni region of Ecuador.  While this area has substantial deposits of bitumen it is also one of the most diverse ecosystems on Earth and some estimates establish it has over 200 undiscovered medical substances in its genomes \u2013 most of which would be destroyed by logging the forest or mining the bitumen.  Effectively, the instructional capital of the genomes is undervalued by analyses that view the rainforest primarily as a source of wood, oil/tar and perhaps food. Increasingly the carbon credit for leaving the extremely carbon-intensive (\"dirty\") bitumen in the ground is also valued \u2013 the government of Ecuador set a price of US$350M for an oil lease with the intent of selling it to someone committed to never exercising it at all and instead preserving the rainforest.\n While this natural capital and ecosystems services approach has proven popular amongst many it has also been contested as failing to address the underlying problems with mainstream economics, growth, market capitalism and monetary valuation of the environment.[40][41][42]  Critiques concern the need to create a more meaningful relationship with Nature and the non-human world than evident in the instrumentalism of shallow ecology and the environmental economists commodification of everything external to the market system.[43][44][45]\n A simple circular flow of income diagram is replaced in ecological economics by a more complex flow diagram reflecting the input of solar energy, which sustains natural inputs and environmental services which are then used as units of production. Once consumed, natural inputs pass out of the economy as pollution and waste. The potential of an environment to provide services and materials is referred to as an \"environment's source function\", and this function is depleted as resources are consumed or pollution contaminates the resources. The \"sink function\" describes an environment's ability to absorb and render harmless waste and pollution: when waste output exceeds the limit of the sink function, long-term damage occurs.[46]:\u200a8\u200a Some persistent pollutants, such as some organic pollutants and nuclear waste are absorbed very slowly or not at all; ecological economists emphasize minimizing \"cumulative pollutants\".[46]:\u200a28\u200a Pollutants affect human health and the health of the ecosystem.\n The economic value of natural capital and ecosystem services is accepted by mainstream environmental economics, but is emphasized as especially important in ecological economics. Ecological economists may begin by estimating how to maintain a stable environment before assessing the cost in dollar terms.[46]:\u200a9\u200a Ecological economist Robert Costanza led an attempted valuation of the global ecosystem in 1997. Initially published in Nature, the article concluded on $33 trillion with a range from $16 trillion to $54 trillion (in 1997, total global GDP was $27 trillion).[47] Half of the value went to nutrient cycling. The open oceans, continental shelves, and estuaries had the highest total value, and the highest per-hectare values went to estuaries, swamps/floodplains, and seagrass/algae beds. The work was criticized by articles in Ecological Economics Volume 25, Issue 1, but the critics acknowledged the positive potential for economic valuation of the global ecosystem.[46]:\u200a129\u200a\n The Earth's carrying capacity is a central issue in ecological economics. Early economists such as Thomas Malthus pointed out the finite carrying capacity of the earth, which was also central to the MIT study Limits to Growth. Diminishing returns suggest that productivity increases will slow if major technological progress is not made. Food production may become a problem, as erosion, an impending water crisis, and soil salinity (from irrigation) reduce the productivity of agriculture. Ecological economists argue that industrial agriculture, which exacerbates these problems, is not sustainable agriculture, and are generally inclined favorably to organic farming, which also reduces the output of carbon.[46]:\u200a26\u200a\n Global wild fisheries are believed to have peaked and begun a decline, with valuable habitat such as estuaries in critical condition.[46]:\u200a28\u200a The aquaculture or farming of piscivorous fish, like salmon, does not help solve the problem because they need to be fed products from other fish. Studies have shown that salmon farming has major negative impacts on wild salmon, as well as the forage fish that need to be caught to feed them.[48][49]\n Since animals are higher on the trophic level, they are less efficient sources of food energy. Reduced consumption of meat would reduce the demand for food, but as nations develop, they tend to adopt high-meat diets similar to that of the United States. Genetically modified food (GMF) a conventional solution to the problem, presents numerous problems \u2013 Bt corn produces its own Bacillus thuringiensis toxin/protein, but the pest resistance is believed to be only a matter of time.[46]:\u200a31\u200a\n Global warming is now widely acknowledged as a major issue, with all national scientific academies expressing agreement on the importance of the issue. As the population growth intensifies and energy demand increases, the world faces an energy crisis. Some economists and scientists forecast a global ecological crisis if energy use is not contained \u2013 the Stern report is an example. The disagreement has sparked a vigorous debate on issue of discounting and intergenerational equity.\n Mainstream economics has attempted to become a value-free 'hard science', but ecological economists argue that value-free economics is generally not realistic. Ecological economics is more willing to entertain alternative conceptions of utility, efficiency, and cost-benefits such as positional analysis or multi-criteria analysis. Ecological economics is typically viewed as economics for sustainable development,[50] and may have goals similar to green politics.\n In international, regional, and national policy circles, the concept of the green economy grew in popularity as a response to the financial predicament at first then became a vehicle for growth and development.[51]\n The United Nations Environment Programme (UNEP) defines a 'green economy' as one that focuses on the human aspects and natural influences and an economic order that can generate high-salary jobs. In 2011, its definition was further developed as the word 'green' is made to refer to an economy that is not only resourceful and well-organized but also impartial, guaranteeing an objective shift to an economy that is low-carbon, resource-efficient, and socially-inclusive.\n The ideas and studies regarding the green economy denote a fundamental shift for more effective, resourceful, environment-friendly and resource\u2010saving technologies that could lessen emissions and alleviate the adverse consequences of climate change, at the same time confront issues about resource exhaustion and grave environmental dilapidation.[52]\n As an indispensable requirement and vital precondition to realizing sustainable development, the Green Economy adherents robustly promote good governance. To boost local investments and foreign ventures, it is crucial to have a constant and foreseeable macroeconomic atmosphere. Likewise, such an environment will also need to be transparent and accountable. In the absence of a substantial and solid governance structure, the prospect of shifting towards a sustainable development route would be insignificant. In achieving a green economy, competent institutions and governance systems are vital in guaranteeing the efficient execution of strategies, guidelines, campaigns, and programmes.\n Shifting to a Green Economy demands a fresh mindset and an innovative outlook of doing business. It likewise necessitates new capacities, skills set from labor and professionals who can competently function across sectors, and able to work as effective components within multi-disciplinary teams. To achieve this goal, vocational training packages must be developed with focus on greening the sectors. Simultaneously, the educational system needs to be assessed as well in order to fit in the environmental and social considerations of various disciplines.[53]\n Among the topics addressed by ecological economics are methodology, allocation of resources, weak versus strong sustainability, energy economics, energy accounting and balance, environmental services, cost shifting, modeling, and monetary policy.\n A primary objective of ecological economics (EE) is to ground economic thinking and practice in physical reality, especially in the laws of physics (particularly the laws of thermodynamics) and in knowledge of biological systems.  It accepts as a goal the improvement of human well-being through development, and seeks to ensure achievement of this through planning for the sustainable development of ecosystems and societies. Of course the terms development and sustainable development are far from lacking controversy. Richard B. Norgaard argues traditional economics has hi-jacked the development terminology in his book Development Betrayed.[54]\n Well-being in ecological economics is also differentiated from welfare as found in mainstream economics and the 'new welfare economics' from the 1930s which informs resource and environmental economics.  This entails a limited preference utilitarian conception of value i.e., Nature is valuable to our economies, that is because people will pay for its services such as clean air, clean water, encounters with wilderness, etc.\n Ecological economics is distinguishable from neoclassical economics primarily by its assertion that the economy is embedded within an environmental system. Ecology deals with the energy and matter transactions of life and the Earth, and the human economy is by definition contained within this system. Ecological economists argue that neoclassical economics has ignored the environment, at best considering it to be a subset of the human economy.\n The neoclassical view ignores much of what the natural sciences have taught us about the contributions of nature to the creation of wealth e.g., the planetary endowment of scarce matter and energy, along with the complex and biologically diverse ecosystems that provide goods and ecosystem services directly to human communities: micro- and macro-climate regulation, water recycling, water purification, storm water regulation, waste absorption, food and medicine production, pollination, protection from solar and cosmic radiation, the view of a starry night sky, etc.\n There has then been a move to regard such things as natural capital and ecosystems functions as goods and services.[55][56]  However, this is far from uncontroversial within ecology or ecological economics due to the potential for narrowing down values to those found in mainstream economics and the danger of merely regarding Nature as a commodity.  This has been referred to as ecologists 'selling out on Nature'.[57]  There is then a concern that ecological economics has failed to learn from the extensive literature in environmental ethics about how to structure a plural value system.\n Resource and neoclassical economics focus primarily on the efficient allocation of resources and less on the two other problems of importance to ecological economics: distribution (equity), and the scale of the economy relative to the ecosystems upon which it relies.[58] Ecological economics makes a clear distinction between growth (quantitative increase in economic output) and development (qualitative improvement of the quality of life), while arguing that neoclassical economics confuses the two. Ecological economists point out that beyond modest levels, increased per-capita consumption (the typical economic measure of \"standard of living\") may not always lead to improvement in human well-being, but may have harmful effects on the environment and broader societal well-being. This situation is sometimes referred to as uneconomic growth (see diagram above).\n Ecological economics challenges the conventional approach towards natural resources, claiming that it undervalues natural capital by considering it as interchangeable with human-made capital\u2014labor and technology.\n The impending depletion of natural resources and increase of climate-changing greenhouse gasses should motivate us to examine how political, economic and social policies can benefit from alternative energy. Shifting dependence on fossil fuels with specific interest within just one of the above-mentioned factors easily benefits at least one other. For instance, photo voltaic (or solar) panels have a 15% efficiency when absorbing the sun's energy, but its construction demand has increased 120% within both commercial and residential properties. Additionally, this construction has led to a roughly 30% increase in work demands (Chen).\n The potential for the substitution of man-made capital for natural capital is an important debate in ecological economics and the economics of sustainability.\nThere is a continuum of views among economists between the strongly neoclassical positions of Robert Solow and Martin Weitzman, at one extreme and the 'entropy pessimists', notably Nicholas Georgescu-Roegen and Herman Daly, at the other.[59]\n Neoclassical economists tend to maintain that man-made capital can, in principle, replace all types of natural capital.  This is known as the weak sustainability view, essentially that every technology can be improved upon or replaced by innovation, and that there is a substitute for any and all scarce materials.\n At the other extreme, the strong sustainability view argues that the stock of natural resources and ecological functions are irreplaceable.  From the premises of strong sustainability, it follows that economic policy has a fiduciary responsibility to the greater ecological world, and that sustainable development must therefore take a different approach to valuing natural resources and ecological functions.\n Recently, Stanislav Shmelev developed a new methodology for the assessment of progress at the macro scale based on multi-criteria methods, which allows consideration of different perspectives, including strong and weak sustainability or conservationists vs industrialists and aims to search for a 'middle way' by providing a strong neo-Keynesian economic push without putting excessive pressure on the natural resources, including water or producing emissions, both directly and indirectly.[60]\n A key concept of energy economics is net energy gain, which recognizes that all energy sources require an initial energy investment in order to produce energy.  To be useful the energy return on energy invested (EROEI) has to be greater than one.  The net energy gain from the production of coal, oil and gas has declined over time as the easiest to produce sources have been most heavily depleted.[62] In traditional energy economics, surplus energy is often seen as something to be capitalized on\u2014either by storing for future use or by converting it into economic growth. \n Ecological economics generally rejects the view of energy economics that growth in the energy supply is related directly to well-being, focusing instead on biodiversity and creativity \u2013 or natural capital and individual capital, in the terminology sometimes adopted to describe these economically. In practice, ecological economics focuses primarily on the key issues of uneconomic growth and quality of life. Ecological economists are inclined to acknowledge that much of what is important in human well-being is not analyzable from a strictly economic standpoint and suggests an interdisciplinary approach combining social and natural sciences as a means to address this. When considering surplus energy, ecological economists state this  could be used for activities that do not directly contribute to economic productivity but instead enhance societal and environmental well-being. This concept of d\u00e9pense, as developed by Georges Bataille, offers a novel perspective on the management of surplus energy within economies. This concept encourages a shift from growth-centric models to approaches that prioritise sustainable and meaningful expenditures of excess resources.[63]\n Thermoeconomics is based on the proposition that the role of energy in biological evolution should be defined and understood through the second law of thermodynamics, but also in terms of such economic criteria as productivity, efficiency, and especially the costs and benefits (or profitability) of the various mechanisms for capturing and utilizing available energy to build biomass and do work.[64][65] As a result, thermoeconomics is often discussed in the field of ecological economics, which itself is related to the fields of sustainability and sustainable development.\n Exergy analysis is performed in the field of industrial ecology to use energy more efficiently.[66] The term exergy, was coined by Zoran Rant in 1956, but the concept was developed by J. Willard Gibbs. In recent decades, utilization of exergy has spread outside of physics and engineering to the fields of industrial ecology, ecological economics, systems ecology, and energetics.\n An energy balance can be used to track energy through a system, and is a very useful tool for determining resource use and environmental impacts, using the First and Second laws of thermodynamics, to determine how much energy is needed at each point in a system, and in what form that energy is a cost in various environmental issues.[citation needed] The energy accounting system keeps track of energy in, energy out, and non-useful energy versus work done, and transformations within the system.[67]\n Scientists have written and speculated on different aspects of energy accounting.[68]\n Ecological economists agree that ecosystems produce enormous flows of goods and services to human beings, playing a key role in producing well-being. At the same time, there is intense debate about how and when to place values on these benefits.[69][70]\n A study was carried out by Costanza and colleagues[71] to determine the 'value' of the services provided by the environment. This was determined by averaging values obtained from a range of studies conducted in very specific context and then transferring these without regard to that context.  Dollar figures were averaged to a per hectare number for different types of ecosystem e.g. wetlands, oceans. A total was then produced which came out at 33 trillion US dollars (1997 values), more than twice the total GDP of the world at the time of the study.  This study was criticized by pre-ecological and even some environmental economists \u2013 for being inconsistent with assumptions of financial capital valuation \u2013 and ecological economists \u2013 for being inconsistent with an ecological economics focus on biological and physical indicators.[72]\n The whole idea of treating ecosystems as goods and services to be valued in monetary terms remains controversial.  A common objection[73][74][75] is that life is precious or priceless, but this demonstrably degrades to it being worthless within cost-benefit analysis and other standard economic methods.[76]  Reducing human bodies to financial values is a necessary part of mainstream economics and not always in the direct terms of insurance or wages. One example of this in practice is the value of a statistical life, which is a dollar value assigned to one life used to evaluate the costs of small changes in risk to life\u2013such as exposure to one pollutant.[77] Economics, in principle, assumes that conflict is reduced by agreeing on voluntary contractual relations and prices instead of simply fighting or coercing or tricking others into providing goods or services.  In doing so, a provider agrees to surrender time and take bodily risks and other (reputation, financial) risks.  Ecosystems are no different from other bodies economically except insofar as they are far less replaceable than typical labour or commodities.\n Despite these issues, many ecologists and conservation biologists are pursuing ecosystem valuation. Biodiversity measures in particular appear to be the most promising way to reconcile financial and ecological values, and there are many active efforts in this regard.[78] The growing field of biodiversity finance[79] began to emerge in 2008 in response to many specific proposals such as the Ecuadoran Yasuni proposal[80][81] or similar ones in the Congo.  US news outlets treated the stories as a \"threat\"[82] to \"drill a park\"[83] reflecting a previously dominant view that NGOs and governments had the primary responsibility to protect ecosystems.  However Peter Barnes and other commentators have recently argued that a guardianship/trustee/commons model is far more effective and takes the decisions out of the political realm.\n Commodification of other ecological relations as in carbon credit and direct payments to farmers to preserve ecosystem services are likewise examples that enable private parties to play more direct roles protecting biodiversity, but is also controversial in ecological economics.[84]  The United Nations Food and Agriculture Organization achieved near-universal agreement in 2008[85] that such payments directly valuing ecosystem preservation and encouraging permaculture were the only practical way out of a food crisis.  The holdouts were all English-speaking countries that export GMOs and promote \"free trade\" agreements that facilitate their own control of the world transport network: The US, UK, Canada and Australia.[86]\n Ecological economics is founded upon the view that the neoclassical economics (NCE) assumption that environmental and community costs and benefits are mutually canceling \"externalities\" is not warranted. Joan Martinez Alier,[87] for instance shows that the bulk of consumers are automatically excluded from having an impact upon the prices of commodities, as these consumers are future generations who have not been born yet.  The assumptions behind future discounting, which assume that future goods will be cheaper than present goods, has been criticized by David Pearce[88] and by the recent Stern Report (although the Stern report itself does employ discounting and has been criticized for this and other reasons by ecological economists such as Clive Spash).[89]\n Concerning these externalities, some like the eco-businessman Paul Hawken argue an orthodox economic line that the only reason why goods produced unsustainably are usually cheaper than goods produced sustainably is due to a hidden subsidy, paid by the non-monetized human environment, community or future generations.[90]  These arguments are developed further by Hawken, Amory and Hunter Lovins to promote their vision of an environmental capitalist utopia in Natural Capitalism: Creating the Next Industrial Revolution.[91]\n In contrast, ecological economists, like Joan Martinez-Alier, appeal to a different line of reasoning.[92]  Rather than assuming some (new) form of capitalism is the best way forward, an older ecological economic critique questions the very idea of internalizing externalities as providing some corrective to the current system. The work by Karl William Kapp explains why the concept of \"externality\" is a misnomer.[93] In fact the modern business enterprise operates on the basis of shifting costs onto others as normal practice to make profits.[94] Charles Eisenstein has argued that this method of privatising profits while socialising the costs through externalities, passing the costs to the community, to the natural environment or to future generations is inherently destructive.[95] As social ecological economist Clive Spash has noted, externality theory fallaciously assumes environmental and social problems are minor aberrations in an otherwise perfectly functioning efficient economic system.[96]  Internalizing the odd externality does nothing to address the structural systemic problem and fails to recognize the all pervasive nature of these supposed 'externalities'.\n Mathematical modeling is a powerful tool that is used in ecological economic analysis. Various approaches and techniques include:[97][98] evolutionary, input-output, neo-Austrian modeling, entropy and thermodynamic models,[99] multi-criteria, and agent-based modeling, the environmental Kuznets curve, and Stock-Flow consistent model frameworks. System dynamics and GIS are techniques applied, among other, to spatial dynamic landscape simulation modeling.[100][101]  The Matrix accounting methods of Christian Felber provide a more sophisticated method for identifying \"the common good\"[102]\n Ecological economics draws upon its work on resource allocation and strong sustainability to address monetary policy. Drawing upon a transdisciplinary literature, ecological economics roots its policy work in monetary theory and its goals of sustainable scale, just distribution, and efficient allocation.[103] Ecological economics' work on monetary theory and policy can be traced to Frederick Soddy's work on money. The field considers questions such as the growth imperative of interest-bearing debt, the nature of money, and alternative policy proposals such as alternative currencies and public banking.\n Assigning monetary value to natural resources such as biodiversity, and the emergent ecosystem services is often viewed as a key process in influencing economic practices, policy, and decision-making.[104][105] While this idea is becoming more and more accepted among ecologists and conservationist, some argue that it is inherently false.\n McCauley argues that ecological economics and the resulting ecosystem service based conservation can be harmful.[106] He describes four main problems with this approach:\n Firstly, it seems to be assumed that all ecosystem services are financially beneficial. This is undermined by a basic characteristic of ecosystems: they do not act specifically in favour of any single species. While certain services might be very useful to us, such as coastal protection from hurricanes by mangroves for example, others might cause financial or personal harm, such as wolves hunting cattle.[107] The complexity of Eco-systems makes it challenging to weigh up the value of a given species. Wolves play a critical role in regulating prey populations; the absence of such an apex predator in the Scottish Highlands has caused the over population of deer, preventing afforestation, which increases the risk of flooding and damage to property.\n Secondly, allocating monetary value to nature would make its conservation reliant on markets that fluctuate. This can lead to devaluation of services that were previously considered financially beneficial. Such is the case of the bees in a forest near former coffee plantations in Finca Santa Fe, Costa Rica. The pollination services were valued to over US$60,000 a year, but soon after the study, coffee prices dropped and the fields were replanted with pineapple.[108] Pineapple does not require bees to be pollinated, so the value of their service dropped to zero.\n Thirdly, conservation programmes for the sake of financial benefit underestimate human ingenuity to invent and replace ecosystem services by artificial means. McCauley argues that such proposals are deemed to have a short lifespan as the history of technology is about how Humanity developed artificial alternatives to nature's services and with time passing the cost of such services tend to decrease. This would also lead to the devaluation of ecosystem services.\n Lastly, it should not be assumed that conserving ecosystems is always financially beneficial as opposed to alteration. In the case of the introduction of the Nile perch to Lake Victoria, the ecological consequence was decimation of native fauna. However, this same event is praised by the local communities as they gain significant financial benefits from trading the fish.\n McCauley argues that, for these reasons, trying to convince decision-makers to conserve nature for monetary reasons is not the path to be followed, and instead appealing to morality is the ultimate way to campaign for the protection of nature.\n"
    },
    {
        "title": "International Conference on Population and Development",
        "url": "https://en.wikipedia.org/wiki/International_Conference_on_Population_and_Development",
        "content": "\n The United Nations coordinated an International Conference on Population and Development (ICPD) in Cairo, Egypt, on 5\u201313 September 1994. Its resulting Programme of Action is the steering document for the United Nations Population Fund (UNFPA).\n Some 20,000 delegates from various governments, UN agencies, NGOs, and the media gathered for a discussion of a variety of population issues, including immigration, infant mortality, birth control, family planning, the education of women, and protection for women from unsafe abortion services.\n The first World Population Conference, organised by the League of Nations and Margaret Sanger, had been held at the Salle Central in Geneva, Switzerland from 29 August to 3 September 1927.[citation needed]\n The first World Population Conference sponsored by the United Nations was held in 1954 in Rome, a second in 1965 in Belgrade, a third in 1974 in Bucharest, a fourth in 1984 in Mexico City.\n The conference received considerable media attention due to disputes regarding the assertion of reproductive rights. The Holy See and several predominantly Islamic nations were staunch critics, and U.S. President Bill Clinton received considerable criticism from conservatives for his participation, considering the fact that President Ronald Reagan did not attend or fund the previous conference held in Mexico City in 1984. The official spokesman for the Holy See was archbishop Renato Martino.[citation needed]\n During and after the ICPD, some interested parties attempted to interpret the term 'reproductive health' in the sense that it implies abortion as a means of family planning or, indeed, a right to abortion. These interpretations, however, do not reflect the consensus reached at the Conference.\nFor the European Union, where legislation on abortion is less restrictive than elsewhere, the Council Presidency has clearly stated that the council's commitment to promoting 'reproductive health' did not include the promotion of abortion.[1]\n Likewise, the European Commission, in response to a question from a Member of the European Parliament, clarified:\n \"The term 'reproductive health' was defined by the United Nations (UN) in 1994 at the Cairo International Conference on Population and Development. All Member States of the Union endorsed the Programme of Action adopted at Cairo. The Union has never adopted an alternative definition of 'reproductive health' to that given in the Programme of Action, which makes no reference to abortion.\"[2]\n With regard to the US, only a few days prior to the Cairo Conference, the head of the US delegation, Vice President Al Gore, had stated for the record:\n \"Let us get a false issue off the table: the US does not seek to establish a new international right to abortion, and we do not believe that abortion should be encouraged as a method of family planning.\"[3]\n Some years later, the position of the US Administration in this debate was reconfirmed by US Ambassador to the UN, Ellen Sauerbrey, when she stated at a meeting of the UN Commission on the Status of Women that:\n\"nongovernmental organizations are attempting to assert that Beijing in some way creates or contributes to the creation of an internationally recognized fundamental right to abortion\".[4] She added: \"There is no fundamental right to abortion. And yet it keeps coming up largely driven by NGOs trying to hijack the term and trying to make it into a definition\".[5]\n According to the official ICPD release, the conference delegates achieved consensus on the following four qualitative and quantitative goals:[6]\n The 2019 Nairobi Summit on ICPD25 was held in Nairobi in November.[8] It marks the 25th year of the International Conference on Population and Development (ICPD). The government of Kenya, Denmark and UNFPA are co-convening the summit. It is a platform for all interested in the pursuit of sexual and reproductive health and rights (SRHR) which includes duty bearers, civil society organisations (CSOs), private sector organisation, women group, youth networks, faith base organisation etc. to discuss and agree on the actions to complete the ICPD Programme of Action.[9][10]\n"
    },
    {
        "title": "Association for Computing Machinery",
        "url": "https://en.wikipedia.org/wiki/SIGSIM",
        "content": "\n The Association for Computing Machinery (ACM) is a US-based international learned society for computing. It was founded in 1947 and is the world's largest scientific and educational computing society.[1] The ACM is a non-profit professional membership group,[2] reporting nearly 110,000 student and professional members as of 2022[update]. Its headquarters are in New York City.\n The ACM is an umbrella organization for academic and scholarly interests in computer science (informatics). Its motto is \"Advancing Computing as a Science & Profession\".\n In 1947, a notice was sent to various people:[3][4]\n On January 10, 1947, at the Symposium on Large-Scale Digital Calculating Machinery at the Harvard computation Laboratory, Professor Samuel H. Caldwell of Massachusetts Institute of Technology spoke of the need for an association of those interested in computing machinery, and of the need for communication between them.\n[...]\nAfter making some inquiries during May and June, we believe there is ample interest to start an informal association of many of those interested in the new machinery for computing and reasoning. Since there has to be a beginning, we are acting as a temporary committee to start such an association:\n\n The committee (except for Curtiss) had gained experience with computers during World War II: Berkeley, Campbell, and Goheen helped build Harvard Mark I under Howard H. Aiken, Mauchly and Sharpless were involved in building ENIAC, Tompkins had used \"the secret Navy code-breaking machines\", and Taylor had worked on Bush's Differential analyzers.[4]\n The ACM was then founded in 1947 under the name Eastern Association for Computing Machinery, which was changed the following year to the Association for Computing Machinery.[5][6][7] The ACM History Committee since 2016 has published the A.M.Turing Oral History project, the ACM Key Award Winners Video Series, and the India Industry Leaders Video project.[8]\n ACM is organized into over 180 local professional chapters[9] and 38 Special Interest Groups (SIGs),[10] through which it conducts most of its activities. Additionally, there are over 680 student chapters.[9] The first student chapter was founded in 1961 at the University of Louisiana at Lafayette.[11][12]\n Many of the SIGs, such as SIGGRAPH, SIGDA, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.[13]\n ACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.[14]\n ACM publishes over 50 journals[15] including the prestigious[16] Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:\n Although Communications no longer publishes primary research and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.\n ACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. ACM also offers insurance, online courses, and other services to its members.\n In 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN\u00a00897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.\n The ACM Portal is an online service of the ACM.[19] Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.[20]\n The ACM Digital Library was launched in October 1997.[21] It is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries.[19] The ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.\n ACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.[22]\n ACM was a \"green\" publisher before the term was invented.[23] Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record.\n All metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription. In addition, starting on April 7, 2022, ACM made its publications from 1951 to 2000 open access through the Digital Library in celebration of the 75th anniversary of the organization's founding.[24]\n In 2020, ACM launched a major push to become a fully open access publisher by 2026. ACM restructured its pricing for the ACM Digital Library on the basis of publishing activity by affiliated lead authors in ACM's journals, magazines, and conference proceedings. Under this model, termed \"ACM Open,\" institutions pay set fees for full access to ACM Digital Library contents as well as unlimited open access publishing by their affiliated authors. Authors not affiliated with a participating institution will be expected to pay an article processing charge.[25][26] As of May 2024, ACM reported that more than 1,340 institutions worldwide had signed on for ACM Open, putting ACM at just over halfway to meeting its target of 2,500 participating institutions by 2026.[27]\n In addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and \"demonstrated performance that sets them apart from their peers\".[28]\n The number of Fellows, Distinguished Members, and Senior Members cannot exceed 1%, 10%, and 25% of the total number of professional members, respectively.[29]\n The ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 \"to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM.\" There are 1,310 Fellows as of 2020[update][30] out of about 100,000 members.\n In 2006, ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and \"have made a significant impact on the computing field\". In 2006 when the Distinguished Members first came out, one of the three levels was called \"Distinguished Member\" and was changed about two years later to \"Distinguished Educator\". Those who already had the Distinguished Member title had their titles changed to one of the other three titles.\n List of Distinguished Members of the Association for Computing Machinery [31]\n Also in 2006, ACM began recognizing Senior Members. According to the ACM, \"The Senior Members Grade recognizes those ACM members with at least 10 years of professional experience and 5 years of continuous Professional Membership who have demonstrated performance through technical leadership, and technical or professional contributions\".[32] Senior membership also requires 3 letters of reference\n While not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'.[33] The distinguished speakers program (DSP) has been in existence for over 20 years and serves as an outreach program that brings renowned experts from Academia, Industry and Government to present on the topic of their expertise.[34]  The DSP is overseen by a committee [35]\n ACM has three kinds of chapters: Special Interest Groups,[36] Professional Chapters, and Student Chapters.[37]\n As of 2022[update], ACM has professional & SIG Chapters in 56 countries.[38]\n As of 2022[update], there exist ACM student chapters in 41 countries.[39]\n ACM and its Special Interest Groups (SIGs) sponsors numerous conferences worldwide. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, SIGGRAPH 2007 attracted about 30000 attendees, while CIKM 2005 and RecSys 2022 had paper acceptance rates of only accepted 15% and 17% respectively.[41]\n The ACM is a co\u2013presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.[47]\n Some conferences are hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM.[48] In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.\n For additional non-ACM conferences, see this list of computer science conferences.\n The ACM presents or co\u2013presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.[49][50][51]\n Over 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.[55]\n The President of ACM for 2022\u20132024 is Yannis Ioannidis, Professor at the National and Kapodistrian University of Athens.[56] He is successor of Gabriele Kotsis (2020\u20132022), Professor at the Johannes Kepler University Linz; Cherri M. Pancake (2018\u20132020), professor emeritus at Oregon State University and Director of the Northwest Alliance for Computational Science and Engineering (NACSE); Vicki L. Hanson (2016\u20132018), Distinguished Professor at the Rochester Institute of Technology and visiting professor at the University of Dundee; Alexander L. Wolf (2014\u20132016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012\u20132014), American computer scientist and Internet pioneer; Alain Chesnais (2010\u20132012); and Dame Wendy Hall of the University of Southampton, UK (2008\u20132010).[57]\n ACM is led by a council consisting of the president, vice-president, treasurer, past president, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members-At-Large. This institution is often referred to simply as \"Council\" in Communications of the ACM.\n ACM has numerous boards, committees, and task forces which run the organization:[58]\n ACM-W,[59] the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM\u2013W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively.  ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and Committee on the Status of Women in Computing Research (CRA-W).\nThe ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science.[60] This program began in 2006. Speakers are nominated by SIG officers.[61]\n ACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards.[62] ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE.[63] Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS.[64] They occasionally cooperate on projects like developing computing curricula.[65]\n ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).[66]\n In December 2019, the ACM co-signed a letter with over one hundred other publishers to President Donald Trump saying that an open access mandate would increase costs to taxpayers or researchers and hurt intellectual property. This was in response to rumors that he was considering issuing an executive order that would require federally funded research be made freely available online immediately after being published. It is unclear how these rumors started.[67] Many ACM members opposed the letter, leading ACM to issue a statement clarifying that they remained committed to open access,[68] and they wanted to see communication with stakeholders about the potential mandate. The statement did not significantly assuage criticism from ACM members.[69]\n The SoCG conference, while originally an ACM conference, parted ways with ACM in 2014[70] because of problems when organizing conferences abroad.[71]\n"
    },
    {
        "title": "Coup d'\u00e9tat",
        "url": "https://en.wikipedia.org/wiki/Coup_d%27%C3%A9tat",
        "content": "\n A coup d'\u00e9tat (/\u02ccku\u02d0de\u026a\u02c8t\u0251\u02d0/ \u24d8; French: [ku deta] \u24d8; lit.\u2009'stroke of state'),[1] or simply a coup, is typically an illegal and overt attempt by a military organization or other government elites to unseat an incumbent leadership.[2][3] A self-coup is when a leader, having come to power through legal means, tries to stay in power through illegal means.[3]\n By one estimate, there were 457 coup attempts from 1950 to 2010, half of which were successful.[2] Most coup attempts occurred in the mid-1960s, but there were also large numbers of coup attempts in the mid-1970s and the early 1990s.[2] Coups occurring in the post-Cold War period have been more likely to result in democratic systems than Cold War coups,[4][5][6] though coups still mostly perpetuate authoritarianism.[7]\n Many factors may lead to the occurrence of a coup, as well as determine the success or failure of a coup. Once a coup is underway, coup success is driven by coup-makers' ability to get others to believe that the coup attempt will be successful.[8] The number of successful coups has decreased over time.[2] Failed coups in authoritarian systems are likely to strengthen the power of the authoritarian ruler.[9][10] The cumulative number of coups is a strong predictor of future coups, a phenomenon referred to as the \"coup trap\".[11][12][13][14]\n In what is referred to as \"coup-proofing\", regimes create structures that make it hard for any small group to seize power. These coup-proofing strategies may include the strategic placing of family, ethnic, and religious groups in the military and the fragmenting of military and security agencies.[15] However, coup-proofing reduces military effectiveness as loyalty is prioritized over experience when filling key positions within the military.[16][17][18][19]\n The term comes from French coup d'\u00c9tat, literally meaning a 'stroke of state' or 'blow of state'.[20][21][22] In French, the word \u00c9tat (French: [eta]) is capitalized when it denotes a sovereign political entity.[23]\n Although the concept of a coup d'\u00e9tat has featured in politics since antiquity, the phrase is of relatively recent coinage.[24] It did not appear within an English text before the 19th century except when used in the translation of a French source, there being no simple phrase in English to convey the contextualized idea of a 'knockout blow to the existing administration within a state'.\n One early use within text translated from French was in 1785 in a printed translation of a letter from a French merchant, commenting on an arbitrary decree, or arr\u00eat, issued by the French king restricting the import of British wool.[25] What may be its first published use within a text composed in English is an editor's note in the London Morning Chronicle,1804, reporting the arrest by Napoleon in France, of Moreau, Berthier, Mass\u00e9na, and Bernadotte: \"There was a report in circulation yesterday of a sort of coup d'\u00e9tat having taken place in France, in consequence of some formidable conspiracy against the existing government.\"\n In the British press, the phrase came to be used to describe the various murders by Napoleon's alleged secret police, the Gens d'Armes d'Elite, who executed the Duke of Enghien: \"the actors in torture, the distributors of the poisoning draughts, and the secret executioners of those unfortunate individuals or families, whom Bonaparte's measures of safety require to remove. In what revolutionary tyrants call grand[s] coups d'\u00e9tat, as butchering, or poisoning, or drowning, en masse, they are exclusively employed.\"[26]\n A self-coup, also called an autocoup (from Spanish  autogolpe) or coup from the top, is a form of coup d'\u00e9tat in which a political leader, having come to power through legal means, stays in power through illegal means through the actions of themselves and/or their supporters.[27] The leader may dissolve or render powerless the national legislature and unlawfully assume extraordinary powers. Other measures may include annulling the nation's constitution, suspending civil courts, and having the head of government assume dictatorial powers.[28][29]\n A soft coup, sometimes referred to as a silent coup or a bloodless coup, is an illegal overthrow of a government, but unlike a regular coup d'\u00e9tat it is achieved without the use of force or violence.[31]\n A palace coup or palace revolution is a coup in which one faction within the ruling group displaces another faction within a ruling group.[32] Along with popular protests, palace coups are a major threat to dictators.[33] The Harem conspiracy of the 12th century BC was one of the earliest attempts. Palace coups were common in Imperial China.[34] They have also occurred among the Habsburg dynasty in Austria, the Al-Thani dynasty in Qatar,[35] and in Haiti in the 19th to early 20th centuries.[36] The majority of Russian tsars between 1725 and 1801 were either overthrown or usurped power in palace coups.[37]\n The term putsch ([p\u028at\u0283], from Swiss German for 'knock'), denotes the political-military actions of an unsuccessful minority reactionary coup.[38][39] The term was initially coined for the Z\u00fcriputsch of 6 September 1839 in Switzerland. It was also used for attempted coups in Weimar Germany, such as the 1920 Kapp Putsch, K\u00fcstrin Putsch, and Adolf Hitler's 1923 Beer Hall Putsch.[40]\n The 1934 Night of the Long Knives was Hitler's purge to eliminate opponents, particularly the paramilitary faction led by Ernst R\u00f6hm, but Nazi propaganda justified it as preventing a supposed putsch planned or attempted by R\u00f6hm. The Nazi term R\u00f6hm-Putsch is still used by Germans to describe the event, often with quotation marks as the 'so-called R\u00f6hm Putsch'.[41]\n The 1961 Algiers putsch and the 1991 August Putsch also use the term.\n The 2023 Wagner Group rebellion has also been described as a putsch.[42][43]\n Pronunciamiento ('pronouncement') is a term of Spanish origin for a type of coup d'\u00e9tat. Specifically the pronunciamiento is the formal declaration deposing the previous government and justifying the installation of the new government by the golpe de estado. One author distinguishes a coup, in which a military or political faction takes power for itself, from a pronunciamiento, in which the military deposes the existing government and hands over power to a new, ostensibly civilian government.[44]\n A \"barracks revolt\" or cuartelazo is another type of military revolt, from the Spanish term cuartel ('quarter' or 'barracks'), in which the mutiny of specific military garrisons sparks a larger military revolt against the government.[45]\n Other types of actual or attempted seizures of power are sometimes called \"coups with adjectives\". The appropriate term can be subjective and carries normative, analytical, and political implications.[31]\n While a coup is usually a conspiracy of a small group, a revolution or rebellion is usually started spontaneously by larger groups of uncoordinated people.[49] The distinction between a revolution and a coup is not always clear. Sometimes, a coup is labelled as a revolution by its plotters to feign democratic legitimacy.[50][51]\n According to Clayton Thyne and Jonathan Powell's coup data set, there were 457 coup attempts from 1950 to 2010, of which 227 (49.7%) were successful and 230 (50.3%) were unsuccessful.[2] They find that coups have \"been most common in Africa and the Americas (36.5% and 31.9%, respectively). Asia and the Middle East have experienced 13.1% and 15.8% of total global coups, respectively. Europe has experienced by far the fewest coup attempts: 2.6%.\"[2] Most coup attempts occurred in the mid-1960s, but there were also large numbers of coup attempts in the mid-1970s and the early 1990s.[2] From 1950 to 2010, a majority of coups failed in the Middle East and Latin America. They had a somewhat higher chance of success in Africa and Asia.[7] Numbers of successful coups have decreased over time.[2]\n A number of political science datasets document coup attempts around the world and over time, generally starting in the post-World War II period. Major examples include the Global Instances of Coups dataset, the Coups & Political Instability dataset by the Center of Systemic Peace, the Coup d'etat Project by the Cline Center, the Colpus coup dataset, and the Coups and Agency Mechanism dataset. A 2023 study argued that major coup datasets tend to over-rely on international news sources to gather their information, potentially biasing the types of events included.[52] Its findings show that while such a strategy is sufficient for gathering information on successful and failed coups, attempts to gather data on coup plots and rumors require a greater consultation of regional and local-specific sources.\n A 2003 review of the academic literature found that the following factors influenced coups:\n The literature review in a 2016 study includes mentions of ethnic factionalism, supportive foreign governments, leader inexperience, slow growth, commodity price shocks, and poverty.[54]\n Coups have been found to appear in environments that are heavily influenced by military powers. Multiple of the above factors are connected to military culture and power dynamics. These factors can be divided into multiple categories, with two of these categories being a threat to military interests and support for military interests. If interests go in either direction, the military will find itself either capitalizing off that power or attempting to gain it back.\n Oftentimes, military spending is an indicator of the likelihood of a coup taking place. Nordvik found that about 75% of coups that took place in many different countries rooted from military spending and oil windfalls.[53]\n The accumulation of previous coups is a strong predictor of future coups,[11][12] a phenomenon called the coup trap.[13][14] A 2014 study of 18 Latin American countries found that the establishment of open political competition helps bring countries out of the coup trap and reduces cycles of political instability.[14]\n Hybrid regimes are more vulnerable to coups than very authoritarian states or democratic states.[55] A 2021 study found that democratic regimes were not substantially more likely to experience coups.[56] A 2015 study finds that terrorism is strongly associated with re-shuffling coups.[57] A 2016 study finds that there is an ethnic component to coups: \"When leaders attempt to build ethnic armies, or dismantle those created by their predecessors, they provoke violent resistance from military officers.\"[58] Another 2016 study shows that protests increase the risk of coups, presumably because they ease coordination obstacles among coup plotters and make international actors less likely to punish coup leaders.[59] A third 2016 study finds that coups become more likely in the wake of elections in autocracies when the results reveal electoral weakness for the incumbent autocrat.[60] A fourth 2016 study finds that inequality between social classes increases the likelihood of coups.[61] A fifth 2016 study finds no evidence that coups are contagious; one coup in a region does not make other coups in the region likely to follow.[62] One study found that coups are more likely to occur in states with small populations, as there are smaller coordination problems for coup-plotters.[63]\n In autocracies, the frequency of coups seems to be affected by the succession rules in place, with monarchies with a fixed succession rule being much less plagued by instability than less institutionalized autocracies.[64][65][66]\n A 2014 study of 18 Latin American countries in the 20th-century study found the legislative powers of the presidency does not influence coup frequency.[14]\n A 2019 study found that when a country's politics is polarized and electoral competition is low, civilian-recruited coups become more likely.[67]\n A 2023 study found that civilian elites are more likely to be associated with instigating military coups while civilians embedded in social networks are more likely to be associated with consolidating military coups.[68]\n \nA 2017 study found that autocratic leaders whose states were involved in international rivalries over disputed territory were more likely to be overthrown in a coup. The authors of the study provide the following logic for why this is: Autocratic incumbents invested in spatial rivalries need to strengthen the military in order to compete with a foreign adversary. The imperative of developing a strong army puts dictators in a paradoxical situation: to compete with a rival state, they must empower the very agency\u2014the military\u2014that is most likely to threaten their own survival in office.[69] However, two 2016 studies found that leaders who were involved in militarized confrontations and conflicts were less likely to face a coup.[70][71]\n A 2019 study found that states that had recently signed civil war peace agreements were much more likely to experience coups, in particular when those agreements contained provisions that jeopardized the interests of the military.[72]\n Research suggests that protests spur coups, as they help elites within the state apparatus to coordinate coups.[73]\n A 2019 study found that regional rebellions made coups by the military more likely.[74]\n A 2018 study found that \"oil price shocks are seen to promote coups in onshore-intensive oil countries, while preventing them in offshore-intensive oil countries\".[75] The study argues that states which have onshore oil wealth tend to build up their military to protect the oil, whereas states do not do that for offshore oil wealth.[75]\n A 2020 study found that elections had a two-sided impact on coup attempts, depending on the state of the economy. During periods of economic expansion, elections reduced the likelihood of coup attempts, whereas elections during economic crises increased the likelihood of coup attempts.[76]\n A 2021 study found that oil wealthy nations see a pronounced risk of coup attempts but these coups are unlikely to succeed.[77]\n A 2014 study of 18 Latin American countries in the 20th century study found that coup frequency does not vary with development levels, economic inequality, or the rate of economic growth.[14][contradictory]\n In what is referred to as \"coup-proofing\", regimes create structures that make it hard for any small group to seize power. These coup-proofing strategies may include the strategic placing of family, ethnic, and religious groups in the military; creation of an armed force parallel to the regular military; and development of multiple internal security agencies with overlapping jurisdiction that constantly monitor one another.[15] It may also involve frequent salary hikes and promotions for members of the military,[78] and the deliberate use of diverse bureaucrats.[79] Research shows that some coup-proofing strategies reduce the risk of coups occurring.[80][81] However, coup-proofing reduces military effectiveness,[82][16][83][17][84][85][18] and limits the rents that an incumbent can extract.[86] One reason why authoritarian governments tend to have incompetent militaries is that authoritarian regimes fear that their military will stage a coup or allow a domestic uprising to proceed uninterrupted \u2013 as a consequence, authoritarian rulers have incentives to place incompetent loyalists in key positions in the military.[19]\n A 2016 study shows that the implementation of succession rules reduce the occurrence of coup attempts.[87] Succession rules are believed to hamper coordination efforts among coup plotters by assuaging elites who have more to gain by patience than by plotting.[87]\n According to political scientists Curtis Bell and Jonathan Powell, coup attempts in neighbouring countries lead to greater coup-proofing and coup-related repression in a region.[88] A 2017 study finds that countries' coup-proofing strategies are heavily influenced by other countries with similar histories.[89] Coup-proofing is more likely in former French colonies.[90]\n A 2018 study in the Journal of Peace Research found that leaders who survive coup attempts and respond by purging known and potential rivals are likely to have longer tenures as leaders.[91] A 2019 study in Conflict Management and Peace Science found that personalist dictatorships are more likely to take coup-proofing measures than other authoritarian regimes; the authors argue that this is because \"personalists are characterized by weak institutions and narrow support bases, a lack of unifying ideologies and informal links to the ruler\".[92]\n In their 2022 book Revolution and Dictatorship: The Violent Origins of Durable Authoritarianism, political scientists Steven Levitsky and Lucan Way found that political-military fusion, where the ruling party is highly interlinked with the military and created the administrative structures of the military from its inception, is extremely effective at preventing military coups. For example, the People's Liberation Army was created by the Chinese Communist Party during the Chinese Civil War, and never instigated a military coup even after large-scale policy failures (i.e. the Great Leap Forward) or the extreme political instability of the Cultural Revolution.[93]\n Successful coups are one method of regime change that thwarts the peaceful transition of power.[94][95]\nA 2016 study categorizes four possible outcomes to coups in dictatorships:[5]\n The study found that about half of all coups in dictatorships\u2014both during and after the Cold War\u2014install new autocratic regimes.[5] New dictatorships launched by coups engage in higher levels of repression in the year after the coup than existed in the year before the coup.[5] One-third of coups in dictatorships during the Cold War and 10% of later ones reshuffled the regime leadership.[5] Democracies were installed in the wake of 12% of Cold War coups in dictatorships and 40% of post-Cold War ones.[5]\n Coups occurring in the post-Cold War period have been more likely to result in democratic systems than Cold War coups,[4][5][6] though coups still mostly perpetuate authoritarianism.[7] Coups that occur during civil wars shorten the war's duration.[97]\n Since the end of the Cold War, coups have become rarer, and more likely to be followed by democratization. Coups still often simply replace one autocracy with another one (with the new autocratic regime usually more repressive, in an attempt to prevent another coup) or have no effect on regime type.[4][5][99][100][101][102][98]\n As of 2017[update], there was debate about whether coups in autocracies should now be considered to promote democratization, on average, or if countries' chances of democratization are still unchanged or worsened by coups (since democratization can take place without a coup). One reason for the increase in the chance of democratization is that a higher proportion of coups (half of post-Cold-War coups) now take place in democracies (a higher percentage of countries are also now democracies). Democratic countries often rebound from coups quickly, restoring democracy, but coups in a democracy are a sign of poor political health, and increase the risk of future coups and loss of democracy. The dataset is small, so statistical significance varies depending on the model used, as of 2017[update]; debate will end if data on more coups makes the pattern clear.[4][5][99][100][101][102][98]\n The post-Cold-War increase in the chances of post-coup democratization may partly be due to the incentives created by international pressure and financing.[5][4] US law, for instance, automatically cuts off all aid to a country if there is a military coup.[103] According to a 2020 study, \"external reactions to coups play important roles in whether coup leaders move toward authoritarianism or democratic governance. When supported by external democratic actors, coup leaders have an incentive to push for elections to retain external support and consolidate domestic legitimacy. When condemned, coup leaders are apt to trend toward authoritarianism to assure their survival.[104]\n But coup conspirators also increasingly say that they chose a coup to save their country from the autocratic incumbents. Successful conspirators may hold free and fair elections simply because they think it is a good idea.[5] A desire for economic growth and legitimacy have also been cited as motivations for democratization.[99]\n Legal scholar Ilya Somin believes that a coup to forcibly overthrow a democratic government might sometimes be justified. Commenting on the 2016 Turkish coup d'\u00e9tat attempt, Somin opined,\n There should be a strong presumption against forcibly removing a democratic regime. But that presumption might be overcome if the government in question poses a grave threat to human rights, or is likely to destroy democracy itself by shutting down future political competition.[105] It has been argued that failed coups might motivate a regime to reform and reduce repression.[99] Such reforms are not obvious in the data, as of 2017[update]. Coups that fail, or merely shuffle the leadership without changing the system, generally don't change the amount of repression (measured in government-sanctioned and pro-government killings).[5][101]\n 2016 research suggests that increased repression and violence typically follow both successful and unsuccessful coup attempts.[98] However, some tentative analysis by political scientist Jay Ulfelder finds no clear pattern of deterioration in human rights practices in wake of failed coups in post-Cold War era.[106][better\u00a0source\u00a0needed] According to a 2019 study, coup attempts lead to a reduction in physical integrity rights.[107]\n Coups that lead to democratization unsurprisingly reduce repression, and coups that bring in a new autocratic regime increase it. Post-Cold-War, post-coup autocracies seem to have become more repressive and post-coup democracies less repressive; the gap between them is therefore larger than it was during the Cold War.[5][101]\n Averaging across democratic and non-democratic outcomes, most coups seem to tend to increase state repression, even coups against autocrats who were already quite repressive. The time interval in which violence is measured matters. The months after a bloodless coup can be bloody. The small sample size and high variability means that this conclusion again does not reach statistical significance, and a firm conclusion cannot be drawn.[108][5][101]\n According to Naunihal Singh, author of Seizing Power: The Strategic Logic of Military Coups (2014), it is \"fairly rare\" for the incumbent government to violently purge the army after a failed coup. If it starts the mass killing of elements of the army, including officers who were not involved in the coup, this may trigger a \"counter-coup\" by soldiers who are afraid they will be next. To prevent such a desperate counter-coup that may be more successful than the initial attempt, governments usually resort to firing prominent officers and replacing them with loyalists instead.[109]\n Notable counter-coups include the Ottoman countercoup of 1909, the 1960 Laotian counter-coup, the Indonesian mass killings of 1965\u201366, the 1966 Nigerian counter-coup, the 1967 Greek counter-coup, 1971 Sudanese counter-coup, and the Coup d'\u00e9tat of December Twelfth in South Korea.\n A 2017 study finds that the use of state broadcasting by the putschist regime after Mali's 2012 coup did not elevate explicit approval for the regime.[110]\n The international community tends to react adversely to coups by reducing aid and imposing sanctions. A 2015 study finds that \"coups against democracies, coups after the Cold War, and coups in states heavily integrated into the international community are all more likely to elicit global reaction.\"[111] Another 2015 study shows that coups are the strongest predictor for the imposition of democratic sanctions.[112] A third 2015 study finds that Western states react strongest against coups of possible democratic and human rights abuses.[112] A 2016 study shows that the international donor community in the post-Cold War period penalizes coups by reducing foreign aid.[113] The US has been inconsistent in applying aid sanctions against coups both during the Cold War and post-Cold War periods, a likely consequence of its geopolitical interests.[113]\n Organizations such as the African Union (AU) and the Organization of American States (OAS) have adopted anti-coup frameworks. Through the threat of sanctions, the organizations actively try to curb coups. A 2016 study finds that the AU has played a meaningful role in reducing African coups.[114]\n A 2017 study found that negative international responses, especially from powerful actors, have a significant effect in shortening the duration of regimes created in coups.[115]\n According to a 2020 study, coups increase the cost of borrowing and increase the likelihood of sovereign default.[116]\n Leaders are arranged in chronological order by the dates they assumed power, and categorized by the continents their countries are in.\n"
    },
    {
        "title": "Kingdom of Iraq",
        "url": "https://en.wikipedia.org/wiki/Kingdom_of_Iraq",
        "content": "The Hashemite Kingdom of Iraq (Arabic: \u0627\u0644\u0645\u0645\u0644\u0643\u0629 \u0627\u0644\u0639\u0631\u0627\u0642\u064a\u0629 \u0627\u0644\u0647\u0627\u0634\u0645\u064a\u0629, romanized:\u00a0al-Mamlakah al-\u02bfIr\u0101qiyyah \u02beal-H\u0101shimiyyah, lit.\u2009'Iraqi Hashemite Kingdom') was a state located in the Middle East from 1932 to 1958.\n It was founded on 23 August 1921 as the Kingdom of Iraq, following the defeat of the Ottoman Empire in the Mesopotamian campaign of the First World War. Although a League of Nations mandate was awarded to the United Kingdom in 1920, the 1920 Iraqi revolt resulted in the scrapping of the original mandate plan in favour of a formally sovereign Iraqi kingdom, but one that was under effective British administration. The plan was formally established by the Anglo-Iraqi Treaty.\n The role of the United Kingdom in the formal administration of the Kingdom of Iraq was ended in 1932,[1] following the Anglo-Iraqi Treaty (1930). Now officially a fully independent kingdom, officially named the Hashemite Kingdom of Iraq, it underwent a period of turbulence under its Hashemite rulers throughout its entire existence. Establishment of Sunni religious domination in Iraq was followed by Assyrian, Yazidi and Shi'a unrests, which were all brutally suppressed.[citation needed] In 1936, the first military coup took place in the Hashemite Kingdom of Iraq, as Bakr Sidqi succeeded in replacing the acting Prime Minister with his associate. Multiple coups followed in a period of political instability, peaking in 1941.\n During the Second World War, the Iraqi government of the Prince-Regent, Prince 'Abd al-Ilah, was overthrown in 1941 by the Golden Square officers, headed by Rashid Ali. The short-lived pro-Nazi government of Iraq was defeated in May 1941 by the Allied forces in the Anglo-Iraqi War. Iraq was later used as a base for Allied attacks on the Vichy-French-held Mandate of Syria and support for the Anglo-Soviet invasion of Iran. At the same time, the Kurdish leader Mustafa Barzani led a rebellion against the central government in Baghdad. After the failure of the uprising, Barzani and his followers fled to the Soviet Union.\n In 1945, during the final stages of World War II, Iraq joined the United Nations and became a founding member of the Arab League. In 1948, massive violent protests, known as the Al-Wathbah uprising, broke out across Baghdad as a popular demand against the government treaty with the British, and with support from the communists. More protests continued in the spring, but were interrupted in May, when martial law was imposed after Iraq entered the 1948 Arab\u2013Israeli War along with other members of the Arab League.\n In February 1958, King Hussein of Jordan and Prince `Abd al-Il\u0101h proposed a union of H\u0101shimite monarchies to counter the recently formed Egyptian\u2013Syrian union. The resulting Arab Federation, formed on 14 February 1958, was short-lived and ended the same year with a military coup led by Abdul-Karim Qasim deposing the monarchy.\n The territory of Iraq was under Ottoman dominance until the end of the First World War, becoming an occupied territory under the British military from 1918. In order to transform the region to civil rule, Mandatory Mesopotamia was proposed as a League of Nations Class A mandate under Article 22 and entrusted to the United Kingdom of Great Britain and Ireland, when the former territories of that Ottoman Empire were divided in August 1920 by the Treaty of S\u00e8vres. However, the 1920 Iraqi revolt resulted in the scrapping of the original mandate plan. Instead, the Kingdom of Iraq was recognised as a sovereign country under King Faisal I of Iraq. Not withstanding the formal sovereignty of the Iraqi king, a treaty of alliance was concluded between the Kingdom of Iraq and the United Kingdom in 1922 called the Anglo-Iraqi Treaty. It provided the United Kingdom with a role in the administration and governance of Iraq. King Faisal had previously been proclaimed King of Syria by a Syrian National Congress in Damascus in March 1920 but was ejected by the French in July of the same year. The British RAF retained certain military control. In this manner, Iraq remained under de facto British administration until 1932.\n Under King Faisal of Iraq, the civil government of postwar Iraq was led by the High Commissioner, Sir Percy Cox, and his deputy, Colonel Arnold Wilson. British reprisals after the murder of a British officer in Najaf failed to restore order. British administration had yet to be established in the mountains of north Iraq. The most striking problem facing the British was the growing anger of the nationalists in the Iraqi kingdom.\n With the signing in Baghdad of the Anglo-Iraqi Treaty on 30 June 1930 and the settling of the Mosul Question, Iraqi politics took on a new dynamic. The treaty came into force on 3 October 1932, when the Kingdom of Iraq officially became fully independent as the Hashemite Kingdom of Iraq. The emerging class of Sunni and Shia landowning tribal sheikhs vied for positions of power with wealthy and prestigious urban-based Sunni families and with Ottoman-trained army officers and bureaucrats. Because Iraq's newly established political institutions were the creation of a foreign power, and because the concept of democratic government had no precedent in Iraqi history, the politicians in Baghdad lacked legitimacy and never developed deeply rooted constituencies. Thus, despite a constitution and an elected assembly, Iraqi politics was more a shifting alliance of important personalities and cliques than a democracy in the Western sense. The absence of broadly based political institutions inhibited the early nationalist movement's ability to make deep inroads into Iraq's diverse social structure.\n The new Anglo-Iraqi Treaty was signed in June 1930. It provided for a \"close alliance,\" for \"full and frank consultations between the two countries in all matters of foreign policy,\" and for mutual assistance in case of war. Iraq granted the British the use of air bases near Basra and at Al Habbaniyah and the right to move troops across the country. The treaty, of twenty-five years' duration, was to come into force upon Iraq's admission to the League of Nations. This occurred on October 3, 1932.\n In 1932, the Hashemite Kingdom of Iraq was granted full independence under King Faisal I. However, the British retained military bases in the country. Iraq was granted official independence on 3 October 1932 in accordance with an agreement signed by the United Kingdom in June 1930, whereby the United Kingdom would end its effective mandate on the condition that the Iraqi government would allow British advisers to take part in government affairs, allow British military bases to remain, and a requirement that Iraq assist the United Kingdom in wartime.[2] Strong political tensions existed between Iraq and the United Kingdom even upon gaining independence. After gaining independence in 1932, the Iraqi government immediately declared that Kuwait was rightfully a territory of Iraq. Kuwait had loosely been under the authority of the Ottoman vil\u00e2yet of Basra for centuries until the British had formally severed it from the Ottoman influence after the First World War. It was on this basis the Iraqi government stated that Kuwait was a British imperialist invention.[3]\n After Faisal died in September 1933, King Ghazi reigned as a figurehead from 1933 to 1939, when he was killed in a motor accident. Pressure from Arab nationalists and Iraqi nationalists demanded that the British leave Iraq, but their demands were ignored by the United Kingdom.\n Upon achieving official independence in October 1932, political tensions arose over the continued British presence in the new Hashemite Kingdom of Iraq, with Iraq's government and politicians split between those considered pro-British politicians, such as Nuri as-Said, who did not oppose a continued British presence, and anti-British politicians, such as Rashid Ali al-Gaylani, who demanded that remaining British influence in the country be removed.[4]\n Various ethnic and religious factions tried to gain political accomplishments during this period, often resulting in violent revolts and a brutal suppression by the Iraqi military, led by Bakr Sidqi. In 1933, thousands of Assyrians were killed in the Simele massacre, in 1935\u20131936 a series of Shi'a uprisings were brutally suppressed in mid-Euphrates region of Iraq,[5] and in parallel an anti-conscription Kurdish uprising in the north and a Yazidi revolt in Jabal Sinjar were crushed in 1935. Throughout the period political instability led to an exchange of numerous governments. Bakr Sidqi himself ascended to power in 1936, following a successful coup d'\u00e9tat against prime minister Yasin al-Hashimi but was later assassinated in 1937 during a visit to Mosul, followed by the death of King Ghazi in a car crash in 1939 suspected to have been planned by the British, causing a regency under Prince 'Abd al-Ilah over the 4 year old king Faisal II of Iraq lasting until 1953.\n From 1917 to 1946, five coups by the Iraqi Army occurred, led by the chief officers of the army against the government to pressure the government to concede to army demands.[4]\n The 1941 Iraqi coup d'\u00e9tat overthrew the pro-British Prime minister Taha al-Hashimi and placed Rashid Ali al-Gaylani as prime minister of a pro-Nazi government called \"the National defense government\", the Regent 'Abd al-Ilah fled the royal palace after learning of this and with British support went to Habbaniyah then to Basra, he would spend the rest of the following months in Jordan and the Mandate of Palestine. His fleeing caused a constitutional crisis upon the new government.[6] Rashid Ali did not abolish the monarchy, but installed \u064dSharif Sharaf bin Rajeh as a more compliant Regent instead, and attempted to restrict the rights of the British under the treaty from 1930. Rashid Ali attempted to secure control over Iraq asking assistance of Nazi Germany, Fascist Italy and Imperial Japan.\n On April 20 the Royal Iraqi Army established itself on the high ground to the south of the Habbaniyah air force base. An Iraqi envoy was sent to demand that no movements, either ground or air, were to take place from the base. The British refused the demand and then themselves demanded that the Iraqi army leave the area at once. After a further ultimatum given in the early hours of May 2 expired, at 0500 hours the British began bombing the Iraqi troops threatening the base, marking the beginning of the Anglo-Iraqi War.\n Hostilities lasted from May 2 to May 31, 1941, between Iraqis and the British and their indigenous Assyrian Levies. The British would continue to occupy Iraq for many years afterwards.\n In the aftermath of the Iraqi defeat, a bloody Farhud massacre broke out in Baghdad on June 2, initiated by the Futuwwa youth and Rashid Ali's supporters, resulting in deaths of some 180 Jews and heavy damage to the Jewish community.\n After the Anglo-Iraqi War ended, Abd al-ilah returned as Regent with Jameel Al-Madfaai as Prime minister and dominated the politics of Iraq until the overthrow of the monarchy and the royal family's assassination in 1958. the Government pursued a largely pro-western policy during this period.[7]\n al-Midfaai's government declared martial law in Baghdad and its surroundings, started a purge in government of Pro-Gaylani elements, banned the listening of axis-aligned radio, and various other procedures aimed at keeping security and order in the country.[8] Despite all these security procedures, this did not satisfy the British who demanded the disbanding of the Iraqi army and arresting any who supported, joined, or was sympathetic to the 1941 coup.\n Midfaai's government was split over the usage of force to cleanse the country of Pro-Gaylani elements, and some ministers were not amused of having to ally with Britain, neither did the Prime minister Himself entertain the idea of creating so many arrests. This policy outraged both the British and the regent, who saw his policy of empathy as indirectly supporting opposition and radical movements. The minister of Finance, Ibrahim Kamal al-Ghuthunfiri [ar], was at the top of the politicians who wanted a change to al-Midfaai's policy, and believed in the usage of harsher measures to keep security in the country, he submitted his resignation on 2 September 1941.[9]\n The resignation of Ibrahim Kamal weakened Midfaai's government, and the retired minister began calling for some politician to prepare the formation of a new government, and paved the way for Nuri al-Said to become the head of a new government. Jameel al-Midfaai's government retired and Abd al-Ilah ordered Nuri to form a new government in 9 October.\n In 1943, the Kurdish leader Mustafa Barzani led a rebellion against the central government in Baghdad. After the failure of the uprising Barzani and his followers fled to the Soviet Union.\n In 1945, during the final stages of World War II, Iraq joined the United Nations and became a founding member of the Arab League.\n The period following the end of the occupation was a time of the creation of various political parties opposed to or supportive of the government including the National Democratic Party led by Kamil Chadirji, the Constitutional Union Party led by Nuri Al-Said, and the Iraqi Independence Party led by Muhammad Mahdi Kubba.\n In 1948, massive violent protests, known as the Al-Wathbah uprising, broke out across Baghdad as a popular demand against the government treaty with the British, and with communist party support. More protests continued in spring, but were interrupted in May, with the martial law, when Iraq entered the 1948 Arab\u2013Israeli War along with other members of the Arab League. Various other protests against the government appeared, including the 1952 Iraqi Intifada which ended just before the 1953 Iraqi parliamentary election.\n King Faisal II reached his majority on 2 May 1953, ending the regency of Abd al-Ilah, who continued however to be influential in politics due to his influence on the young king.\n In 1955, to counter the influence of the Soviet Union on the Middle East, Iran, Iraq, Pakistan, Turkey and the United Kingdom signed the Baghdad Pact, with the United States being heavily involved in the negotiations to form it. Major protest and opposition followed the pact, as many did not approve of an alliance led by the west.\n In September 1956, a planned coup was discussed during spring training by a military faction known as the free officers (inspired by the Egyptian Free Officers Movement) which planned to launch the coup after training by controlling strategic sites in Baghdad and arresting the Regent and King. The coup failed however, as the training was suddenly stopped[clarification needed] .[10][11]\n In February 1958, King Hussein of Jordan and `Abd al-Il\u0101h proposed a union of H\u0101shimite monarchies to counter the recently formed Egyptian\u2013Syrian union. The resulting Arab Federation was formed on 14 February 1958.\n The Hashemite monarchy lasted until 1958, when it was overthrown through a coup d'\u00e9tat by the Iraqi Army, known as the 14 July Revolution. King Faisal II along with members of the Royal Family were executed in the courtyard of the Rihab Palace in central Baghdad (the young King had not yet moved into the newly completed Royal Palace). The coup brought Abd al-Karim Qasim to power. He withdrew from the Baghdad Pact and established friendly relations with the Soviet Union.\n Iraq under the monarchy faced two bare alternatives: either the country would have plunged into chaos or its population should become universally the clients and dependents of an omnipotent but capricious and unstable government. To these two alternatives the overthrow of the monarchy has not added a third.[12] The task of the subsequent governments was to find that third alternative, mainly to establish a modern state that is stable but also politically integrated.\n The population estimate in 1920 was 3 million, with the largest ethnic groups being Arabs, Kurds, Assyrians, and Turkmens, with minorities of Persians, Yezidis, Jews, Mandaeans, Shabaks, Armenians, and Kawliyah. During the Iraqi Hashemite rule, Arab population began to expand at the expense of other ethnic groups both due to higher birth rates and government policies which preferred Arab Sunni minority over other ethnic and religious groups.[13]\n In 1955, Iraqi population reached 6.5 million people. This was after the Iraqi Kingdom lost the most of its Jewish population following Operation Ezra and Nehemiah (some 130 thousand people) in 1951\u20131952.\n"
    }
]